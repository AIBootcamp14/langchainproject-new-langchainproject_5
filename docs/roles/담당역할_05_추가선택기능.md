# 추가 선택 기능 (먼저 개발 완료한 팀원이 선택)

## 문서 정보
- **작성일**: 2025-10-30
- **목적**: 핵심 기능 개발을 먼저 완료한 팀원이 추가로 선택하여 구현할 수 있는 기능 목록
- **우선순위**: 선택 사항 (시간이 있을 경우)

---

## 개요

핵심 기능(AI Agent, RAG, 용어집, 웹 검색, 파일 저장, UI)을 먼저 완료한 팀원이 아래 기능 중 선택하여 구현할 수 있습니다.

**선택 기준:**
1. 프로젝트에 가산점을 줄 수 있는 기능
2. 구현 난이도와 시간을 고려
3. 팀원의 관심사 및 강점

---

## 선택 기능 1: Text-to-SQL 도구 (⭐⭐⭐ 가산점)

### 기능 설명
자연어 질문을 SQL 쿼리로 변환하여 논문 통계 정보를 조회하는 도구

### 사용 시점
- "2024년에 발표된 논문 개수는?"
- "가장 많이 인용된 논문 Top 5는?"
- "저자별 논문 수 알려줘"
- "카테고리별 논문 분포는?"

### Langchain 구현

#### 1. SQLDatabaseChain 사용
```python
# src/tools/text2sql.py

from langchain.tools import tool
from langchain.chains import SQLDatabaseChain
from langchain_community.utilities import SQLDatabase
from langchain_openai import ChatOpenAI

# PostgreSQL DB 연결
db = SQLDatabase.from_uri("postgresql://user:password@localhost/papers")

# SQLDatabaseChain 생성
sql_chain = SQLDatabaseChain.from_llm(
    llm=ChatOpenAI(model="gpt-4", temperature=0),
    db=db,
    verbose=True
)

@tool
def query_paper_statistics(question: str) -> str:
    """
    논문 데이터베이스에서 통계 정보를 조회합니다.

    Args:
        question: 자연어 질문

    Returns:
        쿼리 결과
    """
    # SQLDatabaseChain 실행
    result = sql_chain.run(question)

    return f"## 📊 통계 결과\n\n{result}"
```

#### 2. 커스텀 SQL 생성 (고급)
```python
from langchain.prompts import PromptTemplate

# SQL 생성 프롬프트
SQL_GENERATION_PROMPT = PromptTemplate(
    template="""
    다음 질문을 SQL 쿼리로 변환하세요.

    데이터베이스 스키마:
    - papers (paper_id, title, authors, publish_date, category, citation_count)
    - glossary (term_id, term, definition, category)

    질문: {question}

    SQL 쿼리 (SELECT문만):
    """,
    input_variables=["question"]
)

@tool
def custom_sql_query(question: str) -> str:
    """커스텀 SQL 생성 및 실행"""
    # LLM으로 SQL 생성
    sql_query = llm.invoke(SQL_GENERATION_PROMPT.format(question=question)).content

    # SQL 실행
    import psycopg2
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")
    cursor = conn.cursor()

    try:
        cursor.execute(sql_query)
        results = cursor.fetchall()

        # 결과 포맷팅
        output = f"## 📊 쿼리 결과\n\n**SQL:**\n```sql\n{sql_query}\n```\n\n**결과:**\n"

        for row in results:
            output += f"- {row}\n"

        return output

    except Exception as e:
        return f"SQL 실행 오류: {e}"

    finally:
        cursor.close()
        conn.close()
```

### 구현해야 할 기능
1. SQLDatabaseChain 설정
2. DB 스키마 정보 프롬프트에 포함
3. SQL 쿼리 생성
4. 쿼리 실행 및 결과 포맷팅
5. 오류 처리 (잘못된 SQL 생성 시)

### 사용하는 DB
- **PostgreSQL**: `papers`, `glossary` 테이블

### 예상 소요 시간
**2-3일** (11/02~11/04)

### 참고 자료
- Langchain SQL Database: https://python.langchain.com/docs/integrations/tools/sql_database
- SQLDatabaseChain: https://python.langchain.com/docs/use_cases/sql/

---

## 선택 기능 2: 성능 평가 시스템 (⭐⭐ 가산점)

### 기능 설명
챗봇의 답변 품질을 자동으로 평가하는 시스템

### 평가 항목
1. **답변 정확도**: 논문 내용과 일치하는지
2. **관련성**: 질문과 답변이 관련있는지
3. **난이도 적합성**: Easy/Hard 모드에 맞는 답변인지
4. **출처 명시**: 논문 제목, 저자를 명시했는지

### Langchain 구현

#### 1. LLM-as-a-Judge 평가
```python
# src/evaluation/evaluator.py

from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

EVALUATION_PROMPT = PromptTemplate(
    template="""
    다음 챗봇 답변을 평가해주세요.

    질문: {question}
    답변: {answer}
    참고 문서: {reference_docs}
    난이도: {difficulty}

    평가 기준:
    1. 정확도 (0-10점): 참고 문서와 일치하는가?
    2. 관련성 (0-10점): 질문과 관련있는가?
    3. 난이도 적합성 (0-10점): {difficulty} 모드에 적합한가?
    4. 출처 명시 (0-10점): 논문 정보를 명시했는가?

    평가 결과를 JSON 형식으로 출력:
    {{
        "정확도": 점수,
        "관련성": 점수,
        "난이도_적합성": 점수,
        "출처_명시": 점수,
        "총점": 총점,
        "코멘트": "평가 코멘트"
    }}
    """,
    input_variables=["question", "answer", "reference_docs", "difficulty"]
)

class AnswerEvaluator:
    """답변 평가 시스템"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4", temperature=0)

    def evaluate(self, question: str, answer: str, reference_docs: str, difficulty: str):
        """답변 평가"""
        prompt = EVALUATION_PROMPT.format(
            question=question,
            answer=answer,
            reference_docs=reference_docs,
            difficulty=difficulty
        )

        result = self.llm.invoke(prompt).content

        # JSON 파싱
        import json
        evaluation = json.loads(result)

        return evaluation
```

#### 2. 평가 결과 저장
```python
def save_evaluation_results(evaluations: list):
    """평가 결과를 PostgreSQL에 저장"""
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")
    cursor = conn.cursor()

    # 평가 결과 테이블 생성
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS evaluation_results (
            eval_id SERIAL PRIMARY KEY,
            question TEXT,
            answer TEXT,
            accuracy_score INT,
            relevance_score INT,
            difficulty_score INT,
            citation_score INT,
            total_score INT,
            comment TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)

    # 평가 결과 삽입
    for eval_result in evaluations:
        cursor.execute("""
            INSERT INTO evaluation_results (
                question, answer, accuracy_score, relevance_score,
                difficulty_score, citation_score, total_score, comment
            )
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            eval_result['question'],
            eval_result['answer'],
            eval_result['정확도'],
            eval_result['관련성'],
            eval_result['난이도_적합성'],
            eval_result['출처_명시'],
            eval_result['총점'],
            eval_result['코멘트']
        ))

    conn.commit()
    cursor.close()
    conn.close()
```

### 구현해야 할 기능
1. LLM-as-a-Judge 프롬프트 설계
2. 평가 결과 JSON 파싱
3. PostgreSQL 평가 결과 테이블 생성
4. 평가 결과 저장 및 조회
5. Streamlit UI에 평가 결과 표시

### 예상 소요 시간
**2-3일** (11/02~11/04)

### 참고 자료
- LangChain Evaluation: https://python.langchain.com/docs/guides/evaluation/

---

## 선택 기능 3: Reranking (검색 최적화) (⭐⭐)

### 기능 설명
RAG 검색 결과를 재순위화하여 더 관련성 높은 문서를 상위에 배치

### Langchain 구현

#### 1. Cohere Rerank 사용
```python
# src/rag/reranking.py

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerank
from langchain_postgres.vectorstores import PGVector
from langchain_openai import OpenAIEmbeddings
import os

def create_rerank_retriever(base_retriever):
    """Reranking을 포함한 Retriever 생성"""
    # Cohere Rerank 압축기
    compressor = CohereRerank(
        model="rerank-english-v2.0",
        cohere_api_key=os.getenv("COHERE_API_KEY")
    )

    # ContextualCompressionRetriever 생성
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    return compression_retriever

# 사용
vectorstore = PGVector(
    collection_name="paper_chunks",
    embedding_function=OpenAIEmbeddings(model="text-embedding-3-small"),
    connection_string="postgresql://user:password@localhost:5432/papers"
)
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 20})

# Reranking 적용
rerank_retriever = create_rerank_retriever(base_retriever)

# 검색 (상위 20개 중 재순위화하여 상위 5개 반환)
docs = rerank_retriever.invoke("Transformer architecture")
```

#### 2. LLMChainExtractor (커스텀)
```python
from langchain.retrievers.document_compressors import LLMChainExtractor

def create_llm_extractor_retriever(base_retriever, llm):
    """LLM을 사용한 문서 압축"""
    compressor = LLMChainExtractor.from_llm(llm)

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    return compression_retriever
```

### 구현해야 할 기능
1. Cohere API 키 설정
2. ContextualCompressionRetriever 구현
3. RAG 검색 도구에 Reranking 적용
4. 성능 비교 (Before/After)

### 예상 소요 시간
**1-2일** (11/03~11/04)

### 참고 자료
- ContextualCompressionRetriever: https://python.langchain.com/docs/modules/data_connection/retrievers/contextual_compression/

---

## 선택 기능 4: 논문 비교 도구 (⭐)

### 기능 설명
여러 논문의 차이점을 비교하여 표로 정리

### 사용 시점
- "BERT와 GPT 비교해줘"
- "Transformer와 RNN의 차이는?"

### Langchain 구현
```python
# src/tools/paper_comparison.py

from langchain.tools import tool

@tool
def compare_papers(paper1_title: str, paper2_title: str) -> str:
    """
    두 논문을 비교합니다.

    Args:
        paper1_title: 첫 번째 논문 제목
        paper2_title: 두 번째 논문 제목

    Returns:
        비교 결과 (표 형식)
    """
    # 1. 두 논문 검색
    paper1_docs = rag_retriever.retrieve(paper1_title)
    paper2_docs = rag_retriever.retrieve(paper2_title)

    # 2. LLM에게 비교 요청
    prompt = f"""
    다음 두 논문을 비교하여 표로 정리해주세요.

    논문 1: {paper1_title}
    내용: {paper1_docs[0].page_content}

    논문 2: {paper2_title}
    내용: {paper2_docs[0].page_content}

    비교 항목:
    - 주요 기여
    - 모델 구조
    - 장점
    - 단점
    - 성능

    Markdown 표 형식으로 출력:
    """

    result = llm.invoke(prompt).content

    return result
```

### 예상 소요 시간
**1일** (11/04)

---

## 선택 기능 5: 대화 이력 시각화 (⭐)

### 기능 설명
사용자 질문 패턴, 자주 사용되는 도구 등을 시각화

### Streamlit 구현
```python
# ui/analytics.py

import streamlit as st
import pandas as pd
import plotly.express as px

def show_analytics():
    """대화 이력 분석 대시보드"""
    st.header("📊 대화 이력 분석")

    # PostgreSQL에서 query_logs 조회
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")

    # 도구 사용 통계
    df_tools = pd.read_sql("""
        SELECT tool_used, COUNT(*) as count
        FROM query_logs
        GROUP BY tool_used
    """, conn)

    fig = px.bar(df_tools, x="tool_used", y="count", title="도구 사용 통계")
    st.plotly_chart(fig)

    # 난이도별 질문 분포
    df_difficulty = pd.read_sql("""
        SELECT difficulty_mode, COUNT(*) as count
        FROM query_logs
        GROUP BY difficulty_mode
    """, conn)

    fig2 = px.pie(df_difficulty, names="difficulty_mode", values="count", title="난이도별 질문 분포")
    st.plotly_chart(fig2)

    conn.close()
```

### 예상 소요 시간
**1일** (11/04)

---

## 선택 기능 6: 멀티모달 지원 (이미지 논문) (⭐⭐⭐)

### 기능 설명
논문 내 그래프, 표, 수식 이미지를 GPT-4 Vision으로 분석

### Langchain 구현
```python
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# GPT-4 Vision 사용
llm_vision = ChatOpenAI(model="gpt-4-vision-preview")

def analyze_paper_image(image_path: str, question: str) -> str:
    """논문 이미지 분석"""
    import base64

    # 이미지 인코딩
    with open(image_path, "rb") as f:
        image_data = base64.b64encode(f.read()).decode()

    # GPT-4 Vision 호출
    message = HumanMessage(
        content=[
            {"type": "text", "text": question},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
        ]
    )

    response = llm_vision.invoke([message])

    return response.content
```

### 예상 소요 시간
**2-3일** (11/03~11/05)

---

## 우선순위 추천

### High Priority (가산점 크고 구현 가능)
1. **Text-to-SQL** (⭐⭐⭐) - 2-3일
2. **성능 평가 시스템** (⭐⭐) - 2-3일

### Medium Priority (구현 간단)
3. **Reranking** (⭐⭐) - 1-2일
4. **논문 비교 도구** (⭐) - 1일
5. **대화 이력 시각화** (⭐) - 1일

### Low Priority (시간 많이 소요)
6. **멀티모달 지원** (⭐⭐⭐) - 2-3일

---

## 선택 가이드

### 시간이 2일 남았다면
→ **Text-to-SQL** 또는 **성능 평가 시스템**

### 시간이 1일 남았다면
→ **Reranking** 또는 **논문 비교 도구**

### 시간이 여유 있다면
→ **멀티모달 지원**

---

## 참고 사항

- 선택 기능은 **필수가 아닙니다**
- 핵심 기능을 먼저 완성한 후 시간이 남을 때만 구현
- 팀원들과 상의하여 역할 분배
- 구현 시 **Feature 브랜치 생성** 필수
