# ==========================================
# ğŸ“˜ RAG ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬ ëª¨ë“ˆ + Agent ë…¸ë“œ í†µí•©
# ------------------------------------------
# - @tool: search_paper_database
# - Agent ë…¸ë“œ: search_paper_node
# - ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ (similarity/MMR), MultiQuery ì˜µì…˜
# - ë©”íƒ€ë°ì´í„° í•„í„°(year/author/category)
# - PostgreSQL ë©”íƒ€ ì¡°íšŒ â†’ ê²°ê³¼ í•©ì„± â†’ Markdown ë°˜í™˜
# ==========================================

import os
from typing import Any, Dict, List, Optional, Tuple

import psycopg2
from psycopg2.extras import RealDictCursor
from langchain_core.tools import tool
from langchain_core.documents import Document
from langchain.schema import SystemMessage, HumanMessage

from src.rag.retriever import RAGRetriever
from src.prompts import get_tool_prompt
from src.llm.client import LLMClient


# ==================== ë‚´ë¶€ ìœ í‹¸: í™˜ê²½/DB ==================== #

def _env(primary: str, alt: str, default: Optional[str] = None) -> Optional[str]:
    """í™˜ê²½ ë³€ìˆ˜ ì¡°íšŒ (ìš°ì„ ìˆœìœ„: primary > alt > default)"""
    return os.getenv(primary) or os.getenv(alt) or default


def _pg_conn_str() -> str:
    """PostgreSQL ì—°ê²° ë¬¸ìì—´ ìƒì„±"""
    user = _env("POSTGRES_USER", "PGUSER", "postgres")
    password = _env("POSTGRES_PASSWORD", "PGPASSWORD", "postgres")
    host = _env("POSTGRES_HOST", "PGHOST", "localhost")
    port = _env("POSTGRES_PORT", "PGPORT", "5432")
    db = _env("POSTGRES_DB", "PGDATABASE", "papers")
    return f"postgresql://{user}:{password}@{host}:{port}/{db}"


def _fetch_paper_meta(paper_ids: List[int]) -> Dict[int, Dict[str, Any]]:
    """
    papers í…Œì´ë¸”ì—ì„œ ID ëª©ë¡ì— í•´ë‹¹í•˜ëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì¼ê´„ ì¡°íšŒ.

    Args:
        paper_ids: ë…¼ë¬¸ ID ë¦¬ìŠ¤íŠ¸

    Returns:
        Dict[int, Dict[str, Any]]: {paper_id: {title, authors, ...}}
    """
    if not paper_ids:
        return {}

    conn = psycopg2.connect(_pg_conn_str())
    try:
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute(
                """
                SELECT paper_id, title, authors, publish_date, url, category, citation_count
                FROM papers
                WHERE paper_id = ANY(%s)
                """,
                (paper_ids,),
            )
            rows = cur.fetchall()
            out: Dict[int, Dict[str, Any]] = {}
            for row in rows:
                out[row["paper_id"]] = dict(row)
            return out
    finally:
        conn.close()


def _format_markdown(results: List[Dict[str, Any]]) -> str:
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ Markdown ë¬¸ìì—´ë¡œ ë³€í™˜.

    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        str: Markdown í˜•ì‹ì˜ ë¬¸ìì—´
    """
    if not results:
        return "ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # âœ… ìœ ì‚¬ë„ ì ìˆ˜ ê²€ì¦: ìµœì†Œ í•˜ë‚˜ì˜ ê²°ê³¼ê°€ ì„ê³„ê°’ ì´í•˜(ìœ ì‚¬ë„ ë†’ìŒ)ì—¬ì•¼ í•¨
    SIMILARITY_THRESHOLD = 0.5  # distance ê¸°ì¤€ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬, pgvector cosine distance)
    has_relevant_result = False

    for r in results:
        score = r.get("score")
        # scoreê°€ Noneì´ ì•„ë‹ˆê³  ì„ê³„ê°’ ì´í•˜(ìœ ì‚¬ë„ ë†’ìŒ)ì¸ ê²½ìš°
        if score is not None and score <= SIMILARITY_THRESHOLD:
            has_relevant_result = True
            break

    # ëª¨ë“  ê²°ê³¼ì˜ ìœ ì‚¬ë„ê°€ ë‚®ìœ¼ë©´ (scoreê°€ ëª¨ë‘ ì„ê³„ê°’ ì´ˆê³¼) ì‹¤íŒ¨ ì²˜ë¦¬
    if not has_relevant_result:
        return "ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    lines: List[str] = ["## ê²€ìƒ‰ëœ ë…¼ë¬¸\n"]
    for i, r in enumerate(results, 1):
        score_str = f"{r['score']:.4f}" if r.get("score") is not None else "N/A"

        lines.append(f"### {i}. {r.get('title','(Untitled)')}")
        lines.append(f"- **ì €ì**: {r.get('authors','')}")
        lines.append(f"- **ì¶œíŒì¼**: {r.get('publish_date','')}")
        lines.append(f"- **ì¹´í…Œê³ ë¦¬**: {r.get('category','')}")
        lines.append(f"- **ì¸ìš©ìˆ˜**: {r.get('citation_count','')}")
        lines.append(f"- **URL**: {r.get('url','')}")
        lines.append(f"- **ì„¹ì…˜**: {r.get('section','ë³¸ë¬¸')}")
        lines.append(f"- **ìœ ì‚¬ë„ ì ìˆ˜(ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)**: {score_str}\n")

        preview = (r.get("content") or "")[:600]
        if preview:
            lines.append(preview + ("..." if len(r.get("content","")) > 600 else ""))
        lines.append("\n---\n")

    return "\n".join(lines)


def _build_filter(year_gte: Optional[int], author: Optional[str], category: Optional[str]) -> Dict[str, Any]:
    """
    VectorStore ë©”íƒ€ë°ì´í„° í•„í„° êµ¬ì„±.

    Args:
        year_gte: ì—°ë„ ì´ìƒ í•„í„°
        author: ì €ì ë¶€ë¶„ì¼ì¹˜ í•„í„°
        category: ì¹´í…Œê³ ë¦¬ í•„í„°

    Returns:
        Dict[str, Any]: í•„í„° ë”•ì…”ë„ˆë¦¬
    """
    f: Dict[str, Any] = {}
    if year_gte is not None:
        f["year"] = {"$gte": int(year_gte)}
    if author:
        f["authors"] = {"$ilike": f"%{author}%"}
    if category:
        f["category"] = category
    return f


def _keyword_search(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    PostgreSQL Full-Text Searchë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰.

    Args:
        query: ê²€ìƒ‰ ì§ˆë¬¸
        top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜

    Returns:
        List[Dict[str, Any]]: í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ (paper_id, title, abstract, score)
    """
    import re

    # ì¿¼ë¦¬ ì „ì²˜ë¦¬: ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
    # 1. ê´„í˜¸ ì•ˆì˜ ì˜ì–´ ìš°ì„  ì‚¬ìš© (ì˜ˆ: "RAG (Retrieval-Augmented Generation)" â†’ "Retrieval-Augmented Generation")
    # 2. ì—†ìœ¼ë©´ ì˜ì–´ ë‹¨ì–´ë“¤ ì¶”ì¶œ
    english_keywords = []

    # ê´„í˜¸ ì•ˆì˜ ì˜ì–´ ì¶”ì¶œ
    paren_match = re.search(r'\(([A-Za-z0-9\s\-]+)\)', query)
    if paren_match:
        english_keywords.append(paren_match.group(1).strip())

    # ì˜ì–´ ë‹¨ì–´ ì¶”ì¶œ (3ê¸€ì ì´ìƒ)
    words = re.findall(r'\b[A-Za-z]{3,}(?:-[A-Za-z]+)*\b', query)
    english_keywords.extend(words)

    # ì¤‘ë³µ ì œê±° ë° ê³µë°± ì œê±°
    english_keywords = list(dict.fromkeys([k.strip() for k in english_keywords if k.strip()]))

    if not english_keywords:
        # ì˜ì–´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        search_query = query
    else:
        # ê°€ì¥ ê¸´ í‚¤ì›Œë“œ ìš°ì„  ì‚¬ìš© (ë” êµ¬ì²´ì )
        search_query = max(english_keywords, key=len)

    conn = psycopg2.connect(_pg_conn_str())
    try:
        cursor = conn.cursor()

        # PostgreSQL Full-Text Search (title, abstract)
        sql = """
        SELECT
            paper_id,
            title,
            abstract,
            authors,
            publish_date,
            category,
            citation_count,
            url,
            (
                CASE
                    WHEN title ILIKE %s THEN 2.0
                    ELSE 0.0
                END +
                CASE
                    WHEN abstract ILIKE %s THEN 1.0
                    ELSE 0.0
                END
            ) AS keyword_score
        FROM papers
        WHERE title ILIKE %s OR abstract ILIKE %s
        ORDER BY keyword_score DESC, citation_count DESC
        LIMIT %s
        """

        search_pattern = f"%{search_query}%"
        cursor.execute(sql, (search_pattern, search_pattern, search_pattern, search_pattern, top_k))

        results = []
        for row in cursor.fetchall():
            results.append({
                "paper_id": row[0],
                "title": row[1],
                "abstract": row[2],
                "authors": row[3],
                "publish_date": row[4],
                "category": row[5],
                "citation_count": row[6],
                "url": row[7],
                "keyword_score": float(row[8]),
            })

        cursor.close()
        return results

    except Exception as e:
        return []
    finally:
        conn.close()


# ==================== @tool: ë…¼ë¬¸ ê²€ìƒ‰ ==================== #

@tool
def search_paper_database(
    query: str,
    year_gte: Optional[int] = None,
    author: Optional[str] = None,
    category: Optional[str] = None,
    top_k: int = 5,
    with_scores: bool = True,
    use_multi_query: bool = True,
    search_mode: str = "mmr",  # "similarity" | "mmr"
    use_hybrid: bool = True,   # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
    tool_name: str = "search_paper",  # ë„êµ¬ëª… (ê°€ì¤‘ì¹˜ ì¡°ì •ìš©)
) -> str:
    """
    ë…¼ë¬¸ VectorDB + PostgreSQL ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ì¡°íšŒí•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜.
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: ë²¡í„° ê²€ìƒ‰ + í‚¤ì›Œë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ê²°í•©

    Parameters
    ----------
    query : str
        ì‚¬ìš©ì ì§ˆì˜
    year_gte : Optional[int]
        íŠ¹ì • ì—°ë„ ì´ìƒ í•„í„° (ì˜ˆ: 2020)
    author : Optional[str]
        ì €ì ë¶€ë¶„ì¼ì¹˜ í•„í„°
    category : Optional[str]
        ì¹´í…Œê³ ë¦¬ í•„í„° (ì˜ˆ: 'cs.CL')
    top_k : int
        ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
    with_scores : bool
        ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨ ì—¬ë¶€
    use_multi_query : bool
        MultiQuery(LLM ì¿¼ë¦¬ í™•ì¥) ì‚¬ìš© ì—¬ë¶€
    search_mode : str
        "similarity" ë˜ëŠ” "mmr"
    use_hybrid : bool
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
    tool_name : str
        ë„êµ¬ëª… (glossary, search_paper ë“±)
    """

    # ---------- Configì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ë¡œë“œ ----------
    from src.utils.config_loader import get_model_config

    config = get_model_config()
    hybrid_config = config.get("rag", {}).get("hybrid_search", {})
    hybrid_enabled = hybrid_config.get("enabled", True) and use_hybrid

    # ë„êµ¬ë³„ ê°€ì¤‘ì¹˜ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ê°€ì¤‘ì¹˜
    tool_weights = hybrid_config.get("tool_specific_weights", {}).get(tool_name, {})
    vector_weight = tool_weights.get("vector_weight", hybrid_config.get("vector_weight", 0.7))
    keyword_weight = tool_weights.get("keyword_weight", hybrid_config.get("keyword_weight", 0.3))

    # ---------- Retriever ì¤€ë¹„ ----------
    r = RAGRetriever(search_type=search_mode, k=top_k)

    # ---------- ê²€ìƒ‰ ì‹¤í–‰ (í•„í„°/ë©€í‹°ì¿¼ë¦¬ ì²˜ë¦¬) ----------
    filter_dict = _build_filter(year_gte, author, category)
    docs: List[Document] = []
    pairs: List[Tuple[Document, float]] = []

    if any(filter_dict.values()):
        # ë©”íƒ€ë°ì´í„° í•„í„°ê°€ ìˆìœ¼ë©´ similarity + filterë¡œ ìˆ˜í–‰
        docs = r.search_with_filter(query, filter_dict, k=top_k)
        if with_scores:
            pairs = [(d, None) for d in docs]  # type: ignore
    else:
        if use_multi_query:
            docs = r.multi_query_search(query, k=top_k)
            if with_scores:
                pairs = r.similarity_search_with_score(query, k=len(docs) or top_k)
        else:
            if with_scores:
                pairs = r.similarity_search_with_score(query, k=top_k)
                docs = [d for d, _ in pairs]
            else:
                docs = r.similarity_search(query, k=top_k)

    # ---------- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í‚¤ì›Œë“œ ê²€ìƒ‰ ì¶”ê°€ ----------
    keyword_results = []
    if hybrid_enabled:
        keyword_results = _keyword_search(query, top_k=top_k)

    # ---------- paper_id ë©”íƒ€ë¡œ PostgreSQL ë©”íƒ€ë°ì´í„° ì¡°íšŒ ----------
    paper_ids = []
    for d in docs:
        pid = d.metadata.get("paper_id")
        if isinstance(pid, int):
            paper_ids.append(pid)
        else:
            # í˜¹ì‹œ strì´ë©´ int ë¡œ ë³€í™˜ ì‹œë„
            try:
                paper_ids.append(int(pid))
            except Exception:
                pass
    meta_map = _fetch_paper_meta(list(set(paper_ids)))

    # ---------- ê²°ê³¼ í•©ì„± (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ ì ìš©) ----------
    score_map: Dict[int, float] = {}  # paper_id â†’ final_score

    # 1. ë²¡í„° ê²€ìƒ‰ ì ìˆ˜ (ê°€ì¤‘ì¹˜ ì ìš©)
    if with_scores and pairs:
        for d, s in pairs:
            if s is None:
                continue
            pid = d.metadata.get("paper_id")
            if pid:
                # ë²¡í„° ê²€ìƒ‰ ì ìˆ˜: ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬ (distance) â†’ ì •ê·œí™” í•„ìš”
                # score = 1 / (1 + distance) í˜•íƒœë¡œ ë³€í™˜
                normalized_score = 1.0 / (1.0 + float(s))
                score_map[pid] = score_map.get(pid, 0.0) + normalized_score * vector_weight

    # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ ì ìˆ˜ (ê°€ì¤‘ì¹˜ ì ìš©)
    if hybrid_enabled and keyword_results:
        for kw_result in keyword_results:
            pid = kw_result["paper_id"]
            keyword_score = kw_result["keyword_score"]
            # í‚¤ì›Œë“œ ì ìˆ˜: ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (title: 2.0, abstract: 1.0)
            # ì •ê·œí™”: ìµœëŒ€ 3.0 ê¸°ì¤€ (title + abstract)
            normalized_kw_score = keyword_score / 3.0
            score_map[pid] = score_map.get(pid, 0.0) + normalized_kw_score * keyword_weight

    # 3. ìµœì¢… ê²°ê³¼ ìƒì„± (score ê¸°ì¤€ ì •ë ¬)
    results: List[Dict[str, Any]] = []
    seen_pids = set()

    # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
    for d in docs:
        pid = d.metadata.get("paper_id")
        if pid and pid not in seen_pids:
            seen_pids.add(pid)
            meta = meta_map.get(pid, {}) if pid is not None else {}
            results.append({
                "paper_id": pid,
                "title": meta.get("title") or d.metadata.get("title"),
                "authors": meta.get("authors") or d.metadata.get("authors"),
                "publish_date": meta.get("publish_date") or d.metadata.get("publish_date"),
                "url": meta.get("url") or d.metadata.get("url"),
                "category": meta.get("category") or d.metadata.get("category"),
                "citation_count": meta.get("citation_count"),
                "section": d.metadata.get("section", "ë³¸ë¬¸"),
                "content": d.page_content,
                "score": score_map.get(pid, 0.0) if with_scores else None,
            })

    # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µ ì œì™¸)
    if hybrid_enabled and keyword_results:
        for kw_result in keyword_results:
            pid = kw_result["paper_id"]
            if pid not in seen_pids:
                seen_pids.add(pid)
                # í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œë§Œ ì°¾ì€ ê²½ìš° contentëŠ” abstract ì‚¬ìš©
                results.append({
                    "paper_id": pid,
                    "title": kw_result.get("title"),
                    "authors": kw_result.get("authors"),
                    "publish_date": kw_result.get("publish_date"),
                    "url": kw_result.get("url"),
                    "category": kw_result.get("category"),
                    "citation_count": kw_result.get("citation_count"),
                    "section": "ì´ˆë¡",
                    "content": kw_result.get("abstract", ""),
                    "score": score_map.get(pid, 0.0) if with_scores else None,
                })

    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ (ë†’ì€ ì ìˆ˜ë¶€í„°)
    if with_scores and hybrid_enabled:
        results.sort(key=lambda x: x.get("score", 0.0), reverse=True)

    # top_k ì œí•œ
    results = results[:top_k]

    # ---------- Markdown í¬ë§·ìœ¼ë¡œ ë°˜í™˜ ----------
    return _format_markdown(results)


# ==================== Agent ë…¸ë“œ: RAG ê²€ìƒ‰ ==================== #

def search_paper_node(state, exp_manager=None):
    """
    Agent ë…¸ë“œ: ë…¼ë¬¸ DBì—ì„œ ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±

    Args:
        state (AgentState): Agent ìƒíƒœ
        exp_manager: ExperimentManager ì¸ìŠ¤í„´ìŠ¤ (ì„ íƒ ì‚¬í•­)

    Returns:
        AgentState: ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    # -------------- ìƒíƒœì—ì„œ ì§ˆë¬¸ ë° ë‚œì´ë„ ì¶”ì¶œ -------------- #
    # âœ… refined_query ìš°ì„  ì‚¬ìš© (Multi-turn ì§€ì›)
    question = state.get("refined_query", state["question"])  # ì¬ì‘ì„±ëœ ì§ˆë¬¸ ìš°ì„ , ì—†ìœ¼ë©´ ì›ë³¸
    difficulty = state.get("difficulty", "easy")              # ë‚œì´ë„ (ê¸°ë³¸ê°’: easy)

    # -------------- ë„êµ¬ë³„ Logger ìƒì„± -------------- #
    tool_logger = exp_manager.get_tool_logger('rag_paper') if exp_manager else None

    if tool_logger:
        if "refined_query" in state:
            tool_logger.write(f"RAG ê²€ìƒ‰ ë…¸ë“œ ì‹¤í–‰: {question} (ì¬ì‘ì„±ëœ ì§ˆë¬¸)")
        else:
            tool_logger.write(f"RAG ê²€ìƒ‰ ë…¸ë“œ ì‹¤í–‰: {question}")
        tool_logger.write(f"ë‚œì´ë„: {difficulty}")

    # -------------- search_paper_database ë„êµ¬ í˜¸ì¶œ -------------- #
    try:
        # Langchain @tool í•¨ìˆ˜ í˜¸ì¶œ
        raw_results = search_paper_database.invoke({
            "query": question,                            # ê²€ìƒ‰ ì¿¼ë¦¬
            "year_gte": None,                             # ì—°ë„ í•„í„° ì—†ìŒ
            "author": None,                               # ì €ì í•„í„° ì—†ìŒ
            "category": None,                             # ì¹´í…Œê³ ë¦¬ í•„í„° ì—†ìŒ
            "top_k": 5,                                   # Top-5 ê²€ìƒ‰
            "with_scores": True,                          # ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨
            "use_multi_query": True,                      # âœ… MultiQuery í™œì„±í™” (ê²€ìƒ‰ ê°•í™”)
            "search_mode": "similarity",                  # ìœ ì‚¬ë„ ê²€ìƒ‰
            "use_hybrid": True,                           # âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í™œì„±í™” (ë²¡í„°+í‚¤ì›Œë“œ)
            "tool_name": "search_paper",                  # âœ… ë„êµ¬ëª… (ê°€ì¤‘ì¹˜ ì¡°ì •ìš©)
        })

        if tool_logger:
            tool_logger.write(f"ê²€ìƒ‰ ê²°ê³¼: {len(raw_results)} ê¸€ì")

        # -------------- ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ ì²´í¬ (Fallback íŠ¸ë¦¬ê±°) -------------- #
        if "ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in raw_results:
            if tool_logger:
                tool_logger.write("ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Fallback í•„ìš”.")
                tool_logger.close()

            # ëª…í™•í•œ ì‹¤íŒ¨ ë©”ì‹œì§€ ë°˜í™˜ (failure_detector íŒ¨í„´ê³¼ ì •í™•íˆ ì¼ì¹˜)
            state["final_answer"] = "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            return state

        # -------------- pgvector ê²€ìƒ‰ ê¸°ë¡ -------------- #
        if exp_manager:
            exp_manager.log_pgvector_search({
                "tool": "search_paper",
                "collection": "paper_chunks",
                "query_text": question,
                "search_mode": "similarity",
                "top_k": 5,
                "use_multi_query": False,
                "result_length": len(raw_results)
            })

        # -------------- ë‘ ìˆ˜ì¤€ì˜ ë‹µë³€ ìƒì„± -------------- #
        level_mapping = {
            "easy": ["elementary", "beginner"],
            "hard": ["intermediate", "advanced"]
        }

        levels = level_mapping.get(difficulty, ["beginner", "intermediate"])
        final_answers = {}

        # ë‚œì´ë„ë³„ LLM ì´ˆê¸°í™” (ê³µí†µ)
        llm_client = LLMClient.from_difficulty(
            difficulty=difficulty,
            logger=exp_manager.logger if exp_manager else None
        )

        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ (ê³µí†µ)
        user_content = f"""[ë…¼ë¬¸ ê²€ìƒ‰ ê²°ê³¼]
{raw_results}

[ì§ˆë¬¸]
{question}

ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

        # ê° ìˆ˜ì¤€ë³„ë¡œ ë‹µë³€ ìƒì„±
        for level in levels:
            if tool_logger:
                tool_logger.write(f"ìˆ˜ì¤€ '{level}' ë‹µë³€ ìƒì„± ì‹œì‘")

            # JSON í”„ë¡¬í”„íŠ¸ ë¡œë“œ
            system_prompt = get_tool_prompt("search_paper", level)

            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_content)
            ]

            # í”„ë¡¬í”„íŠ¸ ì €ì¥
            if exp_manager:
                exp_manager.save_system_prompt(system_prompt, {
                    "tool": "search_paper",
                    "difficulty": difficulty,
                    "level": level
                })
                final_prompt = f"""[SYSTEM PROMPT - {level}]
{system_prompt}

[USER PROMPT]
{user_content}"""
                exp_manager.save_final_prompt(final_prompt, {
                    "tool": "search_paper",
                    "difficulty": difficulty,
                    "level": level
                })

            # LLM í˜¸ì¶œ
            response = llm_client.llm.invoke(messages)
            final_answers[level] = response.content

            # ë¡œê¹…
            if tool_logger:
                tool_logger.write(f"ìˆ˜ì¤€ '{level}' ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(response.content)} ê¸€ì")
                tool_logger.write("=" * 80)
                tool_logger.write(f"[{level} ë‹µë³€ ì „ì²´ ë‚´ìš©]")
                tool_logger.write(response.content)
                tool_logger.write("=" * 80)

        if tool_logger:
            tool_logger.close()

        # -------------- ìµœì¢… ë‹µë³€ ì €ì¥ -------------- #
        state["final_answers"] = final_answers
        state["final_answer"] = final_answers[levels[1]]

    except Exception as e:
        if tool_logger:
            tool_logger.write(f"ë…¼ë¬¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            tool_logger.close()

        # ì—ëŸ¬ ë©”ì‹œì§€ ì €ì¥
        state["final_answer"] = f"ë…¼ë¬¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"

    # -------------- ì—…ë°ì´íŠ¸ëœ ìƒíƒœ ë°˜í™˜ -------------- #
    return state
