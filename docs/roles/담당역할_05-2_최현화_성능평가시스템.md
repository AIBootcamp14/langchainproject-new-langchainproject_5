# 담당역할_05-2: 성능 평가 시스템

## 문서 정보
- **작성일**: 2025-11-04
- **작성자**: 최현화[팀장]
- **프로젝트명**: 논문 리뷰 챗봇 (AI Agent + RAG)
- **팀명**: 연결의 민족
- **담당 기능**: 성능 평가 시스템, LLM-as-a-Judge, RAG 평가, Agent 정확도 측정

---

## 담당자 정보

**담당자**: 최현화[팀장]

**역할**: 성능 평가 시스템 구현 및 평가 지표 측정

---

## 담당 업무 개요

이 역할은 챗봇의 답변 품질을 자동으로 평가하는 **성능 평가 시스템**을 구축하는 것을 담당합니다. LLM-as-a-Judge 방식을 사용하여 답변 정확도, 관련성, 난이도 적합성, 출처 명시 여부를 평가하고, RAG 검색 성능과 Agent 라우팅 정확도를 측정합니다.

### 주요 책임

1. **AnswerEvaluator 클래스 구현** (`src/evaluation/evaluator.py`)
   - LLM-as-a-Judge 평가 프롬프트 설계
   - 4가지 평가 기준 (정확도, 관련성, 난이도 적합성, 출처 명시)
   - JSON 형식 평가 결과 파싱

2. **평가 결과 저장**
   - evaluation_results 테이블 생성 (PostgreSQL)
   - 평가 지표 저장 및 조회
   - 통계 집계 기능

3. **Streamlit UI 연동**
   - 평가 결과 표시
   - 평가 통계 시각화 (선택)

---

## 참여 기간 및 우선순위

### 참여 기간
- **구현 기간**: 2025-11-04 ~ 2025-11-04 (1일)
- **테스트 및 검증**: 2025-11-04 (당일)

### 우선순위
1. **최우선 (1-2시간)**: AnswerEvaluator 클래스 구현
   - LLM-as-a-Judge 프롬프트 설계
   - 평가 함수 구현
   - JSON 파싱

2. **우선 (1시간)**: 평가 결과 저장
   - evaluation_results 테이블 생성
   - 저장 함수 구현

3. **선택 (1시간)**: Streamlit UI 연동
   - 평가 결과 표시 (선택)

---

## 세부 업무 및 구현 내용

### 1. AnswerEvaluator 클래스 구현

**파일 경로**: `src/evaluation/evaluator.py`

#### 1.1 코드 구조 설명

**필요 라이브러리**:
- 표준: `json`, `typing.Dict`, `typing.Optional`
- 서드파티: `langchain_openai.ChatOpenAI`, `langchain.prompts.PromptTemplate`
- 프로젝트: `src.llm.client.LLMClient`, `src.utils.logger.Logger`

---

**평가 프롬프트 (EVALUATION_PROMPT_TEMPLATE)**:

**입력 변수**:

| 변수 | 설명 |
|-----|------|
| {question} | 사용자 질문 |
| {answer} | AI 답변 |
| {reference_docs} | 참고 문서 |
| {difficulty} | 난이도 모드 (easy/hard) |

**평가 기준**:

| 항목 | 점수 범위 | 평가 내용 |
|-----|----------|----------|
| 정확도 | 0-10점 | 참고 문서의 내용과 일치하는가? |
| 관련성 | 0-10점 | 질문과 답변이 관련있는가? |
| 난이도 적합성 | 0-10점 | 난이도 모드에 맞는 답변인가? |
| 출처 명시 | 0-10점 | 논문 제목, 저자를 명시했는가? |
| **총점** | **0-40점** | 4개 항목 점수 합계 |

**출력 형식**: JSON (accuracy_score, relevance_score, difficulty_score, citation_score, total_score, comment)

---

**클래스: `AnswerEvaluator`**

**역할**: LLM-as-a-Judge 방식으로 답변 품질 평가

**`__init__` 메서드**:

| 파라미터 | 타입 | 설명 | 기본값 |
|---------|------|------|--------|
| exp_manager | ExperimentManager | 실험 관리 인스턴스 | None |

**초기화 동작**:

| 단계 | 작업 | 설명 |
|-----|------|------|
| 1 | LLM 초기화 | ChatOpenAI(model="gpt-5", temperature=0) |
| 2 | Logger 설정 | exp_manager 있으면 get_tool_logger("evaluator"), 없으면 Logger("logs/evaluator.log") |
| 3 | 로그 기록 | "AnswerEvaluator 초기화 완료" |

---

**`evaluate` 메서드**:

**파라미터**:

| 파라미터 | 타입 | 설명 |
|---------|------|------|
| question | str | 사용자 질문 |
| answer | str | AI 답변 |
| reference_docs | str | 참고 문서 |
| difficulty | str | 난이도 모드 (easy/hard) |

**반환값**: `Dict` - 평가 결과 (accuracy_score, relevance_score, difficulty_score, citation_score, total_score, comment)

**처리 흐름**:

| 단계 | 작업 | 설명 |
|-----|------|------|
| 1 | 프롬프트 포맷팅 | EVALUATION_PROMPT.format()으로 입력값 삽입 |
| 2 | 평가 시작 로그 | "평가 시작: {question}" |
| 3 | LLM 호출 | self.llm.invoke(prompt) |
| 4 | 응답 수신 로그 | "LLM 응답 수신: {len} 글자" |
| 5 | JSON 파싱 | json.loads(result_text) |
| 6 | 평가 완료 로그 | "평가 완료: 총점 {total_score}/40" |
| 7 | 결과 반환 | 평가 결과 딕셔너리 |

**오류 처리**:
- JSONDecodeError: 모든 점수 0, comment: "평가 실패: JSON 파싱 오류"
- Exception: 모든 점수 0, comment: "평가 실패: {error}"

---

**`evaluate_batch` 메서드**:

**파라미터**:

| 파라미터 | 타입 | 설명 |
|---------|------|------|
| test_cases | list | 테스트 케이스 리스트 (각 항목: question, answer, reference_docs, difficulty) |

**반환값**: `list` - 평가 결과 리스트

**처리 흐름**:

| 단계 | 작업 | 설명 |
|-----|------|------|
| 1 | 배치 평가 시작 로그 | "배치 평가 시작: {len} 케이스" |
| 2 | 각 케이스 순회 | enumerate(test_cases, 1)로 순회 |
| 3 | 진행 상황 로그 | "[{i}/{len}] 평가 중..." |
| 4 | 개별 평가 수행 | self.evaluate() 호출 |
| 5 | 질문/답변 추가 | result에 question, answer 추가 |
| 6 | 결과 수집 | results.append(result) |
| 7 | 완료 로그 | "배치 평가 완료: {len} 결과" |

---

**`close` 메서드**:

**역할**: Logger 리소스 해제

**동작**: exp_manager가 없는 경우에만 logger.close() 호출

---

### 2. 평가 결과 저장

**파일 경로**: `src/evaluation/storage.py`

#### 2.1 코드 구조 설명

**필요 라이브러리**:
- 표준: `os`, `typing.List`, `typing.Dict`, `datetime`
- 서드파티: `psycopg2`, `dotenv.load_dotenv`

---

**함수 1: `_get_conn`**

**역할**: PostgreSQL 데이터베이스 연결

**연결 파라미터** (환경 변수):

| 환경 변수 | 기본값 | 설명 |
|----------|--------|------|
| POSTGRES_HOST | localhost | 데이터베이스 호스트 |
| POSTGRES_PORT | 5432 | 데이터베이스 포트 |
| POSTGRES_USER | postgres | 사용자명 |
| POSTGRES_PASSWORD | (empty) | 비밀번호 |
| POSTGRES_DB | papers | 데이터베이스 이름 |

**반환값**: psycopg2 connection 객체

---

**함수 2: `create_evaluation_table`**

**역할**: evaluation_results 테이블 생성 (없을 경우)

**테이블 스키마**:

| 컬럼 | 타입 | 제약조건 | 설명 |
|-----|------|---------|------|
| eval_id | SERIAL | PRIMARY KEY | 평가 ID (자동 증가) |
| question | TEXT | NOT NULL | 사용자 질문 |
| answer | TEXT | NOT NULL | AI 답변 |
| accuracy_score | INT | CHECK (0-10) | 정확도 점수 |
| relevance_score | INT | CHECK (0-10) | 관련성 점수 |
| difficulty_score | INT | CHECK (0-10) | 난이도 적합성 점수 |
| citation_score | INT | CHECK (0-10) | 출처 명시 점수 |
| total_score | INT | CHECK (0-40) | 총점 |
| comment | TEXT | - | 평가 코멘트 |
| created_at | TIMESTAMP | DEFAULT CURRENT_TIMESTAMP | 생성 시간 |

**처리 흐름**:

| 단계 | 작업 |
|-----|------|
| 1 | DB 연결 (_get_conn()) |
| 2 | CREATE TABLE IF NOT EXISTS 실행 |
| 3 | 커밋 및 연결 종료 |

---

**함수 3: `save_evaluation_results`**

**역할**: 평가 결과를 PostgreSQL에 저장

**파라미터**:

| 파라미터 | 타입 | 설명 |
|---------|------|------|
| evaluation_results | List[Dict] | 평가 결과 리스트 (question, answer, 4개 점수, comment 포함) |

**처리 흐름**:

| 단계 | 작업 | 설명 |
|-----|------|------|
| 1 | DB 연결 | _get_conn() |
| 2 | 테이블 생성 확인 | create_evaluation_table() |
| 3 | 결과 리스트 순회 | for result in evaluation_results |
| 4 | INSERT 실행 | 각 결과를 evaluation_results 테이블에 삽입 |
| 5 | 커밋 및 종료 | conn.commit(), cursor/conn close |

**INSERT 컬럼**: question, answer, accuracy_score, relevance_score, difficulty_score, citation_score, total_score, comment

---

**함수 4: `get_evaluation_results`**

**역할**: 최근 평가 결과 조회

**파라미터**:

| 파라미터 | 타입 | 기본값 | 설명 |
|---------|------|--------|------|
| limit | int | 10 | 조회 개수 |

**반환값**: `List[Dict]` - 평가 결과 리스트 (eval_id, question, answer, 4개 점수, comment, created_at 포함)

**처리 흐름**:

| 단계 | 작업 | 설명 |
|-----|------|------|
| 1 | DB 연결 | _get_conn() |
| 2 | SELECT 쿼리 실행 | ORDER BY created_at DESC LIMIT {limit} |
| 3 | 결과 조회 | cursor.fetchall() |
| 4 | 딕셔너리 변환 | 각 row를 딕셔너리로 변환 (10개 컬럼) |
| 5 | 연결 종료 및 반환 | cursor/conn close, return results |

---

**함수 5: `get_evaluation_statistics`**

**역할**: 평가 통계 조회

**반환값**: `Dict` - 평가 통계 (total_count, avg_accuracy, avg_relevance, avg_difficulty, avg_citation, avg_total)

**처리 흐름**:

| 단계 | 작업 | 설명 |
|-----|------|------|
| 1 | DB 연결 | _get_conn() |
| 2 | 통계 쿼리 실행 | SELECT COUNT(*), AVG(점수들) FROM evaluation_results |
| 3 | 결과 조회 | cursor.fetchone() |
| 4 | 통계 딕셔너리 생성 | 각 평균값 소수점 2자리로 반올림 |
| 5 | 연결 종료 및 반환 | cursor/conn close, return stats |

**통계 항목**:

| 키 | 설명 | 처리 |
|---|------|------|
| total_count | 총 평가 개수 | COUNT(*) |
| avg_accuracy | 평균 정확도 | AVG(accuracy_score), round 2자리 |
| avg_relevance | 평균 관련성 | AVG(relevance_score), round 2자리 |
| avg_difficulty | 평균 난이도 적합성 | AVG(difficulty_score), round 2자리 |
| avg_citation | 평균 출처 명시 | AVG(citation_score), round 2자리 |
| avg_total | 평균 총점 | AVG(total_score), round 2자리 |

---

### 3. 평가 스크립트

**파일 경로**: `scripts/evaluate_answers.py`

#### 3.1 스크립트 구조 설명

**필요 라이브러리**:
- 표준: `json`
- 프로젝트: `src.evaluation.evaluator.AnswerEvaluator`, `src.evaluation.storage.save_evaluation_results`, `src.evaluation.storage.get_evaluation_statistics`, `src.utils.experiment_manager.ExperimentManager`

---

**실행 흐름**:

| 단계 | 작업 | 설명 |
|-----|------|------|
| 1 | ExperimentManager 초기화 | with ExperimentManager() as exp |
| 2 | 시작 로그 | "답변 평가 스크립트 실행" |
| 3 | AnswerEvaluator 생성 | AnswerEvaluator(exp_manager=exp) |
| 4 | 테스트 케이스 정의 | 2개 샘플 (Transformer Easy, BERT Hard) |
| 5 | 배치 평가 수행 | evaluator.evaluate_batch(test_cases) |
| 6 | 결과 저장 | save_evaluation_results(results) |
| 7 | 통계 조회 및 로그 | get_evaluation_statistics() |
| 8 | 개별 결과 출력 | 각 결과의 점수 및 코멘트 로그 |
| 9 | 완료 로그 | "평가 완료" |

---

**테스트 케이스 구조**:

각 테스트 케이스는 다음 필드 포함:

| 필드 | 타입 | 설명 | 예시 |
|-----|------|------|------|
| question | str | 사용자 질문 | "Transformer 논문 설명해줘" |
| answer | str | AI 답변 | "Transformer는 2017년..." |
| reference_docs | str | 참고 문서 | "Attention Is All You Need (Vaswani et al., 2017)" |
| difficulty | str | 난이도 | "easy" 또는 "hard" |

---

**출력 형식**:

각 테스트 결과는 다음 정보를 로그에 출력:

| 항목 | 형식 |
|-----|------|
| 질문 | "질문: {result['question']}" |
| 총점 | "총점: {total_score}/40" |
| 정확도 | "- 정확도: {accuracy_score}/10" |
| 관련성 | "- 관련성: {relevance_score}/10" |
| 난이도 적합성 | "- 난이도 적합성: {difficulty_score}/10" |
| 출처 명시 | "- 출처 명시: {citation_score}/10" |
| 코멘트 | "코멘트: {comment}" |

---

## 참고 PRD 문서

아래 PRD 문서들을 참고하여 구현하세요:

1. **[09_평가_기준.md](../PRD/09_평가_기준.md)** ⭐⭐⭐
   - RAG 평가 지표 (Recall@K, Faithfulness 등)
   - LLM-as-a-Judge 평가 방법
   - 평가 기준 및 목표 값

2. **[07_성능_개선_전략.md](../PRD/07_성능_개선_전략.md)** ⭐⭐
   - 캐싱 전략
   - 비동기 처리
   - DB 최적화

3. **[담당역할_05_추가선택기능.md](./담당역할_05_추가선택기능.md)** ⭐⭐
   - 성능 평가 시스템 설명 (196-411줄)
   - 예제 코드 및 사용법

---

## 협업 방법

### 다른 담당자와의 협업

#### 1. 전체 팀원 - 평가 결과 활용
- **협업 내용**: 답변 품질 개선을 위한 평가 결과 활용
- **타이밍**: 평가 시스템 구현 완료 후
- **전달 사항**:
  - AnswerEvaluator 사용법
  - 평가 결과 조회 방법
  - 평가 통계 해석 방법

---

## 구현 체크리스트

### Phase 1: AnswerEvaluator 클래스 구현 (우선순위 1)
- [x] `src/evaluation/evaluator.py` 파일 생성
- [x] LLM-as-a-Judge 프롬프트 설계
- [x] AnswerEvaluator 클래스 구현
  - [x] `__init__()` 메서드
  - [x] `evaluate()` 메서드
  - [x] `evaluate_batch()` 메서드
  - [x] `close()` 메서드
- [x] JSON 파싱 및 오류 처리

### Phase 2: 평가 결과 저장 (우선순위 2)
- [x] `src/evaluation/storage.py` 파일 생성
- [x] evaluation_results 테이블 생성
- [x] `save_evaluation_results()` 함수 구현
- [x] `get_evaluation_results()` 함수 구현
- [x] `get_evaluation_statistics()` 함수 구현

### Phase 3: 평가 스크립트 (우선순위 3)
- [x] `scripts/evaluate_answers.py` 스크립트 작성
- [x] 테스트 케이스 정의
- [x] 배치 평가 수행
- [x] 평가 결과 저장 및 출력

### Phase 4: Streamlit UI 연동 (선택)
- [ ] 평가 결과 표시 페이지 (선택)
- [ ] 평가 통계 시각화 (선택)

---

## 규칙 및 주의사항

### 1. 평가 기준
- **정확도 (0-10점)**: 참고 문서의 내용과 일치하는가?
- **관련성 (0-10점)**: 질문과 답변이 관련있는가?
- **난이도 적합성 (0-10점)**: 난이도 모드에 맞는 답변인가?
- **출처 명시 (0-10점)**: 논문 제목, 저자를 명시했는가?

### 2. 코드 품질
- 모든 함수에 한글 주석 작성 (docs/rules/annotate_style.md 준수)
- 섹션 구분선 사용 (등호 20개, 대시 22개)
- 함수별 상세 설명
- 로직 블록별 설명

### 3. 에러 처리
- JSON 파싱 실패 시 기본 점수 0 반환
- LLM 호출 실패 시 재시도 로직 (선택)
- DB 연결 실패 시 상세 에러 메시지

### 4. 성능 고려사항
- 배치 평가 시 병렬 처리 (선택)
- DB Connection Pool 사용
- 평가 결과 캐싱 (선택)

---

## 예상 결과물

### 1. 파일 구조

```
src/
└── evaluation/
    ├── __init__.py                # 모듈 초기화 (신규)
    ├── evaluator.py               # AnswerEvaluator 클래스 (신규)
    └── storage.py                 # 평가 결과 저장 (신규)

scripts/
└── evaluate_answers.py            # 평가 스크립트 (신규)
```

### 2. 사용 예시

**기본 사용 패턴:**

**단계별 사용법**:

| 단계 | 작업 | 코드/설명 |
|-----|------|----------|
| 1 | 모듈 임포트 | `from src.evaluation.evaluator import AnswerEvaluator`<br>`from src.evaluation.storage import save_evaluation_results` |
| 2 | Evaluator 초기화 | `evaluator = AnswerEvaluator()` |
| 3 | 답변 평가 수행 | `result = evaluator.evaluate(question, answer, reference_docs, difficulty)` |
| 4 | 결과 확인 | result 딕셔너리에서 점수 및 코멘트 확인 |
| 5 | 결과 저장 | `save_evaluation_results([result])` |

**evaluate() 파라미터**:

| 파라미터 | 예시 값 |
|---------|---------|
| question | "Transformer 논문 설명해줘" |
| answer | "Transformer는 2017년 Google에서 발표한..." |
| reference_docs | "Attention Is All You Need (Vaswani et al., 2017)" |
| difficulty | "easy" |

**결과 출력 항목**:
- `total_score`: 총점 (0-40)
- `accuracy_score`: 정확도 (0-10)
- `relevance_score`: 관련성 (0-10)
- `difficulty_score`: 난이도 적합성 (0-10)
- `citation_score`: 출처 명시 (0-10)
- `comment`: 평가 코멘트

**배치 평가 패턴:**

**단계별 사용법**:

| 단계 | 작업 | 설명 |
|-----|------|------|
| 1 | 테스트 케이스 리스트 생성 | 각 케이스는 question, answer, reference_docs, difficulty 포함 |
| 2 | 배치 평가 수행 | `results = evaluator.evaluate_batch(test_cases)` |
| 3 | 결과 저장 | `save_evaluation_results(results)` |

**테스트 케이스 형식**:

각 케이스는 딕셔너리 형태:

| 필드 | 타입 | 필수 여부 |
|-----|------|----------|
| question | str | 필수 |
| answer | str | 필수 |
| reference_docs | str | 선택 (기본값: "") |
| difficulty | str | 선택 (기본값: "easy") |

**반환값**: 평가 결과 리스트 (각 항목에 question, answer 추가됨)

**평가 통계 조회 패턴:**

**사용법**:

| 단계 | 작업 | 코드 |
|-----|------|------|
| 1 | 모듈 임포트 | `from src.evaluation.storage import get_evaluation_statistics` |
| 2 | 통계 조회 | `stats = get_evaluation_statistics()` |
| 3 | 결과 확인 | stats 딕셔너리에서 통계값 확인 |

**반환 딕셔너리 구조**:

| 키 | 타입 | 설명 | 범위 |
|---|------|------|------|
| total_count | int | 총 평가 개수 | 0 이상 |
| avg_accuracy | float | 평균 정확도 | 0-10 (소수점 2자리) |
| avg_relevance | float | 평균 관련성 | 0-10 (소수점 2자리) |
| avg_difficulty | float | 평균 난이도 적합성 | 0-10 (소수점 2자리) |
| avg_citation | float | 평균 출처 명시 | 0-10 (소수점 2자리) |
| avg_total | float | 평균 총점 | 0-40 (소수점 2자리) |

---

## 추가 참고 사항

### 1. evaluation_results 테이블 스키마

```sql
CREATE TABLE IF NOT EXISTS evaluation_results (
    eval_id SERIAL PRIMARY KEY,
    question TEXT NOT NULL,
    answer TEXT NOT NULL,
    accuracy_score INT CHECK (accuracy_score >= 0 AND accuracy_score <= 10),
    relevance_score INT CHECK (relevance_score >= 0 AND relevance_score <= 10),
    difficulty_score INT CHECK (difficulty_score >= 0 AND difficulty_score <= 10),
    citation_score INT CHECK (citation_score >= 0 AND citation_score <= 10),
    total_score INT CHECK (total_score >= 0 AND total_score <= 40),
    comment TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 2. 환경 변수 설정

`.env` 파일에 필요한 환경 변수:
```
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password
POSTGRES_DB=papers

OPENAI_API_KEY=your_openai_key
```

---

## 완료 일자

- **구현 완료**: 2025-11-04
- **테스트 완료**: 2025-11-04
- **문서화 완료**: 2025-11-04
