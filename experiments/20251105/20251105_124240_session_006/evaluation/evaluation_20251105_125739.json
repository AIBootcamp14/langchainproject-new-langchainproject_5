{
  "accuracy_score": 9,
  "relevance_score": 9,
  "difficulty_score": 10,
  "citation_score": 9,
  "total_score": 37,
  "comment": "전반적으로 2023–2024 자기지도학습 핵심 논문과 방법(대조/마스킹/JEPA)을 모달리티별로 정확하게 요약했습니다. DINOv2, I-JEPA, VideoMAE v2, V-JEPA, data2vec 2.0, ImageBind 등 주요 작업의 설명과 특징이 사실과 부합하고, 목적함수/복잡도/실무 팁과 간단 코드까지 제공해 전문성이 높습니다. 다만 일부 항목(MAGE, SigLIP 2 등)은 최신성/정확성 확인이 필요한 부분이 있고, 멀티모달 항목 중 일부는 완전 자기지도는 아님을 언급했지만 경계 사례가 포함되었습니다. 질문에 비해 내용이 매우 방대해 약간의 과잉 정보가 있으나 관련성은 높습니다. 인용은 많은 항목에 제목·저자·연도가 명시되어 우수하나, 일부는 저자/연도 표기가 불완전하여 만점에는 미치지 못합니다.",
  "question": "2023년 이후 Self-supervised learning 관련 논문 찾아줘",
  "answer": "### 중급자용 (23-30세)\n\n아래는 2023년 이후(Self-supervised learning, SSL) 주요 논문들을 모달리티별로 정리한 큐레이션입니다. 각 방법의 핵심 아이디어(대조·마스킹·JEPA 등), 장단점, 실무 적용 포인트와 함께, 핵심 손실함수의 파이토치 예제 코드도 포함했습니다. 필요하시면 특정 도메인(비전·오디오·비디오·시계열·의료 등)으로 좁혀 상세 리뷰/비교표/재현용 스크립트를 드릴 수 있습니다.\n\n1) 이미지(비전) SSL\n- DINOv2: Learning Robust Visual Features without Supervision (Meta, 2023)\n  - 요지: ViT를 교사-학생 자가증류(EMA teacher, centering, sharpening, multi-crop)로 학습. 대규모 무라벨 데이터(LVD-142M) 큐레이션과 스케일링 레시피로 선형 프로빙/전이 성능 대폭 향상.\n  - 방법: 비대칭 teacher-student, 다중뷰 증강, non-contrastive(분산/정규화로 collapse 방지).\n  - 장점: 강력한 전이, 다운스트림에서 적은 라벨로도 우수. 단점: 증강/하이퍼 파고들기와 대규모 데이터·연산 필요.\n  - 코드: facebookresearch/dinov2 (공식)\n\n- I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (Meta/LeCun, 2023; ICLR 2024)\n  - 요지: JEPA(공동 임베딩 예측). 픽셀 복원 대신 잠재공간 예측. 컨텍스트 패치로부터 가려진 타깃 패치의 representation을 직접 예측.\n  - 방법: target encoder(gradient stop) + context encoder + predictor; 마스킹된 블록에 대해 L2/코사인 손실. 대조학습/픽셀 재구성 없이 collapse를 구조적으로 방지.\n  - 장점: 적은 증강 의존, 저주파 복원 편향 없음, 스케일 효율. 단점: 구현 복잡도와 마스킹 설계 감도.\n  - 코드: facebookresearch/ijepa\n\n- MAGE: Masked Generative Encoder (2023, NeurIPS)\n  - 요지: 마스킹 기반 생성적 프리텍스트와 판별적 인코더의 결합. MIM(픽셀/feature 복원)과 contrastive의 장점 절충.\n  - 장점: 다양한 다운스트림에 안정적. 단점: 학습 레시피가 다소 복잡.\n\n- 추가: Masked Feature Prediction 개선 계열(예: MaskFeat 계열 후속, SimMIM 계열 후속), SemMAE/MAE 변형(semantic-aware masking 등)도 2023~24에 다수 보고.\n\n2) 비디오 SSL\n- VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking and Efficient Sampling (2023)\n  - 요지: VideoMAE(2022)의 확장. 시공간 이중 마스킹, 효율적 프레임 샘플링, 더 큰 모델/데이터로 스케일.\n  - 장점: 영상 인식/탐지/행동분류 전반에 강함. 단점: GPU 메모리·I/O 부담 큼.\n  - 코드: MCG-NJU/VideoMAE\n\n- V-JEPA: Joint-Embedding Predictive Architecture for Video (Meta, 2024)\n  - 요지: 영상에 JEPA를 적용. 현재/미래 스패티오-템포럴 토큰 잠재를 직접 예측.\n  - 장점: 장면 동역학 학습에 유리, 픽셀 복원 대비 고수준 표현. 단점: 마스킹/뷰 설계가 성능에 민감.\n\n3) 오디오/음성 SSL\n- BEATs: Audio Pre-Training with Acoustic Tokenizers (Microsoft, 2023)\n  - 요지: teacher 모델이 만든 음향 토큰/feature를 예측하는 BERT-스타일 프리트레이닝.\n  - 장점: 라벨 없이 ASR/음향이벤트/스피커 태스크 전이 우수. 단점: teacher 의존성·토크나이저 편향.\n  - 코드: microsoft/BEATs\n\n- data2vec 2.0 (Meta, 2023)\n  - 요지: 음성·비전·텍스트 공통 JEPA류 프레임워크. teacher의 잠재표현을 학생이 예측.\n  - 장점: 멀티모달 범용 SSL, 효율적. 단점: 구현·튜닝 복잡.\n\n4) 멀티모달 SSL\n- ImageBind: One Embedding Space To Bind Them All (Meta, 2023)\n  - 요지: 이미지-텍스트-오디오-Depth-IMU-열화상 등 이질 모달을 라벨 없이 동시 임베딩 정렬. 공존(co-occurrence) 기반 대조/정렬.\n  - 장점: 제로샷 cross-modal retrieval/전이. 단점: 데이터 소스 품질·편향 관리 필요.\n  - 코드: facebookresearch/ImageBind\n\n5) 3D/포인트클라우드 SSL\n- Point-MAE v2 / 3D MAE 변형들 (2023)\n  - 요지: 포인트클라우드에 마스킹-복원 프레임워크 확장, sparse/voxel 혼합 토큰화, 인코더 경량화.\n  - 장점: 라벨 부족한 3D 도메인에 효과적. 단점: 데이터 전처리/샘플링 설계 난이도.\n\n핵심 알고리즘 분류와 비교\n- Contrastive(InfoNCE)\n  - 아이디어: 같은 인스턴스의 증강 뷰는 끌어당기고(positive), 다른 인스턴스는 밀어냄(negative).\n  - 장점: 강한 선형 프로빙, 시멘틱 클러스터링. 단점: 대배치/메모리뱅크 필요, 증강 의존도 높음.\n  - 손실: L = -log exp(sim(q,k+)/τ) / Σ_j exp(sim(q,k_j)/τ)\n\n- Masked Image/Video Modeling(MIM/MVM)\n  - 아이디어: 입력 일부 마스킹 후 픽셀/feature 복원. MAE는 경량 decoder, heavy masking(60~90%).\n  - 장점: 대배치/네거티브 불필요, 안정적 스케일링. 단점: 저수준 복원 편향, 고수준 의미 학습 한계.\n\n- Non-contrastive Distillation(BYOL/DINO/DINOv2)\n  - 아이디어: teacher-student로 같은 이미지 뷰 간 임베딩 일치. EMA/정규화로 collapse 방지.\n  - 장점: 네거티브 불필요, 강한 전이. 단점: 레시피 복잡, 증강 설계 민감.\n\n- JEPA(I-JEPA/V-JEPA, data2vec 계열)\n  - 아이디어: 보이는 컨텍스트로 가려진 타깃의 잠재표현을 직접 예측(픽셀 복원·네거티브 불필요).\n  - 장점: 의미 표현에 집중, 증강 의존 낮음, 스케일 효율. 단점: 마스킹/뷰/아키텍처 설계 중요.\n\n실무 적용 시 고려사항\n- 데이터: 대규모 무라벨 수집 시 중복 제거나 품질 필터링(DINOv2는 aesthetic/near-dup/NSFW 필터) 성능 영향 큼.\n- 증강: contrastive는 강한 컬러/크롭/멀티크롭, MIM/JEPA는 과도한 색 왜곡 지양, 공간·블록 마스킹 비율(이미지 40~75%, 비디오 시공간 혼합).\n- 모델/옵티마이저: ViT-B/L/H + AdamW, weight decay 0.05~0.2, cosine schedule, warmup; DINO류는 teacher EMA 0.996~0.9995.\n- 배치/분산: contrastive는 큰 글로벌 배치(>= 1024) 유리, MIM/JEPA는 상대적으로 덜 민감하나 I/O와 마스크 샘플링 효율 중요.\n- 평가: 선형 프로빙 vs 풀 파인튜닝, k-NN, 검출/세그/키포인트/비디오 전이. 도메인 특화(의료/위성/산업)일수록 파인지닝 전략이 성능 좌우.\n\n핵심 손실/학습 루프 미니멀 PyTorch 예시\n\n1) Contrastive InfoNCE (SimCLR식)\nimport torch\nimport torch.nn.functional as F\n\ndef info_nce_loss(z1, z2, temperature=0.1):\n    # z1, z2: [B, D], L2 normalize\n    z1 = F.normalize(z1, dim=-1)\n    z2 = F.normalize(z2, dim=-1)\n    B = z1.size(0)\n    reps = torch.cat([z1, z2], dim=0)                     # [2B, D]\n    sim = reps @ reps.t() / temperature                   # [2B, 2B]\n    mask = torch.eye(2*B, dtype=torch.bool, device=sim.device)\n    sim.masked_fill_(mask, -1e9)\n\n    # positives are on (i, i+B) and (i+B, i)\n    pos = torch.cat([torch.arange(B)+B, torch.arange(B)], dim=0).to(sim.device)\n    target = pos\n    loss = F.cross_entropy(sim, target)\n    return loss\n\n2) MAE식 마스킹 복원(MSE on pixels or features)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleMAE(nn.Module):\n    def __init__(self, encoder, decoder, patch_embed, mask_ratio=0.75):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.patch_embed = patch_embed\n        self.mask_ratio = mask_ratio\n\n    def forward(self, imgs):\n        # patchify\n        patches = self.patch_embed(imgs)    # [B, N, D]\n        B, N, D = patches.shape\n\n        # random mask\n        num_mask = int(self.mask_ratio * N)\n        idx = torch.rand(B, N, device=imgs.device).argsort(dim=1)\n        keep_idx = idx[:, :-num_mask]\n        mask_idx = idx[:, -num_mask:]\n\n        patches_keep = torch.gather(patches, 1, keep_idx[..., None].expand(-1, -1, D))\n        enc = self.encoder(patches_keep)    # [B, Nk, De]\n\n        # decode to reconstruct masked tokens\n        dec_in = self._insert_mask_tokens(enc, keep_idx, N)\n        rec = self.decoder(dec_in)          # [B, N, patch_dim]\n        # compute loss only on masked tokens\n        target = self._patchify_pixels(imgs)  # [B, N, patch_dim]\n        target_masked = torch.gather(target, 1, mask_idx[..., None].expand(-1, -1, target.size(-1)))\n        rec_masked = torch.gather(rec, 1, mask_idx[..., None].expand(-1, -1, rec.size(-1)))\n        loss = F.mse_loss(rec_masked, target_masked)\n        return loss\n\n    def _insert_mask_tokens(self, enc, keep_idx, N):\n        # 구현 단순화: 자리 채우기용 mask token 생성 후 원래 순서로 복원\n        B, Nk, De = enc.shape\n        mask_tokens = enc.new_zeros(B, N - Nk, De)\n        tokens = torch.cat([enc, mask_tokens], dim=1)\n        # 복원 인덱스는 생략(학습 코드에서는 복원 permutation 필요)\n        return tokens\n\n3) I-JEPA식 잠재 예측(타깃 임베딩 예측, stop-grad)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass IJEPALoss(nn.Module):\n    def __init__(self, context_encoder, target_encoder, predictor, proj_dim=1024, mask_generator=None):\n        super().__init__()\n        self.context_encoder = context_encoder\n        self.target_encoder = target_encoder\n        self.predictor = predictor\n        self.mask_generator = mask_generator\n\n    @torch.no_grad()\n    def _stop_grad(self, x):\n        return x.detach()\n\n    def forward(self, img):\n        # generate block-wise masks: visible(ctx_mask) and target blocks(tgt_mask)\n        ctx_mask, tgt_mask = self.mask_generator(img)   # boolean masks over patch grid\n        # encode\n        z_ctx = self.context_encoder(img, ctx_mask)     # [B, N_ctx, D]\n        with torch.no_grad():\n            z_tgt = self._stop_grad(self.target_encoder(img, tgt_mask))  # [B, N_tgt, D]\n        # predict target embeddings from context tokens\n        pred = self.predictor(z_ctx, tgt_mask)          # [B, N_tgt, D]\n        # cosine or L2 loss on target blocks\n        pred = F.normalize(pred, dim=-1)\n        z_tgt = F.normalize(z_tgt, dim=-1)\n        loss = 2 - 2 * (pred * z_tgt).sum(dim=-1).mean()\n        return loss\n\n비교 요약\n- 대규모 이미지 표현: DINOv2(자가증류)와 I-JEPA(JEPA)가 2023~24 기준 강력한 축. 대조학습 대비 적은 증강 의존/스케일 효율이 장점.\n- 영상: VideoMAE v2(마스킹 복원) vs V-JEPA(잠재 예측). 동역학/미래예측이 중요하면 V-JEPA가 유리, 넓은 벤치마크 커버리지는 VideoMAE v2가 안전한 선택.\n- 오디오/음성: BEATs, data2vec 2.0이 범용 전이 성능 우수.\n- 멀티모달: ImageBind가 라벨 없이 크로스모달 임베딩 정렬의 대표작.\n\n참고(검색 키워드와 공식 저장소)\n- DINOv2 — “DINOv2: Learning robust visual features without supervision”, GitHub: facebookresearch/dinov2\n- I-JEPA — “Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture”, GitHub: facebookresearch/ijepa\n- V-JEPA — “Joint-Embedding Predictive Architecture for Video”, Meta AI resources\n- VideoMAE V2 — “Scaling Video Masked Autoencoders…”, GitHub: MCG-NJU/VideoMAE\n- BEATs — “Audio Pre-Training with Acoustic Tokenizers”, GitHub: microsoft/BEATs\n- data2vec 2.0 — fairseq examples: facebookresearch/fairseq\n- ImageBind — “One Embedding Space to Bind Them All”, GitHub: facebookresearch/ImageBind\n- MAGE — “Masked Generative Encoder”, 검색 키워드: “MAGE NeurIPS 2023”\n\n원하시면\n- 특정 모달리티(예: 의료 영상, 위성/원격탐사, 로보틱스) 중심으로 2023~2024 논문만 골라, 벤치마크 점수/학습 레시피/리소스 요구사항을 표로 정리하고\n- BibTeX와 함께 재현 코드 스캐폴드(train.py, config, 데이터 파이프라인)까지 제공해 드릴게요. 어느 분야를 더 깊게 보길 원하시나요?\n\n---\n\n### 고급자용 (30세 이상)\n\n요청 범위 메모: 제 지식은 2024-10까지의 공개 논문을 포함합니다. 아래는 2023년 이후(= 2023–2024 기간) Self-Supervised Learning(SSL)의 핵심 논문과 동향을 분야별로 정리한 것입니다. 2025년 최신 업데이트나 특정 하위분야(비전/음성/그래프/시계열 등)로 더 좁혀 드리길 원하시면 알려주세요.\n\n핵심 논문(2023–2024)\n- Vision (이미지)\n  - Assran et al., 2023, I-JEPA: Image-based Joint-Embedding Predictive Architecture. 예측적 자기지도(복원 대신 잠재표현 예측) 패러다임을 정립. 대규모 ViT에서 라벨 없이 강력한 선형 프로브/전이 성능 보고.\n  - Oquab et al., 2023, DINOv2: 대규모 비전 자기지도 사전학습의 강력한 베이스라인. 라벨 없는 데이터 큐레이션+ViT 스케일링으로 분류/검출/세그먼테이션 전이에서 SOTA급.\n  - Bao et al., 2023, CAE v2: Masked Autoencoder(MAE) 계열의 문맥 예측 강화. 재구성 편향을 줄이고 전이 성능 개선.\n- Vision (비디오)\n  - Wang et al., 2023, VideoMAE v2: 비디오용 MAE의 스케일링/효율화. Kinetics, Something-Something 등에서 비지도 사전학습 후 SOTA 전이 성능.\n  - Assran et al., 2024, V-JEPA: 비디오용 JEPA. 시간적 예측을 통해 단기 변형에는 불변, 장기 변화에는 민감한 표현을 학습.\n- 멀티모달(비전-언어; 약지도/자기지도 경계)\n  - Fang et al., 2023, EVA-CLIP: 대규모 이미지-텍스트 대조학습 스케일링과 로버스트니스 개선.\n  - Zhai et al., 2024, SigLIP 2: CLIP 대비 단순한 시그모이드 대조목적과 데이터/모델 스케일링으로 멀티모달 전반 성능 향상. (텍스트 짝 데이터를 쓰므로 “완전” 자기지도는 아님)\n- 음성/오디오\n  - Pratap et al., 2023, MMS: 초대규모 자기지도 음성 사전학습으로 다언어 인식/합성 기초모델. 레이블 희소 언어에서 강점.\n- 그래프\n  - Hou et al., 2023, GraphMAE2: 그래프용 마스킹-복원 기반 SSL의 안정성/전이 성능 강화(노드·링크 예측, 분류 전이).\n- 시계열\n  - (대표 동향) Masked reconstruction·변형-불변 대조학습·예측적(autoregressive/JEPA류) 목적의 스케일링. 도메인(의료/산업) 특화 마스킹 설계가 성능 좌우.\n\n핵심 목적함수 개요(수식)와 복잡도\n- 대조학습(InfoNCE; SimCLR/CLIP 계열)\n  - LInfoNCE = − Σi log [ exp(sim(z_i, z_i+)/τ) / Σj exp(sim(z_i, z_j)/τ) ], sim(u,v)=u^T v / (||u||·||v||)\n  - 배치당 복잡도: 유사도 계산 O(B^2 d), 정규화 O(B^2). 메모리 뱅크/모멘텀 인코더(MoCo류)로 음수 샘플 수를 확장.\n  - 장점: 불변 표현에 강함. 단점: 양/음수 정의 민감, O(B^2) 스케일 병목.\n- 마스킹-복원(MAE/CAE/VideoMAE)\n  - 입력 x를 마스크 M로 부분 관측 xM, 인코더 f로 잠재 h=f(xM), 디코더 g로 복원 x̂=g(h); Lrec = ||x − x̂||_p (대개 p=2) 또는 지각적 손실.\n  - 시퀀스 길이 N, 잠재 차원 d, 마스크율 m일 때, 인코더 FLOPs ~ O(B (1−m) N d^2) (ViT 기준), 디코더 O(B N d^2), m↑로 인코더 비용↓. 고해상도/장시퀀스에 효율적.\n  - 장점: 음수 샘플 불필요, 스케일에 유리. 단점: 픽셀/스펙트럼 복원 편향.\n- JEPA(I/V-JEPA)\n  - 컨텍스트 c와 타깃 t를 분할(공간/시간). 타깃 임베딩 y = stopgrad(ϕ(t)), 예측 p = ψ(f(c)); 손실 LJEPA = ||p − y||_2^2 (또는 cosine/MSE). 복원 대신 잠재예측이므로 저주파 수월성/저수준 복원 편향 감소.\n  - 복잡도: 인코더 두 번(컨텍스트/타깃) O(BNd^2) + 예측기 ψ의 O(Bd^2); 대조학습처럼 O(B^2) 유사도 계산이 없어 대규모 배치에 유리.\n  - 이론적 관점: mutual information 최대화 대신 예측가능한 부분만 보존하는 추상화 가설(LeCun, 2022 제안)을 실증.\n\n실험·벤치마크 요약(주요 경향)\n- DINOv2(Oquab et al., 2023): ImageNet-1K 선형 프로브·k-NN에서 강력, COCO/Cityscapes/ ADE20K 전이에서 감독 대비 경쟁력 보고. 데이터 큐레이션·학습 레시피가 결정적.\n- I-JEPA(Assran et al., 2023): ImageNet 선형 프로브에서 MAE 대비 우수, 적은 학습 스텝으로 수렴. 디코더가 없는 구조로 추론 효율성↑.\n- VideoMAE v2(Wang et al., 2023): Kinetics-400/600, Something-Something V2에서 자기지도 사전학습 후 미세조정 시 SOTA 혹은 준SOTA. 높은 마스크율과 sparse tube 샘플링이 효과적.\n- V-JEPA(Assran et al., 2024): 비디오에서 시간적 예측으로 행동인식·템포럴 전이에서 강함. 복원 없이 잠재예측이라 계산·스케일 효율.\n- EVA-CLIP(Fang et al., 2023), SigLIP 2(Zhai et al., 2024): 이미지-텍스트 대조에서 제로샷/리트리벌/VL 전이 개선. 완전 자기지도는 아니지만 대조목적 설계의 최신 추세를 대변.\n\n최근 연구 동향 비교\n- 복원(reconstruction) → 예측(prediction)으로: MAE 계열의 한계를 줄이고 JEPA류가 고수준 의미표현에 유리함을 보임(Assran et al., 2023/2024).\n- 불변(invariance) + 등변(equivariance) 공존: 순수 불변 대조의 한계를 보완하려 회전/기하 변환에 대한 등변 제약을 추가하는 SSL이 확산(예: equivariant regularization, 2023–2024 다수 후속).\n- 비디오·시계열로 확장: 높은 마스크율·튜브 마스킹, 장문맥 모델링(메모리/스트라이드) 등 효율 중심 설계.\n- 데이터 스케일·큐레이션의 중요성: DINOv2는 레시피·필터링이 핵심임을 보였고, 멀티모달(이미지-텍스트)도 캡션 품질/필터가 성능 좌우.\n- 평가 프로토콜의 다변화: 선형 프로브, 저샷, 분류 외 검출/세그/리트리벌/로버스트니스/OOD/공정성 등 다면 평가.\n\n알고리즘 복잡도 비교(요지)\n- 대조학습: O(B^2 d) 유사도 → 큰 B에서 병목. 메모리/시간 비용 큼.\n- MAE/CAE: O(B (1−m) N d^2) 인코더 우세, m↑로 비용 감소. 고해상도 시 효율적.\n- JEPA: O(B N d^2), pairwise 유사도 없음. 디코더 제거로 메모리·시간 효율. 비디오 확장 시 튜브 샘플링으로 시간축 비용 제어.\n\n미해결 문제와 향후 연구 방향\n- 자기지도 평가 표준화: 전이 세팅/데이터 누수/하이퍼 최적화 공정성.\n- 등변성 학습의 일반해: 변환군(G) 전반에 대한 이론적 보장과 실용적 레시피 결합.\n- 장문맥 시공간 모델링: 수만 프레임/초장기 시계열에서의 메모리-계산 트레이드오프(부분 관측, 압축 상태, 기호적 요약).\n- 데이터 큐레이션·저작권/편향: “라벨 없는” 데이터의 편향 제어와 합법성.\n- 멀티모달에서의 “진정한” 자기지도: 텍스트 의존 없이 다감각 정렬을 학습하는 방법(예: ImageBind 계열의 확장).\n- 경량화·온디바이스 SSL: 사전학습·어댑테이션의 효율적 저자원화.\n\n참고 문헌(선정)\n- Assran, M. et al., 2023. Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (I-JEPA).\n- Assran, M. et al., 2024. V-JEPA: Self-Supervised Learning from Video with Joint-Embedding Prediction.\n- Oquab, M. et al., 2023. DINOv2: Learning Robust Visual Features without Supervision.\n- Bao, H. et al., 2023. CAE v2: Context Autoencoders for Self-Supervised Representation Learning.\n- Wang, J. et al., 2023. VideoMAE v2: Scaling Video Masked Autoencoders with Transformers.\n- Fang, Y. et al., 2023. EVA-CLIP: Improved Training of CLIP at Scale.\n- Zhai, X. et al., 2024. SigLIP 2: Multimodal Contrastive Learning with a Sigmoid Loss at Scale.\n- Pratap, V. et al., 2023. MMS: Scaling Speech Self-Supervision to Massive Multilingual Data.\n- Hou, Z. et al., 2023. GraphMAE2: Revisiting Masked Autoencoders for Graphs.\n\n원하시면\n- 특정 하위 분야(예: 비전-이미지 vs 비디오 vs 음성 vs 그래프 vs 시계열)\n- 과제(분류/검출/세그/리트리벌/로버스트니스)\n- 형식(BibTeX/CSV)과 함께, 2024년 논문 위주로 더 많은 타이틀과 링크, 간단 요약, 데이터셋·성능 표, 연산량(FLOPs/Throughput)까지 정리해 드리겠습니다.\n\n---",
  "reference_docs": "참고 문서 없음 (일반 답변)",
  "difficulty": "hard",
  "timestamp": "2025-11-05T12:57:39.600661"
}