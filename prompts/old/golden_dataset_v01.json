{
  "golden_dataset": {
    "basic_queries": [
      {
        "id": "basic_001",
        "question": "BERT가 뭐야?",
        "user_intent": "용어 정의 요청",
        "difficulty": "easy",
        "complexity": "low",
        "expected_tool": "glossary",
        "expected_answer": "BERT(Bidirectional Encoder Representations from Transformers)는 Google이 2018년에 발표한 언어 모델로, 양방향으로 문맥을 이해하는 것이 특징입니다. 마치 책을 읽을 때 앞뒤 문맥을 모두 고려하는 것처럼, 단어의 의미를 양쪽에서 파악합니다. (Devlin et al., 2018)",
        "reference_docs": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)",
        "expected_answer_keywords": ["양방향", "Transformer", "사전학습", "문맥", "Google"],
        "evaluation_criteria": {
          "must_include": ["양방향", "Transformer"],
          "should_include": ["사전학습", "Devlin"],
          "must_not_include": ["단방향", "autoregressive"],
          "citation_required": true
        },
        "category": "용어 정의",
        "tags": ["NLP", "언어모델", "기초개념"]
      },
      {
        "id": "basic_002",
        "question": "Self-Attention이 뭔지 쉽게 설명해줘",
        "user_intent": "복잡한 개념의 쉬운 설명 요청",
        "difficulty": "easy",
        "complexity": "medium",
        "expected_tool": "glossary",
        "expected_answer": "Self-Attention은 문장 내 각 단어가 다른 단어들과 얼마나 관련있는지를 계산하는 메커니즘입니다. 예를 들어 '그가 공을 찼다'라는 문장에서 '그'가 누구를 가리키는지 이해하려면 문장의 다른 단어들을 함께 봐야 하는데, Self-Attention이 이런 관계를 자동으로 파악합니다.",
        "reference_docs": "Attention is All You Need (Vaswani et al., 2017)",
        "expected_answer_keywords": ["관계", "가중치", "단어", "문맥"],
        "evaluation_criteria": {
          "must_include": ["관계", "가중치"],
          "should_include": ["예시", "비유"],
          "tone": "easy",
          "citation_required": false
        },
        "category": "용어 정의",
        "tags": ["Attention", "기초개념", "쉬운설명"]
      }
    ],

    "search_queries": [
      {
        "id": "search_001",
        "question": "RAG 관련 논문 찾아줘",
        "user_intent": "논문 검색 요청",
        "difficulty": "easy",
        "complexity": "low",
        "expected_tool": "search_paper",
        "expected_answer": "RAG(Retrieval-Augmented Generation) 관련 주요 논문을 찾았습니다:\n\n1. **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks** (Lewis et al., 2020) - RAG의 원조 논문으로, 검색과 생성을 결합한 방법론을 제안합니다.\n\n필요하신 특정 측면(성능 개선, 적용 사례 등)이 있으시면 말씀해주세요!",
        "reference_docs": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020)",
        "expected_answer_keywords": ["Retrieval", "Generation", "Lewis", "2020"],
        "evaluation_criteria": {
          "must_include": ["논문 제목", "저자"],
          "should_include": ["발행연도", "핵심 내용"],
          "citation_required": true
        },
        "category": "논문 검색",
        "tags": ["RAG", "논문검색"]
      },
      {
        "id": "search_002",
        "question": "2024년에 나온 Mixture of Experts 최신 연구 알려줘",
        "user_intent": "특정 기간의 최신 연구 검색",
        "difficulty": "hard",
        "complexity": "medium",
        "expected_tool": "web_search",
        "expected_answer": "2024년 Mixture of Experts(MoE) 관련 최신 연구를 검색했습니다:\n\n1. **Mixtral 8x7B** - 희소 MoE 아키텍처로 효율성을 크게 개선\n2. **MoE 스케일링 연구** - 더 많은 전문가 모델을 효과적으로 활용하는 방법\n\n(실제 검색 결과 기반 답변)",
        "reference_docs": "N/A - Web search required",
        "expected_answer_keywords": ["2024", "MoE", "Mixture of Experts", "최신"],
        "evaluation_criteria": {
          "must_include": ["2024", "최신"],
          "should_include": ["구체적 연구명"],
          "must_not_include": ["2023년 이전 정보만"],
          "citation_required": true
        },
        "category": "최신 정보",
        "tags": ["MoE", "최신연구", "웹검색"]
      }
    ],

    "comparison_queries": [
      {
        "id": "compare_001",
        "question": "BERT랑 GPT 차이가 뭐야?",
        "user_intent": "두 개념의 핵심 차이 비교",
        "difficulty": "hard",
        "complexity": "high",
        "expected_tool": "general",
        "expected_answer": "BERT와 GPT의 주요 차이점:\n\n**1. 학습 방향**\n- BERT: 양방향 (문장 전체를 동시에 고려)\n- GPT: 단방향 (왼쪽에서 오른쪽으로만)\n\n**2. 학습 목표**\n- BERT: Masked Language Modeling (빈칸 채우기)\n- GPT: Next Token Prediction (다음 단어 예측)\n\n**3. 주요 용도**\n- BERT: 분류, 질의응답 등 이해 task\n- GPT: 텍스트 생성, 대화 등 생성 task\n\nBERT는 'Devlin et al., 2018', GPT는 'Radford et al., 2018'에서 소개되었습니다.",
        "reference_docs": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)\nImproving Language Understanding by Generative Pre-Training (Radford et al., 2018)",
        "expected_answer_keywords": ["양방향", "단방향", "MLM", "autoregressive", "차이"],
        "evaluation_criteria": {
          "must_include": ["양방향", "단방향", "차이점"],
          "should_include": ["학습 방법", "용도", "두 논문 모두 인용"],
          "structure": "comparison_table_or_sections",
          "citation_required": true
        },
        "category": "개념 비교",
        "tags": ["BERT", "GPT", "비교분석"]
      },
      {
        "id": "compare_002",
        "question": "LoRA랑 Full Fine-tuning 중에 뭐가 더 좋아?",
        "user_intent": "방법론 비교 및 추천 요청",
        "difficulty": "hard",
        "complexity": "high",
        "expected_tool": "general",
        "expected_answer": "상황에 따라 다릅니다:\n\n**LoRA가 유리한 경우:**\n- 제한된 GPU 메모리 (LoRA는 학습 파라미터가 0.1-1%)\n- 여러 task에 빠르게 적용 (모델 교체 용이)\n- 비용 효율성 중시\n\n**Full Fine-tuning이 유리한 경우:**\n- 충분한 컴퓨팅 리소스\n- 최고 성능이 필수적인 경우\n- 도메인이 pre-training과 매우 다른 경우\n\n실무에서는 LoRA가 성능 손실이 1-2% 이내면서 효율이 높아 자주 사용됩니다. (Hu et al., 2021)",
        "reference_docs": "LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)",
        "expected_answer_keywords": ["LoRA", "Fine-tuning", "파라미터", "효율", "성능", "상황에 따라"],
        "evaluation_criteria": {
          "must_include": ["장단점", "상황별 추천"],
          "should_include": ["구체적 수치", "실무 관점"],
          "must_not_include": ["무조건적 추천"],
          "citation_required": true
        },
        "category": "개념 비교",
        "tags": ["LoRA", "Fine-tuning", "비교분석", "실무"]
      }
    ],

    "technical_queries": [
      {
        "id": "tech_001",
        "question": "Self-Attention의 시간 복잡도가 O(n²)인 이유를 수식으로 설명해줘",
        "user_intent": "기술적 원리의 수학적 설명 요청",
        "difficulty": "hard",
        "complexity": "high",
        "expected_tool": "general",
        "expected_answer": "Self-Attention의 시간 복잡도가 O(n²)인 이유:\n\n**수식:**\nAttention(Q, K, V) = softmax(QK^T / √d_k)V\n\n**복잡도 분석:**\n1. QK^T 계산: (n × d_k) × (d_k × n) = O(n² × d_k)\n2. softmax 적용: O(n²)\n3. V와 곱셈: (n × n) × (n × d_k) = O(n² × d_k)\n\n여기서 n은 시퀀스 길이, d_k는 차원입니다.\n\n**핵심:** 각 토큰(n개)이 모든 토큰(n개)과 attention 점수를 계산하므로 n × n = O(n²)입니다. 이것이 긴 시퀀스 처리 시 병목이 되는 이유입니다.\n\n(Vaswani et al., 2017)",
        "reference_docs": "Attention is All You Need (Vaswani et al., 2017)",
        "expected_answer_keywords": ["O(n²)", "QK^T", "시퀀스 길이", "수식"],
        "evaluation_criteria": {
          "must_include": ["O(n²)", "수식", "이유"],
          "should_include": ["단계별 설명", "n의 의미"],
          "tone": "hard",
          "citation_required": true
        },
        "category": "기술적 분석",
        "tags": ["복잡도", "수식", "Attention", "하드"]
      },
      {
        "id": "tech_002",
        "question": "Transformer가 RNN보다 나은 이유 3가지만 알려줘",
        "user_intent": "기술적 장점의 구조화된 설명 요청",
        "difficulty": "hard",
        "complexity": "high",
        "expected_tool": "general",
        "expected_answer": "Transformer가 RNN보다 우수한 3가지 이유:\n\n**1. 병렬 처리 가능**\n- RNN: 순차적 처리 필수 (t-1 계산 후 t 계산)\n- Transformer: 모든 위치를 동시에 계산 가능\n- 결과: 학습 속도 수십~수백 배 향상\n\n**2. Long-range Dependency 처리**\n- RNN: 멀리 떨어진 단어 관계 파악 어려움 (gradient vanishing)\n- Transformer: Self-Attention으로 거리 관계없이 직접 연결\n- 결과: 긴 문맥 이해 능력 향상\n\n**3. 표현력**\n- RNN: 고정된 hidden state 크기로 정보 압축\n- Transformer: Multi-head Attention으로 다양한 관점 동시 학습\n- 결과: 더 풍부한 표현 가능\n\n(Vaswani et al., 2017)",
        "reference_docs": "Attention is All You Need (Vaswani et al., 2017)",
        "expected_answer_keywords": ["병렬화", "long-range", "의존성", "속도"],
        "evaluation_criteria": {
          "must_include": ["3가지", "각각의 이유"],
          "should_include": ["구체적 설명", "결과/효과"],
          "structure": "numbered_list",
          "citation_required": true
        },
        "category": "개념 비교",
        "tags": ["Transformer", "RNN", "비교", "기술분석"]
      }
    ],

    "summarization_queries": [
      {
        "id": "summary_001",
        "question": "Attention Is All You Need 논문 핵심만 3문장으로 요약해줘",
        "user_intent": "논문의 간결한 요약 요청",
        "difficulty": "hard",
        "complexity": "high",
        "expected_tool": "summarize",
        "expected_answer": "1) Transformer는 RNN/CNN 없이 오직 Attention 메커니즘만으로 구성된 새로운 아키텍처입니다.\n\n2) Encoder-Decoder 구조에서 Multi-Head Self-Attention과 Position-wise FFN을 사용하여 병렬 처리가 가능하며, 번역 task에서 SOTA를 달성했습니다.\n\n3) 이 구조는 이후 BERT, GPT 등 현대 언어 모델의 기반이 되었습니다.\n\n(Vaswani et al., 2017)",
        "reference_docs": "Attention is All You Need (Vaswani et al., 2017)",
        "expected_answer_keywords": ["Transformer", "Attention", "Encoder-Decoder", "병렬", "Vaswani"],
        "evaluation_criteria": {
          "must_include": ["핵심 기여", "Attention only"],
          "should_include": ["영향력", "성능"],
          "max_sentences": 3,
          "citation_required": true
        },
        "category": "논문 요약",
        "tags": ["Transformer", "논문요약", "간결"]
      }
    ],

    "edge_cases": [
      {
        "id": "edge_001",
        "question": "버트가 뭐야?",
        "user_intent": "오타가 있는 질문",
        "difficulty": "easy",
        "complexity": "low",
        "expected_tool": "glossary",
        "expected_answer": "BERT에 대해 설명드리겠습니다. [정상 답변과 동일]",
        "reference_docs": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)",
        "expected_answer_keywords": ["BERT", "양방향"],
        "evaluation_criteria": {
          "must_include": ["오타 교정 인식"],
          "should_include": ["정상 답변"]
        },
        "category": "오타 처리",
        "tags": ["edge_case", "typo"]
      },
      {
        "id": "edge_002",
        "question": "BERT 설명하고 논문도 찾아줘",
        "user_intent": "다중 의도 - 정의 + 논문검색",
        "difficulty": "hard",
        "complexity": "high",
        "expected_tool": "multiple",
        "expected_tools": ["glossary", "search_paper"],
        "expected_answer": "**BERT 설명:**\nBERT는 양방향 Transformer 기반 언어 모델로... [정의]\n\n**관련 논문:**\n- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)\n- 원문: https://arxiv.org/abs/1810.04805",
        "reference_docs": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018)",
        "expected_answer_keywords": ["정의", "논문", "링크"],
        "evaluation_criteria": {
          "must_include": ["두 가지 답변 모두"],
          "should_include": ["구분된 섹션"],
          "structure": "multi_section"
        },
        "category": "다중 의도",
        "tags": ["edge_case", "multi_intent"]
      },
      {
        "id": "edge_003",
        "question": "Transformer 논문 내용 중에 Positional Encoding 부분만 자세히 설명해줘",
        "user_intent": "논문의 특정 부분만 상세 설명",
        "difficulty": "hard",
        "complexity": "high",
        "expected_tool": "summarize",
        "expected_answer": "Transformer의 Positional Encoding:\n\n**필요성:**\nSelf-Attention은 순서 정보가 없어, 위치 정보를 수동으로 주입해야 합니다.\n\n**수식:**\nPE(pos, 2i) = sin(pos / 10000^(2i/d_model))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n\n**특징:**\n- pos: 위치, i: 차원\n- sin/cos 함수로 고유한 위치 표현\n- 상대적 위치 관계 학습 가능\n- 학습이 아닌 고정된 함수 사용\n\n(Vaswani et al., 2017)",
        "reference_docs": "Attention is All You Need (Vaswani et al., 2017)",
        "expected_answer_keywords": ["Positional Encoding", "sin", "cos", "위치"],
        "evaluation_criteria": {
          "must_include": ["수식", "이유", "특징"],
          "should_include": ["구체적 설명"],
          "must_not_include": ["전체 논문 요약"],
          "citation_required": true
        },
        "category": "부분 요약",
        "tags": ["edge_case", "partial_summary"]
      },
      {
        "id": "edge_004",
        "question": "AI가 뭐야?",
        "user_intent": "너무 일반적인 질문",
        "difficulty": "easy",
        "complexity": "low",
        "expected_tool": "general",
        "expected_answer": "AI(Artificial Intelligence, 인공지능)는 인간의 지능을 컴퓨터로 구현하는 기술입니다.\n\n질문이 광범위한데, 다음 중 어떤 부분이 궁금하신가요?\n1. NLP(자연어처리) 분야의 AI\n2. 머신러닝과 딥러닝\n3. 특정 AI 모델 (GPT, BERT 등)\n4. AI의 역사와 발전",
        "reference_docs": "N/A",
        "expected_answer_keywords": ["AI", "인공지능", "구체화 요청"],
        "evaluation_criteria": {
          "must_include": ["간단한 정의", "구체화 요청"],
          "should_include": ["선택지 제시"],
          "must_not_include": ["지나치게 긴 설명"]
        },
        "category": "모호한 질문",
        "tags": ["edge_case", "too_broad"]
      },
      {
        "id": "edge_005",
        "question": "내 논문 요약해줘",
        "user_intent": "파일 업로드 없이 요약 요청",
        "difficulty": "easy",
        "complexity": "low",
        "expected_tool": "error_handling",
        "expected_answer": "논문을 요약해드리려면 파일이나 링크가 필요합니다.\n\n다음 방법 중 하나를 선택해주세요:\n1. PDF 파일 업로드\n2. 논문 제목과 저자 알려주시면 검색 후 요약\n3. arXiv 등 논문 링크 제공\n\n어떤 방법이 편하신가요?",
        "reference_docs": "N/A",
        "expected_answer_keywords": ["파일 필요", "대안 제시"],
        "evaluation_criteria": {
          "must_include": ["불가능 이유", "대안"],
          "should_include": ["친절한 톤"],
          "must_not_include": ["거절만 하기"]
        },
        "category": "불완전한 요청",
        "tags": ["edge_case", "missing_info"]
      }
    ],

    "realistic_scenarios": [
      {
        "id": "real_001",
        "question": "우리 회사에서 RAG 시스템 도입하려고 하는데, 어떤 논문들 참고하면 좋을까? 그리고 실제 구현할 때 주의사항도 알려줘",
        "user_intent": "실무 적용을 위한 복합적 정보 요청",
        "difficulty": "hard",
        "complexity": "high",
        "expected_tool": "multiple",
        "expected_tools": ["search_paper", "general"],
        "expected_answer": "**참고 논문:**\n1. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020) - RAG 기초\n2. In-Context Retrieval-Augmented Language Models (Ram et al., 2023) - 실전 개선\n\n**실무 구현 주의사항:**\n1. 검색 품질이 전체 성능을 좌우 - 임베딩 모델과 chunking 전략 중요\n2. Latency 관리 - 검색 시간이 추가되므로 캐싱 필수\n3. 문서 업데이트 주기 설정\n4. Retrieval 실패 시 fallback 전략\n5. 비용 - API 호출이 2배 (검색 + 생성)\n\n구체적인 도메인이나 규모를 알려주시면 더 맞춤형 조언 가능합니다!",
        "reference_docs": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020)",
        "expected_answer_keywords": ["논문", "실무", "주의사항", "구현"],
        "evaluation_criteria": {
          "must_include": ["논문 추천", "실무 조언"],
          "should_include": ["구체적 주의사항", "후속 질문 유도"],
          "tone": "practical",
          "citation_required": true
        },
        "category": "실무 컨설팅",
        "tags": ["realistic", "multi_intent", "practical"]
      },
      {
        "id": "real_002",
        "question": "GPT-3 논문 읽어봤는데 few-shot learning 부분이 이해가 안 돼. 왜 example을 주면 성능이 올라가는 거야?",
        "user_intent": "논문의 특정 개념 심화 이해",
        "difficulty": "hard",
        "complexity": "high",
        "expected_tool": "general",
        "expected_answer": "Few-shot learning이 효과적인 이유:\n\n**1. Context로부터의 패턴 학습**\nGPT-3는 사전학습 시 수많은 패턴을 본 상태입니다. Few-shot example은 \"지금 해야 할 task가 이런 패턴이다\"라는 힌트를 주는 것입니다.\n\n**비유:**\n시험 문제 유형을 모를 때 vs 예시 문제를 볼 때의 차이와 같습니다.\n\n**2. In-context Learning 능력**\nGPT-3의 175B 파라미터는 충분히 크기 때문에, 새로운 파라미터 학습 없이도 context에서 task를 추론할 수 있습니다.\n\n**성능 향상 메커니즘:**\n- 0-shot: 모델이 추측\n- Few-shot: 입출력 패턴 명시 → 불확실성 감소\n\n논문 표 3.1을 보면 few-shot이 0-shot보다 10-20% 성능 향상을 보입니다.\n\n(Brown et al., 2020 - GPT-3 논문)",
        "reference_docs": "Language Models are Few-Shot Learners (Brown et al., 2020)",
        "expected_answer_keywords": ["few-shot", "in-context", "패턴", "예시"],
        "evaluation_criteria": {
          "must_include": ["이유 설명", "메커니즘"],
          "should_include": ["비유", "구체적 근거"],
          "tone": "educational",
          "citation_required": true
        },
        "category": "개념 심화",
        "tags": ["realistic", "deep_understanding", "paper_specific"]
      }
    ]
  },

  "evaluation_templates": {
    "basic_answer_template": {
      "question": "{question}",
      "answer": "{ai_generated_answer}",
      "reference_docs": "{reference_docs}",
      "difficulty": "{difficulty}",
      "expected_scores": {
        "question_answer_alignment": "질문이 요구한 정보를 포함하는가?",
        "factual_accuracy": "참고 문서와 사실이 일치하는가?",
        "completeness": "답변이 충분히 완전한가?",
        "difficulty_alignment": "난이도에 맞는 설명인가?",
        "citation_quality": "출처를 명확히 밝혔는가?"
      }
    }
  },

  "dataset_metadata": {
    "version": "2.0",
    "created_date": "2025-11-03",
    "total_questions": 20,
    "description": "프롬프트 엔지니어링 고도화 Golden Dataset - 실전 시나리오 및 엣지 케이스 포함",
    
    "difficulty_distribution": {
      "easy": 8,
      "hard": 12
    },
    
    "complexity_distribution": {
      "low": 6,
      "medium": 5,
      "high": 9
    },
    
    "category_distribution": {
      "용어 정의": 3,
      "논문 검색": 2,
      "최신 정보": 2,
      "논문 요약": 2,
      "개념 비교": 4,
      "기술적 분석": 2,
      "오타 처리": 1,
      "다중 의도": 1,
      "모호한 질문": 1,
      "불완전한 요청": 1,
      "실무 컨설팅": 1
    },
    
    "tool_distribution": {
      "glossary": 3,
      "search_paper": 2,
      "web_search": 2,
      "summarize": 3,
      "general": 6,
      "multiple": 2,
      "error_handling": 1
    },
    
    "coverage": {
      "basic_queries": "기본적인 정의, 검색, 비교 질문",
      "technical_queries": "심화 기술 분석, 수식, 복잡도 등",
      "edge_cases": "오타, 다중 의도, 모호한 질문, 불완전한 요청",
      "realistic_scenarios": "실무 적용, 논문 심화 이해 등 현실적 시나리오"
    },
    
    "quality_features": [
      "참고 문서(reference_docs) 포함",
      "예상 답변(expected_answer) 샘플 제공",
      "구체적인 평가 기준(evaluation_criteria) 명시",
      "엣지 케이스 5개 포함",
      "실전 시나리오 2개 포함",
      "다중 Tool 사용 시나리오 포함"
    ],
    
    "usage_guide": {
      "step1": "질문 카테고리 선택 (basic/search/comparison/technical/edge/realistic)",
      "step2": "AI 시스템에 질문 입력",
      "step3": "생성된 답변과 expected_answer 비교",
      "step4": "evaluation_criteria에 따라 5가지 기준 평가",
      "step5": "total_score 계산 및 개선점 도출"
    }
  },

  "annotation_guidelines": {
    "difficulty_criteria": {
      "easy": "일반인도 이해 가능한 수준, 단순 정의나 검색",
      "hard": "전문가 수준의 깊이, 비교/분석/수식 포함"
    },
    "complexity_criteria": {
      "low": "단일 Tool, 단일 의도, 명확한 답변",
      "medium": "1-2개 Tool, 약간의 맥락 파악 필요",
      "high": "복수 Tool, 다중 의도, 심화 분석 필요"
    }
  }
}