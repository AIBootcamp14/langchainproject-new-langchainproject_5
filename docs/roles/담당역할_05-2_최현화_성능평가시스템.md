# 담당역할_05-2: 성능 평가 시스템

## 문서 정보
- **작성일**: 2025-11-04
- **작성자**: 최현화[팀장]
- **프로젝트명**: 논문 리뷰 챗봇 (AI Agent + RAG)
- **팀명**: 연결의 민족
- **담당 기능**: 성능 평가 시스템, LLM-as-a-Judge, RAG 평가, Agent 정확도 측정

---

## 담당자 정보

**담당자**: 최현화[팀장]

**역할**: 성능 평가 시스템 구현 및 평가 지표 측정

---

## 담당 업무 개요

이 역할은 챗봇의 답변 품질을 자동으로 평가하는 **성능 평가 시스템**을 구축하는 것을 담당합니다. LLM-as-a-Judge 방식을 사용하여 답변 정확도, 관련성, 난이도 적합성, 출처 명시 여부를 평가하고, RAG 검색 성능과 Agent 라우팅 정확도를 측정합니다.

### 주요 책임

1. **AnswerEvaluator 클래스 구현** (`src/evaluation/evaluator.py`)
   - LLM-as-a-Judge 평가 프롬프트 설계
   - 4가지 평가 기준 (정확도, 관련성, 난이도 적합성, 출처 명시)
   - JSON 형식 평가 결과 파싱

2. **평가 결과 저장**
   - evaluation_results 테이블 생성 (PostgreSQL)
   - 평가 지표 저장 및 조회
   - 통계 집계 기능

3. **Streamlit UI 연동**
   - 평가 결과 표시
   - 평가 통계 시각화 (선택)

---

## 참여 기간 및 우선순위

### 참여 기간
- **구현 기간**: 2025-11-04 ~ 2025-11-04 (1일)
- **테스트 및 검증**: 2025-11-04 (당일)

### 우선순위
1. **최우선 (1-2시간)**: AnswerEvaluator 클래스 구현
   - LLM-as-a-Judge 프롬프트 설계
   - 평가 함수 구현
   - JSON 파싱

2. **우선 (1시간)**: 평가 결과 저장
   - evaluation_results 테이블 생성
   - 저장 함수 구현

3. **선택 (1시간)**: Streamlit UI 연동
   - 평가 결과 표시 (선택)

---

## 세부 업무 및 구현 내용

### 1. AnswerEvaluator 클래스 구현

**파일 경로**: `src/evaluation/evaluator.py`

#### 1.1 전체 코드 구조

```python
# -------------------------표준 라이브러리 ------------------------- #
import json
from typing import Dict, Optional

# ------------------------- 서드파티 라이브러리 ------------------------- #
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

# ------------------------- 프로젝트 모듈 ------------------------- #
from src.llm.client import LLMClient
from src.utils.logger import Logger


# ==================== 평가 프롬프트 정의 ==================== #
EVALUATION_PROMPT_TEMPLATE = """
다음 AI 챗봇의 답변을 평가해주세요.

[사용자 질문]
{question}

[AI 답변]
{answer}

[참고 문서]
{reference_docs}

[난이도 모드]
{difficulty}

[평가 기준]
1. 정확도 (0-10점): 참고 문서의 내용과 일치하는가?
2. 관련성 (0-10점): 질문과 답변이 관련있는가?
3. 난이도 적합성 (0-10점): 난이도 모드에 맞는 답변인가?
4. 출처 명시 (0-10점): 논문 제목, 저자를 명시했는가?

JSON 형식으로만 반환하세요:
{{
    "accuracy_score": <점수>,
    "relevance_score": <점수>,
    "difficulty_score": <점수>,
    "citation_score": <점수>,
    "total_score": <총점>,
    "comment": "<평가 코멘트>"
}}
"""

EVALUATION_PROMPT = PromptTemplate(
    template=EVALUATION_PROMPT_TEMPLATE,
    input_variables=["question", "answer", "reference_docs", "difficulty"]
)


# ==================== AnswerEvaluator 클래스 ==================== #
class AnswerEvaluator:
    """
    답변 품질 평가 클래스 (LLM-as-a-Judge)

    사용자 질문, AI 답변, 참고 문서, 난이도 모드를 입력받아
    4가지 평가 기준으로 답변 품질을 평가합니다.
    """

    # ---------------------- 초기화 메서드 ---------------------- #
    def __init__(self, exp_manager=None):
        """
        AnswerEvaluator 초기화

        Args:
            exp_manager: ExperimentManager 인스턴스 (선택 사항)
        """
        # LLM 클라이언트 초기화 (OpenAI GPT-5)
        self.llm_client = LLMClient()
        self.llm = ChatOpenAI(model="gpt-5", temperature=0)

        # ExperimentManager 설정
        self.exp_manager = exp_manager

        # Logger 초기화
        if exp_manager:
            self.logger = exp_manager.get_tool_logger("evaluator")
        else:
            self.logger = Logger("logs/evaluator.log")

        self.logger.write("AnswerEvaluator 초기화 완료")

    # ---------------------- 답변 평가 메서드 ---------------------- #
    def evaluate(
        self,
        question: str,
        answer: str,
        reference_docs: str,
        difficulty: str
    ) -> Dict:
        """
        답변 평가

        Args:
            question (str): 사용자 질문
            answer (str): AI 답변
            reference_docs (str): 참고 문서
            difficulty (str): 난이도 모드 (easy/hard)

        Returns:
            Dict: 평가 결과 딕셔너리
                - accuracy_score (int): 정확도 점수 (0-10)
                - relevance_score (int): 관련성 점수 (0-10)
                - difficulty_score (int): 난이도 적합성 점수 (0-10)
                - citation_score (int): 출처 명시 점수 (0-10)
                - total_score (int): 총점 (0-40)
                - comment (str): 평가 코멘트
        """
        # -------------------- 프롬프트 포맷팅 -------------------- #
        # 프롬프트 포맷팅
        prompt = EVALUATION_PROMPT.format(
            question=question,
            answer=answer,
            reference_docs=reference_docs,
            difficulty=difficulty
        )

        self.logger.write(f"평가 시작: {question[:50]}...")

        # -------------------- LLM 호출 -------------------- #
        try:
            # LLM 호출
            response = self.llm.invoke(prompt)
            result_text = response.content

            self.logger.write(f"LLM 응답 수신: {len(result_text)} 글자")

            # -------------------- JSON 파싱 -------------------- #
            # JSON 파싱
            result = json.loads(result_text)

            self.logger.write(f"평가 완료: 총점 {result.get('total_score', 0)}/40")

            return result

        # -------------------- 예외 처리 -------------------- #
        except json.JSONDecodeError as e:
            self.logger.write(f"JSON 파싱 오류: {e}")
            return {
                "accuracy_score": 0,
                "relevance_score": 0,
                "difficulty_score": 0,
                "citation_score": 0,
                "total_score": 0,
                "comment": f"평가 실패: JSON 파싱 오류 ({str(e)})"
            }

        except Exception as e:
            self.logger.write(f"평가 오류: {e}")
            return {
                "accuracy_score": 0,
                "relevance_score": 0,
                "difficulty_score": 0,
                "citation_score": 0,
                "total_score": 0,
                "comment": f"평가 실패: {str(e)}"
            }

    # ---------------------- 배치 평가 메서드 ---------------------- #
    def evaluate_batch(self, test_cases: list) -> list:
        """
        배치 평가

        Args:
            test_cases (list): 테스트 케이스 리스트
                [{"question": ..., "answer": ..., "reference_docs": ..., "difficulty": ...}, ...]

        Returns:
            list: 평가 결과 리스트
        """
        results = []

        self.logger.write(f"배치 평가 시작: {len(test_cases)}개 케이스")

        # 각 테스트 케이스 평가
        for i, case in enumerate(test_cases, 1):
            self.logger.write(f"[{i}/{len(test_cases)}] 평가 중...")

            # 평가 수행
            result = self.evaluate(
                question=case["question"],
                answer=case["answer"],
                reference_docs=case.get("reference_docs", ""),
                difficulty=case.get("difficulty", "easy")
            )

            # 질문과 답변 추가
            result["question"] = case["question"]
            result["answer"] = case["answer"]

            results.append(result)

        self.logger.write(f"배치 평가 완료: {len(results)}개 결과")

        return results

    # ---------------------- 종료 메서드 ---------------------- #
    def close(self):
        """Logger 종료"""
        if self.logger and not self.exp_manager:
            self.logger.close()
```

---

### 2. 평가 결과 저장

**파일 경로**: `src/evaluation/storage.py`

#### 2.1 전체 코드

```python
# -------------------------표준 라이브러리 ------------------------- #
import os
from typing import List, Dict
from datetime import datetime

# ------------------------- 서드파티 라이브러리 ------------------------- #
import psycopg2
from dotenv import load_dotenv

# ------------------------- 환경 변수 로드 ------------------------- #
load_dotenv()


# ==================== DB 연결 함수 ==================== #
def _get_conn():
    """PostgreSQL 연결"""
    return psycopg2.connect(
        host=os.getenv("POSTGRES_HOST", "localhost"),
        port=os.getenv("POSTGRES_PORT", 5432),
        user=os.getenv("POSTGRES_USER", "postgres"),
        password=os.getenv("POSTGRES_PASSWORD", ""),
        database=os.getenv("POSTGRES_DB", "papers")
    )


# ==================== 테이블 생성 함수 ==================== #
def create_evaluation_table():
    """
    evaluation_results 테이블 생성

    테이블 스키마:
    - eval_id: 평가 ID (Primary Key, Serial)
    - question: 사용자 질문
    - answer: AI 답변
    - accuracy_score: 정확도 점수 (0-10)
    - relevance_score: 관련성 점수 (0-10)
    - difficulty_score: 난이도 적합성 점수 (0-10)
    - citation_score: 출처 명시 점수 (0-10)
    - total_score: 총점 (0-40)
    - comment: 평가 코멘트
    - created_at: 생성 시간
    """
    # DB 연결
    conn = _get_conn()
    cursor = conn.cursor()

    # 테이블 생성
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS evaluation_results (
            eval_id SERIAL PRIMARY KEY,
            question TEXT NOT NULL,
            answer TEXT NOT NULL,
            accuracy_score INT CHECK (accuracy_score >= 0 AND accuracy_score <= 10),
            relevance_score INT CHECK (relevance_score >= 0 AND relevance_score <= 10),
            difficulty_score INT CHECK (difficulty_score >= 0 AND difficulty_score <= 10),
            citation_score INT CHECK (citation_score >= 0 AND citation_score <= 10),
            total_score INT CHECK (total_score >= 0 AND total_score <= 40),
            comment TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)

    # 커밋 및 종료
    conn.commit()
    cursor.close()
    conn.close()


# ==================== 평가 결과 저장 함수 ==================== #
def save_evaluation_results(evaluation_results: List[Dict]):
    """
    평가 결과를 PostgreSQL에 저장

    Args:
        evaluation_results (List[Dict]): 평가 결과 리스트
            [{"question": ..., "answer": ..., "accuracy_score": ..., ...}, ...]
    """
    # DB 연결
    conn = _get_conn()
    cursor = conn.cursor()

    # 테이블 생성 (없을 경우)
    create_evaluation_table()

    # 평가 결과 삽입
    for result in evaluation_results:
        cursor.execute("""
            INSERT INTO evaluation_results
            (question, answer, accuracy_score, relevance_score, difficulty_score, citation_score, total_score, comment)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """, (
            result['question'],
            result['answer'],
            result['accuracy_score'],
            result['relevance_score'],
            result['difficulty_score'],
            result['citation_score'],
            result['total_score'],
            result['comment']
        ))

    # 커밋 및 종료
    conn.commit()
    cursor.close()
    conn.close()


# ==================== 평가 결과 조회 함수 ==================== #
def get_evaluation_results(limit: int = 10) -> List[Dict]:
    """
    최근 평가 결과 조회

    Args:
        limit (int): 조회 개수 (기본: 10)

    Returns:
        List[Dict]: 평가 결과 리스트
    """
    # DB 연결
    conn = _get_conn()
    cursor = conn.cursor()

    # 최근 평가 결과 조회
    cursor.execute("""
        SELECT eval_id, question, answer, accuracy_score, relevance_score,
               difficulty_score, citation_score, total_score, comment, created_at
        FROM evaluation_results
        ORDER BY created_at DESC
        LIMIT %s
    """, (limit,))

    rows = cursor.fetchall()

    # 딕셔너리 변환
    results = []
    for row in rows:
        results.append({
            "eval_id": row[0],
            "question": row[1],
            "answer": row[2],
            "accuracy_score": row[3],
            "relevance_score": row[4],
            "difficulty_score": row[5],
            "citation_score": row[6],
            "total_score": row[7],
            "comment": row[8],
            "created_at": row[9]
        })

    # 종료
    cursor.close()
    conn.close()

    return results


# ==================== 평가 통계 조회 함수 ==================== #
def get_evaluation_statistics() -> Dict:
    """
    평가 통계 조회

    Returns:
        Dict: 평가 통계 딕셔너리
            - total_count: 총 평가 개수
            - avg_accuracy: 평균 정확도
            - avg_relevance: 평균 관련성
            - avg_difficulty: 평균 난이도 적합성
            - avg_citation: 평균 출처 명시
            - avg_total: 평균 총점
    """
    # DB 연결
    conn = _get_conn()
    cursor = conn.cursor()

    # 통계 조회
    cursor.execute("""
        SELECT COUNT(*),
               AVG(accuracy_score),
               AVG(relevance_score),
               AVG(difficulty_score),
               AVG(citation_score),
               AVG(total_score)
        FROM evaluation_results
    """)

    row = cursor.fetchone()

    # 통계 딕셔너리 생성
    stats = {
        "total_count": row[0] or 0,
        "avg_accuracy": round(row[1] or 0, 2),
        "avg_relevance": round(row[2] or 0, 2),
        "avg_difficulty": round(row[3] or 0, 2),
        "avg_citation": round(row[4] or 0, 2),
        "avg_total": round(row[5] or 0, 2)
    }

    # 종료
    cursor.close()
    conn.close()

    return stats
```

---

### 3. 평가 스크립트

**파일 경로**: `scripts/evaluate_answers.py`

#### 3.1 전체 코드

```python
# -------------------------표준 라이브러리 ------------------------- #
import json

# ------------------------- 프로젝트 모듈 ------------------------- #
from src.evaluation.evaluator import AnswerEvaluator
from src.evaluation.storage import save_evaluation_results, get_evaluation_statistics
from src.utils.experiment_manager import ExperimentManager


# ==================== 메인 실행부 ==================== #
if __name__ == "__main__":
    # ExperimentManager 초기화
    with ExperimentManager() as exp:
        exp.logger.write("=" * 60)
        exp.logger.write("답변 평가 스크립트 실행")
        exp.logger.write("=" * 60)

        # AnswerEvaluator 초기화
        evaluator = AnswerEvaluator(exp_manager=exp)

        # 테스트 케이스 정의
        test_cases = [
            {
                "question": "Transformer 논문 설명해줘",
                "answer": "Transformer는 2017년 Google에서 발표한 딥러닝 모델입니다. Self-Attention 메커니즘을 사용하여...",
                "reference_docs": "Attention Is All You Need (Vaswani et al., 2017)",
                "difficulty": "easy"
            },
            {
                "question": "BERT의 pre-training 방법은?",
                "answer": "BERT는 Masked Language Model (MLM)과 Next Sentence Prediction (NSP) 두 가지 방법으로 pre-training을 진행합니다...",
                "reference_docs": "BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)",
                "difficulty": "hard"
            }
        ]

        # 배치 평가 수행
        exp.logger.write(f"테스트 케이스 {len(test_cases)}개 평가 시작")
        results = evaluator.evaluate_batch(test_cases)

        # 평가 결과 저장
        exp.logger.write("평가 결과 PostgreSQL에 저장")
        save_evaluation_results(results)

        # 평가 통계 조회
        stats = get_evaluation_statistics()
        exp.logger.write(f"평가 통계: {json.dumps(stats, indent=2, ensure_ascii=False)}")

        # 평가 결과 출력
        exp.logger.write("=" * 60)
        exp.logger.write("평가 완료")
        for i, result in enumerate(results, 1):
            exp.logger.write(f"\n[테스트 {i}]")
            exp.logger.write(f"질문: {result['question']}")
            exp.logger.write(f"총점: {result['total_score']}/40")
            exp.logger.write(f"- 정확도: {result['accuracy_score']}/10")
            exp.logger.write(f"- 관련성: {result['relevance_score']}/10")
            exp.logger.write(f"- 난이도 적합성: {result['difficulty_score']}/10")
            exp.logger.write(f"- 출처 명시: {result['citation_score']}/10")
            exp.logger.write(f"코멘트: {result['comment']}")

        exp.logger.write("=" * 60)
```

---

## 참고 PRD 문서

아래 PRD 문서들을 참고하여 구현하세요:

1. **[09_평가_기준.md](../PRD/09_평가_기준.md)** ⭐⭐⭐
   - RAG 평가 지표 (Recall@K, Faithfulness 등)
   - LLM-as-a-Judge 평가 방법
   - 평가 기준 및 목표 값

2. **[07_성능_개선_전략.md](../PRD/07_성능_개선_전략.md)** ⭐⭐
   - 캐싱 전략
   - 비동기 처리
   - DB 최적화

3. **[담당역할_05_추가선택기능.md](./담당역할_05_추가선택기능.md)** ⭐⭐
   - 성능 평가 시스템 설명 (196-411줄)
   - 예제 코드 및 사용법

---

## 협업 방법

### 다른 담당자와의 협업

#### 1. 전체 팀원 - 평가 결과 활용
- **협업 내용**: 답변 품질 개선을 위한 평가 결과 활용
- **타이밍**: 평가 시스템 구현 완료 후
- **전달 사항**:
  - AnswerEvaluator 사용법
  - 평가 결과 조회 방법
  - 평가 통계 해석 방법

---

## 구현 체크리스트

### Phase 1: AnswerEvaluator 클래스 구현 (우선순위 1)
- [x] `src/evaluation/evaluator.py` 파일 생성
- [x] LLM-as-a-Judge 프롬프트 설계
- [x] AnswerEvaluator 클래스 구현
  - [x] `__init__()` 메서드
  - [x] `evaluate()` 메서드
  - [x] `evaluate_batch()` 메서드
  - [x] `close()` 메서드
- [x] JSON 파싱 및 오류 처리

### Phase 2: 평가 결과 저장 (우선순위 2)
- [x] `src/evaluation/storage.py` 파일 생성
- [x] evaluation_results 테이블 생성
- [x] `save_evaluation_results()` 함수 구현
- [x] `get_evaluation_results()` 함수 구현
- [x] `get_evaluation_statistics()` 함수 구현

### Phase 3: 평가 스크립트 (우선순위 3)
- [x] `scripts/evaluate_answers.py` 스크립트 작성
- [x] 테스트 케이스 정의
- [x] 배치 평가 수행
- [x] 평가 결과 저장 및 출력

### Phase 4: Streamlit UI 연동 (선택)
- [ ] 평가 결과 표시 페이지 (선택)
- [ ] 평가 통계 시각화 (선택)

---

## 규칙 및 주의사항

### 1. 평가 기준
- **정확도 (0-10점)**: 참고 문서의 내용과 일치하는가?
- **관련성 (0-10점)**: 질문과 답변이 관련있는가?
- **난이도 적합성 (0-10점)**: 난이도 모드에 맞는 답변인가?
- **출처 명시 (0-10점)**: 논문 제목, 저자를 명시했는가?

### 2. 코드 품질
- 모든 함수에 한글 주석 작성 (docs/rules/annotate_style.md 준수)
- 섹션 구분선 사용 (등호 20개, 대시 22개)
- 함수별 상세 설명
- 로직 블록별 설명

### 3. 에러 처리
- JSON 파싱 실패 시 기본 점수 0 반환
- LLM 호출 실패 시 재시도 로직 (선택)
- DB 연결 실패 시 상세 에러 메시지

### 4. 성능 고려사항
- 배치 평가 시 병렬 처리 (선택)
- DB Connection Pool 사용
- 평가 결과 캐싱 (선택)

---

## 예상 결과물

### 1. 파일 구조

```
src/
└── evaluation/
    ├── __init__.py                # 모듈 초기화 (신규)
    ├── evaluator.py               # AnswerEvaluator 클래스 (신규)
    └── storage.py                 # 평가 결과 저장 (신규)

scripts/
└── evaluate_answers.py            # 평가 스크립트 (신규)
```

### 2. 사용 예시

**기본 사용:**
```python
from src.evaluation.evaluator import AnswerEvaluator
from src.evaluation.storage import save_evaluation_results

# AnswerEvaluator 초기화
evaluator = AnswerEvaluator()

# 답변 평가
result = evaluator.evaluate(
    question="Transformer 논문 설명해줘",
    answer="Transformer는 2017년 Google에서 발표한 딥러닝 모델입니다...",
    reference_docs="Attention Is All You Need (Vaswani et al., 2017)",
    difficulty="easy"
)

print(f"총점: {result['total_score']}/40")
print(f"정확도: {result['accuracy_score']}/10")
print(f"관련성: {result['relevance_score']}/10")
print(f"난이도 적합성: {result['difficulty_score']}/10")
print(f"출처 명시: {result['citation_score']}/10")
print(f"코멘트: {result['comment']}")

# 평가 결과 저장
save_evaluation_results([result])
```

**배치 평가:**
```python
# 테스트 케이스 정의
test_cases = [
    {"question": "...", "answer": "...", "reference_docs": "...", "difficulty": "easy"},
    {"question": "...", "answer": "...", "reference_docs": "...", "difficulty": "hard"}
]

# 배치 평가
results = evaluator.evaluate_batch(test_cases)

# 평가 결과 저장
save_evaluation_results(results)
```

**평가 통계 조회:**
```python
from src.evaluation.storage import get_evaluation_statistics

# 평가 통계 조회
stats = get_evaluation_statistics()

print(f"총 평가 개수: {stats['total_count']}")
print(f"평균 정확도: {stats['avg_accuracy']}/10")
print(f"평균 관련성: {stats['avg_relevance']}/10")
print(f"평균 난이도 적합성: {stats['avg_difficulty']}/10")
print(f"평균 출처 명시: {stats['avg_citation']}/10")
print(f"평균 총점: {stats['avg_total']}/40")
```

---

## 추가 참고 사항

### 1. evaluation_results 테이블 스키마

```sql
CREATE TABLE IF NOT EXISTS evaluation_results (
    eval_id SERIAL PRIMARY KEY,
    question TEXT NOT NULL,
    answer TEXT NOT NULL,
    accuracy_score INT CHECK (accuracy_score >= 0 AND accuracy_score <= 10),
    relevance_score INT CHECK (relevance_score >= 0 AND relevance_score <= 10),
    difficulty_score INT CHECK (difficulty_score >= 0 AND difficulty_score <= 10),
    citation_score INT CHECK (citation_score >= 0 AND citation_score <= 10),
    total_score INT CHECK (total_score >= 0 AND total_score <= 40),
    comment TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 2. 환경 변수 설정

`.env` 파일에 필요한 환경 변수:
```
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password
POSTGRES_DB=papers

OPENAI_API_KEY=your_openai_key
```

---

## 완료 일자

- **구현 완료**: 2025-11-04
- **테스트 완료**: 2025-11-04
- **문서화 완료**: 2025-11-04
