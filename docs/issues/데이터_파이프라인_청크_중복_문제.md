# ì´ìŠˆ: ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì²­í¬ ì¤‘ë³µ ë° ì„ë² ë”© í’ˆì§ˆ ë¬¸ì œ

## ğŸ“‹ ì´ìŠˆ ìš”ì•½

**ë°œê²¬ ì¼ì**: 2025-11-05
**ì‹¬ê°ë„**: ğŸ”´ ë†’ìŒ
**ì˜í–¥ ë²”ìœ„**: RAG ê²€ìƒ‰ í’ˆì§ˆ, ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì „ì²´

`scripts/data/run_full_pipeline.py` ì‹¤í–‰ ì‹œ ìƒì„±ë˜ëŠ” ë…¼ë¬¸ ì²­í¬ì— ì¤‘ë³µ ë°ì´í„°ê°€ ë°œìƒí•˜ë©°, íŠ¹ì • ë…¼ë¬¸(ì˜ˆ: paper_id=1 "Attention Is All You Need")ì˜ ì„ë² ë”© í’ˆì§ˆì´ ë‚®ì•„ ê²€ìƒ‰ì´ ì œëŒ€ë¡œ ë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.

---

## ğŸ” ë¬¸ì œ ìƒì„¸

### 1. ì²­í¬ ì¤‘ë³µ ë¬¸ì œ

**ì¦ìƒ**:
- paper_id=1ì˜ 15ê°œ ì²­í¬ ì¤‘ ì²­í¬ 1-3ì´ ì™„ì „íˆ ë™ì¼í•œ ë‚´ìš©
- ì²­í¬ 4-5ë„ ì™„ì „íˆ ë™ì¼í•œ ë‚´ìš©
- chunk_index ë©”íƒ€ë°ì´í„°ê°€ ì œëŒ€ë¡œ ì„¤ì •ë˜ì§€ ì•ŠìŒ (NULL ê°’)

**ì›ì¸**:
```python
# src/data/document_loader.py:43-44
for i, ch in enumerate(chunks):
    ch.metadata["chunk_id"] = i  # âŒ chunk_idë§Œ ì„¤ì •, chunk_indexëŠ” ë¯¸ì„¤ì •
```

PyPDFLoaderê°€ PDFë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¡œë“œí•˜ëŠ”ë°:
- ì²« í˜ì´ì§€: ì €ì‘ê¶Œ í‘œì‹œ + ì œëª© + ì €ì ëª©ë¡
- RecursiveCharacterTextSplitterê°€ ì´ ì§§ì€ í˜ì´ì§€ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
- ê²°ê³¼: ë™ì¼í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì§„ ì¤‘ë³µ ì²­í¬ ìƒì„±

**ì‹¤ì œ ë°ì´í„° ì˜ˆì‹œ**:
```
[ì²­í¬ 1] (1000ì)
Provided proper attribution is provided, Google hereby grants permission...
Attention Is All You Need
Ashish Vaswani...

[ì²­í¬ 2] (1000ì)  â† ì²­í¬ 1ê³¼ ë™ì¼!
Provided proper attribution is provided, Google hereby grants permission...
Attention Is All You Need
Ashish Vaswani...

[ì²­í¬ 3] (1000ì)  â† ì²­í¬ 1, 2ì™€ ë™ì¼!
Provided proper attribution is provided, Google hereby grants permission...
Attention Is All You Need
Ashish Vaswani...
```

### 2. ì„ë² ë”© í’ˆì§ˆ ì €í•˜ ë¬¸ì œ

**ì¦ìƒ**:
- "Attention is all you need" ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹œ paper_id=1ì´ Top-10ì— í¬í•¨ ì•ˆ ë¨
- ë‹¤ë¥¸ ë…¼ë¬¸ë“¤(paper_id=79, 14)ì´ ì´ ë…¼ë¬¸ì„ **ì¸ìš©**í•œ ë‚´ìš©ì´ ë” ë†’ì€ ìˆœìœ„
- ê²€ìƒ‰ ê²°ê³¼:
  ```
  1ìœ„: paper_id=11 (L2=0.5606) - ë‹¤ë¥¸ ë…¼ë¬¸
  7ìœ„: paper_id=79 (L2=0.5347) - "Attention is all you need" ì¸ìš© ë‚´ìš©
  10ìœ„: paper_id=14 (L2=0.5421) - "Ashish Vaswani... Attention is all you need" ì¸ìš©
  âŒ paper_id=1 ì—†ìŒ
  ```

**ì›ì¸**:
1. **ì˜ë¯¸ ì—†ëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ì²­í¬í™”**: ì €ì‘ê¶Œ í‘œì‹œ, ì´ë©”ì¼ ì£¼ì†Œ, ì €ì ëª©ë¡ë§Œ ë°˜ë³µ
2. **ì‹¤ì œ ë…¼ë¬¸ ë³¸ë¬¸ ë¶€ì¡±**: ë³¸ë¬¸ ë‚´ìš©ì´ ê±°ì˜ ì—†ì–´ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ ë‚®ìŒ
3. **ì¸ìš© ë…¼ë¬¸ì˜ ë§¥ë½ì´ ë” í’ë¶€**: ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ì´ "Attention is all you need"ë¥¼ ì‹¤ì œ ì—°êµ¬ ë§¥ë½ì—ì„œ ì–¸ê¸‰í•˜ì—¬ ë” ë†’ì€ ìœ ì‚¬ë„

### 3. Fallback ì‹œìŠ¤í…œ ì˜¤ì‘ë™

**ì¦ìƒ**:
- RAG ê²€ìƒ‰ì—ì„œ "ê²°ê³¼ ì—†ìŒ" íŒ¨í„´ ê°ì§€ ì‹œ general ë‹µë³€ìœ¼ë¡œ í´ë°±
- ì‹¤ì œë¡œëŠ” ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆì§€ë§Œ, LLMì´ "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"ì´ë¼ê³  íŒë‹¨

**ì›ì¸**:
```python
# src/agent/failure_detector.py:40
r".*ê²°ê³¼ê°€?\s*ì—†.*",  # ì •ê·œì‹ íŒ¨í„´
```

LLMì´ ìƒì„±í•œ ë‹µë³€ì— "ìœ ì‚¬ë„ ì ìˆ˜: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ" ê°™ì€ ë¬¸êµ¬ê°€ í¬í•¨ë˜ë©´ ì‹¤íŒ¨ë¡œ ì˜¤ì¸

---

## ğŸ¯ í•´ê²° ë°©ì•ˆ

### 1. chunk_index ë©”íƒ€ë°ì´í„° ì˜¬ë°”ë¥´ê²Œ ì„¤ì •

**ìˆ˜ì • í•„ìš” íŒŒì¼**: `src/data/document_loader.py`

```python
# ìˆ˜ì • ì „
def load_and_split(self, pdf_path: str | Path, metadata: Optional[Dict] = None) -> List[Document]:
    docs = self.load_pdf(pdf_path, metadata)
    chunks = self.text_splitter.split_documents(docs)
    for i, ch in enumerate(chunks):
        ch.metadata["chunk_id"] = i  # âŒ ë¬¸ì œ
    return chunks

# ìˆ˜ì • í›„
def load_and_split(self, pdf_path: str | Path, metadata: Optional[Dict] = None) -> List[Document]:
    docs = self.load_pdf(pdf_path, metadata)
    chunks = self.text_splitter.split_documents(docs)
    for i, ch in enumerate(chunks):
        ch.metadata["chunk_id"] = i
        ch.metadata["chunk_index"] = i  # âœ… ì¶”ê°€
    return chunks
```

### 2. PDF í˜ì´ì§€ í•„í„°ë§ ì¶”ê°€

**ì˜µì…˜ A**: ì²« í˜ì´ì§€ ì œì™¸ (ì €ì‘ê¶Œ/ì œëª© í˜ì´ì§€)
```python
def load_pdf(self, pdf_path: str | Path, metadata: Optional[Dict] = None) -> List[Document]:
    loader = PyPDFLoader(str(pdf_path))
    documents = loader.load()

    # ì²« í˜ì´ì§€ê°€ ë©”íƒ€ë°ì´í„°ë§Œ ìˆìœ¼ë©´ ì œì™¸
    if len(documents) > 1 and len(documents[0].page_content) < 500:
        documents = documents[1:]  # ì²« í˜ì´ì§€ ê±´ë„ˆë›°ê¸°

    if metadata:
        for d in documents:
            d.metadata.update(metadata)
    return documents
```

**ì˜µì…˜ B**: ì˜ë¯¸ ì—†ëŠ” íŒ¨í„´ í•„í„°ë§
```python
def load_pdf(self, pdf_path: str | Path, metadata: Optional[Dict] = None) -> List[Document]:
    loader = PyPDFLoader(str(pdf_path))
    documents = loader.load()

    # ì €ì‘ê¶Œ í˜ì´ì§€ í•„í„°ë§
    filtered_docs = []
    for doc in documents:
        content = doc.page_content.lower()
        if "copyright" in content and "permission" in content and len(doc.page_content) < 1000:
            continue  # ì €ì‘ê¶Œ í˜ì´ì§€ ê±´ë„ˆë›°ê¸°
        filtered_docs.append(doc)

    if metadata:
        for d in filtered_docs:
            d.metadata.update(metadata)
    return filtered_docs
```

### 3. ì²­í¬ ì¤‘ë³µ ì œê±°

**ìˆ˜ì • í•„ìš” íŒŒì¼**: `scripts/data/load_embeddings.py`

```python
def deduplicate_chunks(chunks: List[Document]) -> List[Document]:
    """ë‚´ìš©ì´ ë™ì¼í•œ ì²­í¬ ì œê±°"""
    seen_contents = set()
    unique_chunks = []

    for chunk in chunks:
        # ë‚´ìš© í•´ì‹œ ìƒì„±
        content_hash = hash(chunk.page_content)

        if content_hash not in seen_contents:
            seen_contents.add(content_hash)
            unique_chunks.append(chunk)

    return unique_chunks

# load_embeddings.pyì˜ main í•¨ìˆ˜ì—ì„œ ì‚¬ìš©
chunks = loader.load_all_pdfs(pdf_dir, metadata_path)
chunks = deduplicate_chunks(chunks)  # âœ… ì¤‘ë³µ ì œê±°
print(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ (ì¤‘ë³µ ì œê±°)")
```

### 4. Fallback íŒ¨í„´ ê°œì„ 

**ìˆ˜ì • í•„ìš” íŒŒì¼**: `src/agent/failure_detector.py`

```python
# ê¸°ì¡´ íŒ¨í„´
REGEX_PATTERNS = [
    r".*ê²°ê³¼ê°€?\s*ì—†.*",  # âŒ ë„ˆë¬´ ê´‘ë²”ìœ„
]

# ê°œì„ ëœ íŒ¨í„´
REGEX_PATTERNS = [
    r"^ê²€ìƒ‰.*ê²°ê³¼ê°€?\s*ì—†",      # âœ… ë¬¸ì¥ ì‹œì‘ ë¶€ë¶„ë§Œ
    r"ê´€ë ¨.*ë…¼ë¬¸.*ì°¾ì„\s*ìˆ˜\s*ì—†", # âœ… ë” êµ¬ì²´ì 
]
```

---

## ğŸ“Š ì˜í–¥ ë²”ìœ„

### ì˜í–¥ë°›ëŠ” ê¸°ëŠ¥
1. âœ… **RAG ë…¼ë¬¸ ê²€ìƒ‰**: ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜
2. âœ… **Fallback ì‹œìŠ¤í…œ**: ì˜¤ì‘ë™ ê°€ëŠ¥ì„±
3. âœ… **ì„ë² ë”© ì €ì¥ ê³µê°„**: ì¤‘ë³µ ì²­í¬ë¡œ ì¸í•œ ê³µê°„ ë‚­ë¹„
4. âœ… **ê²€ìƒ‰ ì„±ëŠ¥**: ì¤‘ë³µ ì²­í¬ë¡œ ì¸í•œ ì—°ì‚° ë‚­ë¹„

### ì˜í–¥ë°›ì§€ ì•ŠëŠ” ê¸°ëŠ¥
1. âŒ Glossary ë„êµ¬
2. âŒ General ë‹µë³€
3. âŒ Web Search

---

## âœ… ê²€ì¦ ë°©ë²•

### 1. ì²­í¬ ì¤‘ë³µ í™•ì¸
```sql
-- ë™ì¼ paper_idì˜ ì²­í¬ ì¤‘ë³µ í™•ì¸
SELECT
    cmetadata->>'paper_id' as paper_id,
    document,
    COUNT(*) as cnt
FROM langchain_pg_embedding
WHERE collection_id = (SELECT uuid FROM langchain_pg_collection WHERE name = 'paper_chunks')
GROUP BY cmetadata->>'paper_id', document
HAVING COUNT(*) > 1
ORDER BY cnt DESC;
```

### 2. ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
```python
from src.database.vector_store import get_pgvector_store

store = get_pgvector_store("paper_chunks")
results = store.similarity_search_with_score("Attention is all you need", k=10)

# paper_id=1ì´ Top-10ì— ìˆëŠ”ì§€ í™•ì¸
for i, (doc, score) in enumerate(results, 1):
    if doc.metadata.get('paper_id') == 1:
        print(f"âœ… paper_id=1 ë°œê²¬! ìˆœìœ„: {i}, ê±°ë¦¬: {score:.4f}")
        break
else:
    print("âŒ paper_id=1ì´ Top-10ì— ì—†ìŠµë‹ˆë‹¤.")
```

### 3. chunk_index í™•ì¸
```sql
SELECT
    cmetadata->>'paper_id' as paper_id,
    cmetadata->>'chunk_index' as chunk_index,
    COUNT(*) as cnt
FROM langchain_pg_embedding
WHERE collection_id = (SELECT uuid FROM langchain_pg_collection WHERE name = 'paper_chunks')
GROUP BY cmetadata->>'paper_id', cmetadata->>'chunk_index'
ORDER BY paper_id::int, chunk_index::int;
```

---

## ğŸ“ ê´€ë ¨ íŒŒì¼

- `scripts/data/run_full_pipeline.py` - ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
- `scripts/data/process_documents.py` - PDF ì²­í¬ ë¶„í• 
- `scripts/data/load_embeddings.py` - ì„ë² ë”© ìƒì„± ë° ì €ì¥
- `src/data/document_loader.py` - PDF ë¡œë” ë° ì²­í‚¹ ë¡œì§
- `src/agent/failure_detector.py` - Fallback íŒ¨í„´ ê°ì§€

---

## ğŸ”— ì°¸ê³  ìë£Œ

- ì‹¤í—˜ ë¡œê·¸: `experiments/20251105/20251105_190225_session_026/chatbot.log`
- ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸: `/tmp/check_attention.py`, `/tmp/final_diagnosis.py`
- ê´€ë ¨ ì´ìŠˆ: [RAG System QnA](../../docs/QnA/)
