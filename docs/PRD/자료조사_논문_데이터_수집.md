# 자료조사: 논문 데이터 수집 방안

## 문서 정보
- **작성일**: 2025-10-29
- **프로젝트**: 논문 리뷰 챗봇 (AI Agent + RAG)
- **팀명**: 연결의 민족

---

## 1. 논문 데이터 소스

### 1.1 주요 논문 사이트

#### 1. arXiv (★★★ 최우선 추천)

**링크:** https://arxiv.org/

**특징:**
- 오픈 액세스 논문 저장소 (무료)
- AI/ML 최신 논문이 가장 빠르게 올라옴
- PDF 직접 다운로드 가능
- API 제공 (프로그래밍 방식 접근 가능)

**카테고리:**
- cs.AI (Artificial Intelligence)
- cs.CL (Computation and Language/NLP)
- cs.CV (Computer Vision)
- cs.LG (Machine Learning)
- cs.NE (Neural and Evolutionary Computing)

**API 예시:**
```python
import arxiv

# arXiv API로 논문 검색
search = arxiv.Search(
    query="attention mechanism",
    max_results=10,
    sort_by=arxiv.SortCriterion.SubmittedDate
)

for result in search.results():
    print(f"Title: {result.title}")
    print(f"Authors: {result.authors}")
    print(f"Published: {result.published}")
    print(f"PDF URL: {result.pdf_url}")
    print(f"Summary: {result.summary}")
```

---

#### 2. Papers with Code (★★★ 추천)

**링크:** https://paperswithcode.com/

**특징:**
- 논문 + 구현 코드 함께 제공
- 벤치마크 및 리더보드
- State-of-the-art 모델 추적 가능
- API 제공

**API 예시:**
```python
import requests

# Papers with Code API
response = requests.get(
    "https://paperswithcode.com/api/v1/papers/",
    params={"q": "transformer"}
)
papers = response.json()
```

---

#### 3. Semantic Scholar (★★★ 추천)

**링크:** https://www.semanticscholar.org/

**특징:**
- AI 기반 논문 검색 엔진
- 논문 간 인용 관계 시각화
- 핵심 내용 요약 제공
- 무료 API (Academic Graph API)

**API 예시:**
```python
import requests

# Semantic Scholar API
paper_id = "204e3073870fae3d05bcbc2f6a8e263d9b72e776"  # BERT 논문
response = requests.get(
    f"https://api.semanticscholar.org/graph/v1/paper/{paper_id}",
    params={"fields": "title,authors,abstract,year,citationCount,references"}
)
paper_data = response.json()
```

---

#### 4. Google Scholar

**링크:** https://scholar.google.com/

**특징:**
- 가장 방대한 논문 데이터베이스
- 인용 횟수 추적
- 공식 API 없음 (스크래핑 필요)

**스크래핑 라이브러리:**
```python
from scholarly import scholarly

# Google Scholar 검색
search_query = scholarly.search_pubs("transformer attention")
paper = next(search_query)

print(f"Title: {paper['bib']['title']}")
print(f"Authors: {paper['bib']['author']}")
print(f"Citations: {paper['num_citations']}")
```

**주의:** Google Scholar는 공식 API가 없어 스크래핑이 제한될 수 있음

---

#### 5. IEEE Xplore

**링크:** https://ieeexplore.ieee.org/

**특징:**
- 전기전자공학 및 컴퓨터 과학 논문
- 고품질 peer-reviewed 논문
- API 제공 (API 키 필요)

---

#### 6. ACL Anthology (NLP 특화)

**링크:** https://aclanthology.org/

**특징:**
- NLP 분야 최고 권위 학회 논문 (ACL, EMNLP, NAACL 등)
- PDF 무료 다운로드
- BibTeX 제공

---

#### 7. PubMed (의료/생명과학)

**링크:** https://pubmed.ncbi.nlm.nih.gov/

**특징:**
- 의료 AI, BioNLP 논문
- 무료 API

---

### 1.2 논문 사이트 비교표

| 사이트 | 무료 여부 | API 제공 | AI/ML 논문 | 업데이트 속도 | 추천도 |
|--------|----------|---------|-----------|--------------|--------|
| arXiv | ✅ | ✅ | ★★★ | 매우 빠름 | ★★★ |
| Papers with Code | ✅ | ✅ | ★★★ | 빠름 | ★★★ |
| Semantic Scholar | ✅ | ✅ | ★★★ | 빠름 | ★★★ |
| Google Scholar | ✅ | ❌ | ★★★ | 빠름 | ★★ |
| IEEE Xplore | 부분 유료 | ✅ | ★★ | 보통 | ★ |
| ACL Anthology | ✅ | ✅ | ★★ (NLP만) | 보통 | ★★ |

---

## 2. 데이터 수집 방법

### 2.1 방법 1: API 활용 (자동 수집) ★ 추천

**장점:**
- 자동화 가능
- 대량 데이터 수집 용이
- 메타데이터 함께 수집
- 최신 논문 실시간 업데이트 가능

**추천 라이브러리:**
- `arxiv` (arXiv API)
- `scholarly` (Google Scholar 스크래핑)
- `requests` (일반 API 호출)
- Langchain `ArxivLoader`

#### 구현 예시: arXiv에서 자동 수집

```python
import arxiv
import os
from datetime import datetime

def collect_arxiv_papers(query, max_results=50, save_dir="data/raw"):
    """
    arXiv에서 논문을 자동으로 수집
    """
    os.makedirs(save_dir, exist_ok=True)

    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )

    papers_data = []

    for result in search.results():
        paper_info = {
            "title": result.title,
            "authors": [author.name for author in result.authors],
            "published_date": result.published,
            "summary": result.summary,
            "pdf_url": result.pdf_url,
            "entry_id": result.entry_id,
            "categories": result.categories
        }

        papers_data.append(paper_info)

        # PDF 다운로드
        pdf_filename = f"{save_dir}/{result.entry_id.split('/')[-1]}.pdf"
        result.download_pdf(filename=pdf_filename)

        print(f"Downloaded: {result.title}")

    return papers_data

# 사용 예시
papers = collect_arxiv_papers(
    query="transformer OR attention mechanism OR BERT OR GPT",
    max_results=100
)
```

#### 구현 예시: Semantic Scholar API

```python
import requests
import json

def search_semantic_scholar(query, limit=100):
    """
    Semantic Scholar에서 논문 검색
    """
    url = "https://api.semanticscholar.org/graph/v1/paper/search"

    params = {
        "query": query,
        "limit": limit,
        "fields": "title,authors,abstract,year,citationCount,url,venue"
    }

    response = requests.get(url, params=params)
    data = response.json()

    papers = data.get("data", [])

    # 메타데이터 저장
    with open("data/raw/semantic_scholar_papers.json", "w") as f:
        json.dump(papers, f, indent=2)

    return papers

# 사용
papers = search_semantic_scholar("transformer attention", limit=100)
```

---

### 2.2 방법 2: 수동 업로드 (PDF 직접 다운로드)

**언제 사용:**
- 특정 논문만 필요할 때
- API로 접근 불가능한 논문
- 개발 초기 테스트용 소량 데이터

**프로세스:**
1. arXiv, Google Scholar 등에서 PDF 다운로드
2. `data/raw/` 폴더에 저장
3. 파일명 규칙: `{저자성}_{년도}_{키워드}.pdf`
   - 예: `Vaswani_2017_Attention_Is_All_You_Need.pdf`

**PDF 처리:**
```python
from langchain.document_loaders import PyPDFLoader

def load_manual_pdfs(pdf_dir="data/raw"):
    """
    수동으로 업로드한 PDF 파일들을 로드
    """
    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]

    all_documents = []

    for pdf_file in pdf_files:
        pdf_path = os.path.join(pdf_dir, pdf_file)
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # 메타데이터 추가
        for doc in documents:
            doc.metadata['source_file'] = pdf_file
            doc.metadata['source_type'] = 'manual_upload'

        all_documents.extend(documents)

    return all_documents
```

---

### 2.3 방법 3: 웹 크롤링 (스크래핑)

**언제 사용:**
- API가 없는 사이트
- 특정 학회 홈페이지에서 논문 수집

**주의사항:**
- 사이트 이용 약관 확인
- 크롤링 속도 제한 (Rate Limiting)
- robots.txt 준수

**구현 예시: BeautifulSoup 활용**

```python
import requests
from bs4 import BeautifulSoup
import time

def crawl_paper_website(url):
    """
    논문 사이트에서 HTML 크롤링
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    # 예: arXiv 논문 페이지에서 초록 추출
    abstract = soup.find('blockquote', class_='abstract').get_text()

    return abstract

# Rate Limiting (1초 대기)
time.sleep(1)
```

---

## 3. 데이터 수집 전략

### 3.1 단계별 수집 계획

#### Phase 1: 초기 데이터 수집 (10월 29-30일)

**목표:** 개발 및 테스트용 논문 50-100편 수집

**수집 방법:**
- 수동 업로드: 대표 논문 10편 (Transformer, BERT, GPT 등)
- arXiv API: 최신 논문 50편

**키워드:**
- "transformer"
- "attention mechanism"
- "BERT"
- "GPT"
- "large language model"
- "neural machine translation"

#### Phase 2: 본격 데이터 확장 (11월 1-3일)

**목표:** 500-1000편 논문 수집

**수집 방법:**
- arXiv API (자동)
- Semantic Scholar API (자동)
- Papers with Code API

#### Phase 3: 지속적 업데이트 (프로젝트 기간 내)

**목표:** 매일 최신 논문 자동 추가

**스케줄러 구현:**
```python
import schedule
import time

def daily_update_papers():
    """
    매일 최신 논문 자동 수집
    """
    today = datetime.now().strftime("%Y-%m-%d")

    papers = collect_arxiv_papers(
        query="cs.AI OR cs.CL OR cs.CV",
        max_results=20
    )

    print(f"[{today}] {len(papers)} new papers collected.")

# 매일 오전 9시에 실행
schedule.every().day.at("09:00").do(daily_update_papers)

while True:
    schedule.run_pending()
    time.sleep(60)
```

---

### 3.2 데이터 저장 구조

**디렉토리 구조:**
```
data/
├── raw/                    # 원본 데이터
│   ├── pdfs/              # PDF 파일
│   │   ├── Vaswani_2017_Attention.pdf
│   │   └── ...
│   ├── json/              # 메타데이터 JSON
│   │   ├── arxiv_papers.json
│   │   └── semantic_scholar_papers.json
│   └── txt/               # 추출된 텍스트
│       └── ...
├── processed/             # 전처리된 데이터
│   ├── chunks/            # 청크로 분할된 데이터
│   └── embeddings/        # 임베딩 벡터
├── rdbms/                 # PostgreSQL 백업
│   └── papers_metadata.sql
└── vectordb/              # Vector DB 저장소
    ├── paper_chunks/
    ├── abstracts/
    └── glossary/
```

---

## 4. 데이터 저장 방안

### 4.1 관계형 DB (PostgreSQL)에 메타데이터 저장

```python
import psycopg2
import json

def save_paper_metadata_to_db(paper_data):
    """
    논문 메타데이터를 PostgreSQL에 저장
    """
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")
    cursor = conn.cursor()

    cursor.execute("""
        INSERT INTO papers (
            title, authors, publish_date, source, url, abstract, category, citation_count
        )
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (url) DO NOTHING
        RETURNING paper_id
    """, (
        paper_data['title'],
        json.dumps(paper_data['authors']),  # 배열을 JSON으로 저장
        paper_data['published_date'],
        'arXiv',
        paper_data['pdf_url'],
        paper_data['summary'],
        paper_data['categories'][0] if paper_data['categories'] else None,
        0  # 초기 인용 횟수
    ))

    paper_id = cursor.fetchone()
    conn.commit()
    cursor.close()

    return paper_id[0] if paper_id else None
```

### 4.2 Vector DB에 본문 임베딩 저장

```python
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

def save_paper_to_vectordb(pdf_path, paper_id, embeddings):
    """
    논문 PDF를 Vector DB에 저장
    """
    # 1. PDF 로드
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()

    # 2. 청크 분할
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_documents(documents)

    # 3. 메타데이터 추가
    for chunk in chunks:
        chunk.metadata['paper_id'] = paper_id

    # 4. Vector DB에 저장
    vectorstore = Chroma(
        collection_name="paper_chunks",
        embedding_function=embeddings,
        persist_directory="data/vectordb/paper_chunks"
    )

    vectorstore.add_documents(chunks)
    vectorstore.persist()

    print(f"Paper {paper_id} saved to vector DB. Total chunks: {len(chunks)}")
```

---

## 5. 최신 논문 vs 기존 논문 관리

### 5.1 문제 상황

- **최신 논문**: LLM이 웹 검색으로 찾기 어려움
- **기존 논문**: RAG 지식 베이스에 저장되어 있어야 함

### 5.2 해결 방안

#### 전략 1: 하이브리드 접근 ★ 추천

**구조:**
1. **로컬 RAG DB**: 2020년~현재까지 주요 논문 500-1000편 저장
2. **웹 검색 도구**: 최신 논문 (최근 1-2주) 또는 DB에 없는 논문 검색

**라우팅 로직:**
```python
def route_paper_query(query):
    """
    질문을 분석하여 로컬 DB 검색 vs 웹 검색 결정
    """
    # 1. 최신 논문 키워드 감지
    recent_keywords = ["최신", "2025년", "오늘", "이번 주", "recent", "latest"]

    if any(keyword in query for keyword in recent_keywords):
        # 웹 검색 도구 사용
        return "web_search"

    # 2. 로컬 DB에서 검색
    results = vectorstore.similarity_search(query, k=5)

    if len(results) > 0 and results[0].metadata.get('score', 0) > 0.8:
        # 충분히 관련성 높은 문서 있음
        return "local_rag"
    else:
        # 로컬 DB에 없으면 웹 검색
        return "web_search"
```

#### 전략 2: 스케줄링을 통한 자동 업데이트

**매일 최신 논문 자동 추가:**
```python
import schedule

def daily_paper_update():
    """
    매일 arXiv에서 최신 논문 수집 및 DB 저장
    """
    # 1. 어제 날짜의 논문 수집
    yesterday = datetime.now() - timedelta(days=1)
    query = f"submittedDate:[{yesterday.strftime('%Y%m%d')}0000 TO {yesterday.strftime('%Y%m%d')}2359]"

    papers = collect_arxiv_papers(query, max_results=50)

    # 2. DB에 저장
    for paper in papers:
        paper_id = save_paper_metadata_to_db(paper)
        if paper_id:
            save_paper_to_vectordb(paper['pdf_path'], paper_id, embeddings)

    print(f"Updated {len(papers)} papers from {yesterday.date()}")

# 매일 실행
schedule.every().day.at("09:00").do(daily_paper_update)
```

#### 전략 3: 수동 추가 기능

**사용자가 직접 논문 추가:**
```python
# Streamlit UI에서 논문 URL 입력
import streamlit as st

st.title("논문 수동 추가")

paper_url = st.text_input("논문 URL 입력 (arXiv, PDF 등)")

if st.button("추가"):
    if "arxiv.org" in paper_url:
        # arXiv 논문 자동 다운로드
        arxiv_id = paper_url.split('/')[-1]
        paper = next(arxiv.Search(id_list=[arxiv_id]).results())

        # DB에 저장
        paper_id = save_paper_metadata_to_db(paper)
        save_paper_to_vectordb(f"data/raw/{arxiv_id}.pdf", paper_id, embeddings)

        st.success(f"논문 '{paper.title}'이 추가되었습니다!")
```

---

## 6. 크롤링 우선순위 및 키워드

### 6.1 우선순위 높은 논문 키워드

**카테고리 1: Transformer 기반 모델**
- "transformer"
- "attention mechanism"
- "self-attention"
- "BERT"
- "GPT"
- "T5"

**카테고리 2: Large Language Models**
- "large language model"
- "LLM"
- "few-shot learning"
- "prompt engineering"
- "in-context learning"

**카테고리 3: NLP 기초**
- "neural machine translation"
- "text classification"
- "named entity recognition"
- "question answering"

**카테고리 4: 최신 트렌드**
- "retrieval augmented generation"
- "RAG"
- "AI agent"
- "tool use"
- "function calling"

### 6.2 수집 키워드 조합

```python
PAPER_QUERIES = [
    "transformer AND attention",
    "BERT OR GPT OR T5",
    "large language model",
    "retrieval augmented generation",
    "neural machine translation",
    "question answering system",
    "AI agent OR autonomous agent",
    "prompt engineering",
    "few-shot learning",
    "transfer learning NLP"
]

for query in PAPER_QUERIES:
    papers = collect_arxiv_papers(query, max_results=50)
    # 저장 로직
```

---

## 7. 데이터 품질 관리

### 7.1 중복 제거

```python
def remove_duplicate_papers(papers):
    """
    제목 유사도 기반 중복 논문 제거
    """
    unique_papers = []
    seen_titles = set()

    for paper in papers:
        title_normalized = paper['title'].lower().strip()

        if title_normalized not in seen_titles:
            unique_papers.append(paper)
            seen_titles.add(title_normalized)

    return unique_papers
```

### 7.2 논문 필터링

**품질 기준:**
- 최소 페이지 수: 5페이지 이상
- 초록(Abstract) 존재 여부
- 저자 정보 존재 여부

```python
def filter_quality_papers(papers):
    """
    품질 기준에 맞는 논문만 필터링
    """
    filtered = []

    for paper in papers:
        if (paper.get('summary') and
            len(paper.get('summary', '')) > 100 and
            paper.get('authors')):
            filtered.append(paper)

    return filtered
```

---

## 8. 구현 예시: 통합 데이터 수집 파이프라인

```python
# scripts/collect_papers.py

import arxiv
import os
from datetime import datetime
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import psycopg2

class PaperCollectionPipeline:
    def __init__(self, db_conn, vectorstore, embeddings):
        self.db_conn = db_conn
        self.vectorstore = vectorstore
        self.embeddings = embeddings

    def collect_from_arxiv(self, query, max_results=100):
        """
        arXiv에서 논문 수집
        """
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )

        papers = []
        for result in search.results():
            papers.append({
                'title': result.title,
                'authors': [a.name for a in result.authors],
                'published_date': result.published,
                'summary': result.summary,
                'pdf_url': result.pdf_url,
                'entry_id': result.entry_id
            })

        return papers

    def save_to_db(self, paper_data):
        """
        PostgreSQL에 메타데이터 저장
        """
        cursor = self.db_conn.cursor()
        cursor.execute("""
            INSERT INTO papers (title, authors, publish_date, source, url, abstract)
            VALUES (%s, %s, %s, %s, %s, %s)
            ON CONFLICT (url) DO NOTHING
            RETURNING paper_id
        """, (
            paper_data['title'],
            str(paper_data['authors']),
            paper_data['published_date'],
            'arXiv',
            paper_data['pdf_url'],
            paper_data['summary']
        ))
        result = cursor.fetchone()
        self.db_conn.commit()
        return result[0] if result else None

    def save_to_vectordb(self, pdf_path, paper_id):
        """
        Vector DB에 논문 본문 저장
        """
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documents)

        for chunk in chunks:
            chunk.metadata['paper_id'] = paper_id

        self.vectorstore.add_documents(chunks)

    def run_pipeline(self, query, max_results=100):
        """
        전체 파이프라인 실행
        """
        print(f"Collecting papers for query: {query}")

        # 1. arXiv 수집
        papers = self.collect_from_arxiv(query, max_results)

        # 2. 저장
        for paper in papers:
            # DB 저장
            paper_id = self.save_to_db(paper)

            if paper_id:
                # PDF 다운로드
                pdf_path = f"data/raw/pdfs/{paper['entry_id'].split('/')[-1]}.pdf"
                # (다운로드 로직 생략)

                # Vector DB 저장
                self.save_to_vectordb(pdf_path, paper_id)

                print(f"Saved: {paper['title']}")

# 실행
if __name__ == "__main__":
    pipeline = PaperCollectionPipeline(db_conn, vectorstore, embeddings)
    pipeline.run_pipeline("transformer attention", max_results=100)
```

---

## 9. 참고 자료

- arXiv API 문서: https://info.arxiv.org/help/api/index.html
- Semantic Scholar API: https://api.semanticscholar.org/
- Papers with Code API: https://paperswithcode.com/api/v1/docs/
- Langchain Document Loaders: https://python.langchain.com/docs/integrations/document_loaders/
- scholarly (Google Scholar): https://scholarly.readthedocs.io/
