# 자료조사: 논문 데이터 수집 방안

## 문서 정보
- **작성일**: 2025-10-29
- **프로젝트**: 논문 리뷰 챗봇 (AI Agent + RAG)
- **팀명**: 연결의 민족

---

## 1. 논문 데이터 소스

### 1.1 주요 논문 사이트

#### 1. arXiv (★★★ 최우선 추천)

**링크:** https://arxiv.org/

**특징:**
- 오픈 액세스 논문 저장소 (무료)
- AI/ML 최신 논문이 가장 빠르게 올라옴
- PDF 직접 다운로드 가능
- API 제공 (프로그래밍 방식 접근 가능)

**카테고리:**
- cs.AI (Artificial Intelligence)
- cs.CL (Computation and Language/NLP)
- cs.CV (Computer Vision)
- cs.LG (Machine Learning)
- cs.NE (Neural and Evolutionary Computing)

**API 구성:**

**필요 라이브러리:** `arxiv`

**검색 파라미터:**

| 파라미터 | 값 예시 | 설명 |
|---------|---------|------|
| query | "attention mechanism" | 검색 쿼리 |
| max_results | 10 | 최대 결과 수 |
| sort_by | arxiv.SortCriterion.SubmittedDate | 정렬 기준 (제출일자) |

**반환되는 정보:**
- title: 논문 제목
- authors: 저자 목록
- published: 출판일
- pdf_url: PDF 다운로드 URL
- summary: 논문 초록

---

#### 2. Papers with Code (★★★ 추천)

**링크:** https://paperswithcode.com/

**특징:**
- 논문 + 구현 코드 함께 제공
- 벤치마크 및 리더보드
- State-of-the-art 모델 추적 가능
- API 제공

**API 구성:**

**필요 라이브러리:** `requests`

**API 엔드포인트:** `https://paperswithcode.com/api/v1/papers/`

**요청 파라미터:**

| 파라미터 | 값 예시 | 설명 |
|---------|---------|------|
| q | "transformer" | 검색 쿼리 |

**사용 방법:** GET 요청으로 논문 검색, JSON 형식으로 결과 반환

---

#### 3. Semantic Scholar (★★★ 추천)

**링크:** https://www.semanticscholar.org/

**특징:**
- AI 기반 논문 검색 엔진
- 논문 간 인용 관계 시각화
- 핵심 내용 요약 제공
- 무료 API (Academic Graph API)

**API 구성:**

**필요 라이브러리:** `requests`

**API 엔드포인트:** `https://api.semanticscholar.org/graph/v1/paper/{paper_id}`

**요청 파라미터:**

| 파라미터 | 값 예시 | 설명 |
|---------|---------|------|
| paper_id | "204e3073870fae3d05bcbc2f6a8e263d9b72e776" | 논문 고유 ID (BERT 논문 예시) |
| fields | "title,authors,abstract,year,citationCount,references" | 요청할 필드 목록 |

**사용 방법:** GET 요청으로 특정 논문의 상세 정보 조회, JSON 형식으로 반환

---

#### 4. Google Scholar

**링크:** https://scholar.google.com/

**특징:**
- 가장 방대한 논문 데이터베이스
- 인용 횟수 추적
- 공식 API 없음 (스크래핑 필요)

**스크래핑 라이브러리:**

**필요 라이브러리:** `scholarly`

**사용 방법:** scholarly.search_pubs() 메서드로 Google Scholar 검색

**반환되는 정보:**

| 정보 | 접근 방법 | 설명 |
|------|----------|------|
| 제목 | paper['bib']['title'] | 논문 제목 |
| 저자 | paper['bib']['author'] | 저자 목록 |
| 인용 수 | paper['num_citations'] | 인용 횟수 |

**주의:** Google Scholar는 공식 API가 없어 스크래핑이 제한될 수 있음

---

#### 5. IEEE Xplore

**링크:** https://ieeexplore.ieee.org/

**특징:**
- 전기전자공학 및 컴퓨터 과학 논문
- 고품질 peer-reviewed 논문
- API 제공 (API 키 필요)

---

#### 6. ACL Anthology (NLP 특화)

**링크:** https://aclanthology.org/

**특징:**
- NLP 분야 최고 권위 학회 논문 (ACL, EMNLP, NAACL 등)
- PDF 무료 다운로드
- BibTeX 제공

---

#### 7. PubMed (의료/생명과학)

**링크:** https://pubmed.ncbi.nlm.nih.gov/

**특징:**
- 의료 AI, BioNLP 논문
- 무료 API

---

### 1.2 논문 사이트 비교표

| 사이트 | 무료 여부 | API 제공 | AI/ML 논문 | 업데이트 속도 | 추천도 |
|--------|----------|---------|-----------|--------------|--------|
| arXiv | ✅ | ✅ | ★★★ | 매우 빠름 | ★★★ |
| Papers with Code | ✅ | ✅ | ★★★ | 빠름 | ★★★ |
| Semantic Scholar | ✅ | ✅ | ★★★ | 빠름 | ★★★ |
| Google Scholar | ✅ | ❌ | ★★★ | 빠름 | ★★ |
| IEEE Xplore | 부분 유료 | ✅ | ★★ | 보통 | ★ |
| ACL Anthology | ✅ | ✅ | ★★ (NLP만) | 보통 | ★★ |

---

## 2. 데이터 수집 방법

### 2.1 방법 1: API 활용 (자동 수집) ★ 추천

**장점:**
- 자동화 가능
- 대량 데이터 수집 용이
- 메타데이터 함께 수집
- 최신 논문 실시간 업데이트 가능

**추천 라이브러리:**
- `arxiv` (arXiv API)
- `scholarly` (Google Scholar 스크래핑)
- `requests` (일반 API 호출)
- Langchain `ArxivLoader`

#### 구현 방법: arXiv에서 자동 수집

**필요 라이브러리:** `arxiv`, `os`, `datetime`

**함수: collect_arxiv_papers**

**파라미터:**

| 파라미터 | 기본값 | 설명 |
|---------|--------|------|
| query | (필수) | 검색 쿼리 (예: "transformer OR BERT") |
| max_results | 50 | 최대 수집 논문 수 |
| save_dir | "data/raw" | PDF 저장 디렉토리 |

**처리 흐름:**
1. 저장 디렉토리 생성 (없을 경우)
2. arxiv.Search 객체 생성 (쿼리, 최대 결과, 정렬 기준)
3. 검색 결과 순회하며 각 논문 정보 수집:
   - title, authors, published_date, summary, pdf_url, entry_id, categories
4. PDF 파일 다운로드 (entry_id 기반 파일명)
5. 수집된 논문 정보 리스트 반환

**반환값:** 논문 정보 딕셔너리 리스트 (papers_data)

**사용 예시 쿼리:** "transformer OR attention mechanism OR BERT OR GPT"

#### 구현 방법: Semantic Scholar API

**필요 라이브러리:** `requests`, `json`

**함수: search_semantic_scholar**

**파라미터:**

| 파라미터 | 기본값 | 설명 |
|---------|--------|------|
| query | (필수) | 검색 쿼리 (예: "transformer attention") |
| limit | 100 | 최대 검색 결과 수 |

**API 설정:**

| 항목 | 값 | 설명 |
|------|-----|------|
| 엔드포인트 | https://api.semanticscholar.org/graph/v1/paper/search | 논문 검색 API |
| 요청 필드 | title,authors,abstract,year,citationCount,url,venue | 반환받을 메타데이터 |

**처리 흐름:**
1. GET 요청으로 논문 검색
2. JSON 응답에서 data 필드 추출
3. 메타데이터를 JSON 파일로 저장 (data/raw/semantic_scholar_papers.json)
4. 논문 리스트 반환

**사용 예시:** query="transformer attention", limit=100

---

### 2.2 방법 2: 수동 업로드 (PDF 직접 다운로드)

**언제 사용:**
- 특정 논문만 필요할 때
- API로 접근 불가능한 논문
- 개발 초기 테스트용 소량 데이터

**프로세스:**
1. arXiv, Google Scholar 등에서 PDF 다운로드
2. `data/raw/` 폴더에 저장
3. 파일명 규칙: `{저자성}_{년도}_{키워드}.pdf`
   - 예: `Vaswani_2017_Attention_Is_All_You_Need.pdf`

**PDF 처리 방법:**

**필요 라이브러리:** `langchain.document_loaders.PyPDFLoader`

**함수: load_manual_pdfs**

**파라미터:**

| 파라미터 | 기본값 | 설명 |
|---------|--------|------|
| pdf_dir | "data/raw" | PDF 파일이 저장된 디렉토리 |

**처리 흐름:**
1. 디렉토리에서 .pdf 확장자 파일 목록 추출
2. 각 PDF 파일을 PyPDFLoader로 로드
3. 각 문서에 메타데이터 추가 (source_file, source_type='manual_upload')
4. 모든 문서를 하나의 리스트로 반환

**반환값:** Document 객체 리스트 (all_documents)

---

### 2.3 방법 3: 웹 크롤링 (스크래핑)

**언제 사용:**
- API가 없는 사이트
- 특정 학회 홈페이지에서 논문 수집

**주의사항:**
- 사이트 이용 약관 확인
- 크롤링 속도 제한 (Rate Limiting)
- robots.txt 준수

**구현 방법: BeautifulSoup 활용**

**필요 라이브러리:** `requests`, `bs4.BeautifulSoup`, `time`

**함수: crawl_paper_website**

**파라미터:**

| 파라미터 | 설명 |
|---------|------|
| url | 크롤링할 논문 페이지 URL |

**처리 흐름:**
1. User-Agent 헤더 설정 (Mozilla/5.0...)
2. GET 요청으로 HTML 페이지 가져오기
3. BeautifulSoup으로 HTML 파싱
4. 필요한 요소 추출 (예: arXiv 초록 - blockquote.abstract)
5. Rate Limiting 적용 (1초 대기)

**사용 예시:** arXiv 논문 페이지에서 초록(abstract) 추출

**주의:** Rate Limiting을 반드시 적용하여 서버 부하 방지

---

## 3. 데이터 수집 전략

### 3.1 단계별 수집 계획

#### Phase 1: 초기 데이터 수집 (10월 29-30일)

**목표:** 개발 및 테스트용 논문 50-100편 수집

**수집 방법:**
- 수동 업로드: 대표 논문 10편 (Transformer, BERT, GPT 등)
- arXiv API: 최신 논문 50편

**키워드:**
- "transformer"
- "attention mechanism"
- "BERT"
- "GPT"
- "large language model"
- "neural machine translation"

#### Phase 2: 본격 데이터 확장 (11월 1-3일)

**목표:** 500-1000편 논문 수집

**수집 방법:**
- arXiv API (자동)
- Semantic Scholar API (자동)
- Papers with Code API

#### Phase 3: 지속적 업데이트 (프로젝트 기간 내)

**목표:** 매일 최신 논문 자동 추가

**스케줄러 구현 방법:**

**필요 라이브러리:** `schedule`, `time`, `datetime`

**함수: daily_update_papers**

**동작:**
- 매일 최신 논문 자동 수집
- 수집 쿼리: "cs.AI OR cs.CL OR cs.CV"
- 최대 결과: 20편
- 수집 완료 후 로그 출력

**스케줄 설정:**

| 설정 | 값 | 설명 |
|------|-----|------|
| 실행 시간 | 매일 오전 9시 | schedule.every().day.at("09:00") |
| 반복 주기 | 1분마다 체크 | time.sleep(60) |
| 실행 방식 | 무한 루프 | while True로 지속 실행 |

**사용 방법:** 백그라운드 프로세스로 실행하여 매일 자동으로 논문 수집

---

### 3.2 데이터 저장 구조

**디렉토리 구조:**
```
data/
├── raw/                    # 원본 데이터
│   ├── pdfs/              # PDF 파일
│   │   ├── Vaswani_2017_Attention.pdf
│   │   └── ...
│   ├── json/              # 메타데이터 JSON
│   │   ├── arxiv_papers.json
│   │   └── semantic_scholar_papers.json
│   └── txt/               # 추출된 텍스트
│       └── ...
├── processed/             # 전처리된 데이터
│   ├── chunks/            # 청크로 분할된 데이터
│   └── embeddings/        # 임베딩 벡터
├── rdbms/                 # PostgreSQL 백업
│   └── papers_metadata.sql
└── vectordb/              # Vector DB 저장소
    ├── paper_chunks/
    ├── abstracts/
    └── glossary/
```

---

## 4. 데이터 저장 방안

### 4.1 관계형 DB (PostgreSQL)에 메타데이터 저장

**필요 라이브러리:** `psycopg2`, `json`

**함수: save_paper_metadata_to_db**

**파라미터:**

| 파라미터 | 타입 | 설명 |
|---------|------|------|
| paper_data | dict | 논문 정보 딕셔너리 (title, authors, published_date, summary, pdf_url, categories) |

**저장되는 필드:**

| 필드 | 값 | 설명 |
|------|-----|------|
| title | paper_data['title'] | 논문 제목 |
| authors | json.dumps(paper_data['authors']) | 저자 목록 (JSON 형식) |
| publish_date | paper_data['published_date'] | 출판일 |
| source | 'arXiv' | 출처 |
| url | paper_data['pdf_url'] | PDF URL (중복 방지 키) |
| abstract | paper_data['summary'] | 논문 초록 |
| category | paper_data['categories'][0] | 카테고리 (첫 번째) |
| citation_count | 0 | 초기 인용 횟수 |

**처리 흐름:**
1. PostgreSQL 연결
2. INSERT 쿼리 실행 (ON CONFLICT DO NOTHING으로 중복 방지)
3. RETURNING paper_id로 생성된 ID 반환
4. 트랜잭션 커밋 및 연결 종료

**반환값:** paper_id (생성된 논문 ID) 또는 None (중복 시)

### 4.2 Vector DB에 본문 임베딩 저장

**필요 라이브러리:** `langchain_postgres.vectorstores.PGVector`, `langchain.text_splitter.RecursiveCharacterTextSplitter`, `PyPDFLoader`

**함수: save_paper_to_vectordb**

**파라미터:**

| 파라미터 | 타입 | 설명 |
|---------|------|------|
| pdf_path | str | PDF 파일 경로 |
| paper_id | int | PostgreSQL의 paper_id (외래 키 역할) |
| embeddings | OpenAIEmbeddings | 임베딩 함수 객체 |

**처리 흐름:**
1. **PDF 로드**: PyPDFLoader로 PDF 문서 로드
2. **청크 분할**: RecursiveCharacterTextSplitter로 텍스트 분할 (chunk_size=1000, chunk_overlap=200)
3. **메타데이터 추가**: 각 청크에 paper_id 추가
4. **Vector DB 저장**: PGVector에 청크 저장 (collection_name="paper_chunks")

**PGVector 설정:**

| 설정 | 값 | 설명 |
|------|-----|------|
| collection_name | "paper_chunks" | 컬렉션 이름 |
| embedding_function | embeddings | OpenAI 임베딩 함수 |
| connection_string | "postgresql://user:password@localhost:5432/papers" | PostgreSQL 연결 문자열 |

**출력:** 저장된 논문 ID와 총 청크 수 로그

---

## 5. 최신 논문 vs 기존 논문 관리

### 5.1 문제 상황

- **최신 논문**: LLM이 웹 검색으로 찾기 어려움
- **기존 논문**: RAG 지식 베이스에 저장되어 있어야 함

### 5.2 해결 방안

#### 전략 1: 하이브리드 접근 ★ 추천

**구조:**
1. **로컬 RAG DB**: 2020년~현재까지 주요 논문 500-1000편 저장
2. **웹 검색 도구**: 최신 논문 (최근 1-2주) 또는 DB에 없는 논문 검색

**라우팅 로직:**

**함수: route_paper_query**

**파라미터:** query (사용자 질문)

**판단 기준:**

| 조건 | 판단 결과 | 설명 |
|------|----------|------|
| 최신 논문 키워드 포함 | "web_search" | "최신", "2025년", "오늘", "이번 주", "recent", "latest" 중 하나 포함 시 |
| 로컬 DB 유사도 > 0.8 | "local_rag" | Top-K 검색 결과의 첫 번째 문서 유사도가 0.8 이상일 때 |
| 로컬 DB 결과 없음 | "web_search" | 유사도가 낮거나 결과가 없을 때 |

**처리 흐름:**
1. 최신 논문 키워드 리스트에서 매칭 검사
2. 키워드 발견 시 즉시 "web_search" 반환
3. 로컬 Vector DB에서 similarity_search 수행 (k=5)
4. 유사도 점수 기반으로 "local_rag" 또는 "web_search" 반환

#### 전략 2: 스케줄링을 통한 자동 업데이트

**매일 최신 논문 자동 추가:**

**필요 라이브러리:** `schedule`, `datetime`, `timedelta`

**함수: daily_paper_update**

**처리 흐름:**
1. 어제 날짜의 논문 수집 (submittedDate 범위 쿼리)
2. collect_arxiv_papers() 호출 (최대 50편)
3. 각 논문을 순회하며:
   - save_paper_metadata_to_db()로 메타데이터 저장
   - paper_id 반환받으면 save_paper_to_vectordb()로 벡터 저장
4. 업데이트 완료 로그 출력

**스케줄 설정:** schedule.every().day.at("09:00")로 매일 오전 9시 실행

**쿼리 예시:** `submittedDate:[202501010000 TO 202501012359]` (2025년 1월 1일 논문)

#### 전략 3: 수동 추가 기능

**사용자가 직접 논문 추가:**

**필요 라이브러리:** `streamlit`, `arxiv`

**UI 구성:**

| 컴포넌트 | 기능 | 설명 |
|---------|------|------|
| st.title | 페이지 제목 | "논문 수동 추가" 표시 |
| st.text_input | URL 입력창 | 논문 URL 입력 (arXiv, PDF 등) |
| st.button | 추가 버튼 | 버튼 클릭 시 논문 저장 |
| st.success | 성공 메시지 | 추가 완료 알림 |

**처리 흐름:**
1. 사용자가 논문 URL 입력
2. "추가" 버튼 클릭 시 URL 파싱 (arXiv ID 추출)
3. arxiv.Search로 논문 메타데이터 조회
4. save_paper_metadata_to_db()로 PostgreSQL에 저장
5. save_paper_to_vectordb()로 Vector DB에 저장
6. 성공 메시지 표시

**지원 URL:** arXiv 논문 URL (예: https://arxiv.org/abs/1706.03762)

---

## 6. 크롤링 우선순위 및 키워드

### 6.1 우선순위 높은 논문 키워드

**카테고리 1: Transformer 기반 모델**
- "transformer"
- "attention mechanism"
- "self-attention"
- "BERT"
- "GPT"
- "T5"

**카테고리 2: Large Language Models**
- "large language model"
- "LLM"
- "few-shot learning"
- "prompt engineering"
- "in-context learning"

**카테고리 3: NLP 기초**
- "neural machine translation"
- "text classification"
- "named entity recognition"
- "question answering"

**카테고리 4: 최신 트렌드**
- "retrieval augmented generation"
- "RAG"
- "AI agent"
- "tool use"
- "function calling"

### 6.2 수집 키워드 조합

**권장 키워드 쿼리 목록:**

| 번호 | 쿼리 | 설명 |
|------|------|------|
| 1 | "transformer AND attention" | Transformer 및 Attention 메커니즘 |
| 2 | "BERT OR GPT OR T5" | 주요 사전학습 모델 |
| 3 | "large language model" | 대규모 언어 모델 |
| 4 | "retrieval augmented generation" | RAG 관련 논문 |
| 5 | "neural machine translation" | 신경망 기계 번역 |
| 6 | "question answering system" | 질문 답변 시스템 |
| 7 | "AI agent OR autonomous agent" | AI Agent 관련 |
| 8 | "prompt engineering" | 프롬프트 엔지니어링 |
| 9 | "few-shot learning" | Few-shot 학습 |
| 10 | "transfer learning NLP" | 전이 학습 (NLP) |

**사용 방법:**
- 각 쿼리로 collect_arxiv_papers() 호출 (max_results=50)
- 순회하며 각 논문 수집 및 저장 로직 실행

---

## 7. 데이터 품질 관리

### 7.1 중복 제거

**함수: remove_duplicate_papers**

**파라미터:** papers (논문 리스트)

**처리 흐름:**
1. 빈 unique_papers 리스트와 seen_titles set 초기화
2. 각 논문을 순회하며:
   - 제목을 소문자로 변환하고 공백 제거 (정규화)
   - seen_titles에 없으면 unique_papers에 추가
   - seen_titles에 정규화된 제목 추가
3. 중복 제거된 unique_papers 반환

**중복 판단 기준:** 제목 정규화 후 완전 일치

**반환값:** 중복이 제거된 논문 리스트

### 7.2 논문 필터링

**품질 기준:**
- 최소 페이지 수: 5페이지 이상
- 초록(Abstract) 존재 여부
- 저자 정보 존재 여부

**함수: filter_quality_papers**

**파라미터:** papers (논문 리스트)

**필터링 조건:**

| 조건 | 기준 | 설명 |
|------|------|------|
| summary 존재 | paper.get('summary') | 초록 필드가 있어야 함 |
| summary 길이 | len(summary) > 100 | 초록이 100자 이상 |
| authors 존재 | paper.get('authors') | 저자 정보가 있어야 함 |

**처리 흐름:**
1. 빈 filtered 리스트 초기화
2. 각 논문을 순회하며 3가지 조건 모두 만족하는지 확인
3. 조건 만족 시 filtered 리스트에 추가
4. 필터링된 논문 리스트 반환

**반환값:** 품질 기준을 통과한 논문 리스트

---

## 8. 구현 방법: 통합 데이터 수집 파이프라인

**구현 파일:** `scripts/collect_papers.py`

**필요 라이브러리:** `arxiv`, `os`, `datetime`, `langchain.document_loaders.PyPDFLoader`, `langchain.text_splitter.RecursiveCharacterTextSplitter`, `psycopg2`

**클래스: PaperCollectionPipeline**

### 클래스 구조

**초기화 파라미터:**

| 파라미터 | 타입 | 설명 |
|---------|------|------|
| db_conn | psycopg2.connection | PostgreSQL 데이터베이스 연결 |
| vectorstore | PGVector | pgvector vectorstore 인스턴스 |
| embeddings | OpenAIEmbeddings | OpenAI 임베딩 함수 |

**주요 메서드:**

| 메서드 | 파라미터 | 반환 | 설명 |
|--------|---------|------|------|
| collect_from_arxiv() | query, max_results=100 | papers 리스트 | arXiv에서 논문 검색 및 메타데이터 수집 |
| save_to_db() | paper_data | paper_id 또는 None | PostgreSQL에 논문 메타데이터 저장 |
| save_to_vectordb() | pdf_path, paper_id | None | Vector DB에 논문 청크 저장 |
| run_pipeline() | query, max_results=100 | None | 전체 파이프라인 실행 (수집→저장→벡터화) |

### 메서드 상세

**1. collect_from_arxiv(query, max_results=100)**
- arXiv API로 논문 검색 (제출일자 기준 정렬)
- 각 논문의 title, authors, published_date, summary, pdf_url, entry_id 수집
- 논문 정보 딕셔너리 리스트 반환

**2. save_to_db(paper_data)**
- PostgreSQL INSERT 쿼리 실행 (papers 테이블)
- ON CONFLICT DO NOTHING으로 중복 방지
- RETURNING paper_id로 생성된 ID 반환

**3. save_to_vectordb(pdf_path, paper_id)**
- PyPDFLoader로 PDF 문서 로드
- RecursiveCharacterTextSplitter로 청크 분할 (size=1000, overlap=200)
- 각 청크에 paper_id 메타데이터 추가
- vectorstore.add_documents()로 저장

**4. run_pipeline(query, max_results=100)**

**실행 흐름:**
1. arXiv에서 논문 수집 (collect_from_arxiv)
2. 각 논문을 순회하며:
   - PostgreSQL에 메타데이터 저장 (save_to_db)
   - paper_id 반환받으면 PDF 다운로드
   - Vector DB에 청크 저장 (save_to_vectordb)
   - 저장 완료 로그 출력

**사용 예시:**
- 인스턴스 생성: `PaperCollectionPipeline(db_conn, vectorstore, embeddings)`
- 파이프라인 실행: `pipeline.run_pipeline("transformer attention", max_results=100)`

---

## 9. 참고 자료

- arXiv API 문서: https://info.arxiv.org/help/api/index.html
- Semantic Scholar API: https://api.semanticscholar.org/
- Papers with Code API: https://paperswithcode.com/api/v1/docs/
- Langchain Document Loaders: https://python.langchain.com/docs/integrations/document_loaders/
- scholarly (Google Scholar): https://scholarly.readthedocs.io/
