# 담당역할: 박재홍 - 논문 데이터 수집 및 DB 구축

## 담당자 정보
- **이름**: 박재홍
- **역할**: 데이터 인프라 구축 담당
- **참여 기간**: 단기 참여 (4일)
- **핵심 역할**: 논문 데이터 수집, Langchain Document 처리, DB 적재

---

## 담당 모듈 및 작업

### 1. 데이터 수집 (`scripts/`)
- arXiv API로 논문 수집 스크립트
- Semantic Scholar API 연동 (선택)
- 논문 메타데이터 수집

### 2. Langchain 문서 처리 (`src/data/`)
- Langchain Document Loader 구현 (PyPDFLoader)
- Langchain Text Splitter 구현 (RecursiveCharacterTextSplitter)
- PDF → Langchain Document 변환

### 3. 임베딩 및 Vector DB 적재 (`src/data/embeddings.py`)
- OpenAI Embeddings를 사용한 임베딩 생성
- Langchain PGVector (PostgreSQL + pgvector) 연동 및 문서 적재
- 용어집 데이터 임베딩 및 저장
- 배치 처리 최적화

### 4. 데이터베이스 초기 설정
- PostgreSQL 스키마 생성 (papers, glossary 테이블)
- pgvector extension 설치 및 초기화
- 벡터 컬렉션 생성
- 테스트 데이터 로드

---

## 핵심 작업: 논문 데이터 수집

### 목표
**최소 50-100편 논문 데이터**를 Langchain 파이프라인으로 처리하여 Vector DB 저장 완료

### 1. arXiv API로 논문 수집

**파일 경로**: `scripts/collect_arxiv_papers.py`

**구현 방법**:
1. `ArxivPaperCollector` 클래스 정의
   - 초기화 시 저장 디렉토리 설정 (기본값: "data/raw/pdfs")
   - 디렉토리 자동 생성

2. `collect_papers` 메서드 구현
   - arxiv.Search 객체 생성 (query, max_results, sort_by 설정)
   - 검색 결과를 순회하며 메타데이터 수집
   - 각 논문의 title, authors, published_date, summary, pdf_url, entry_id, categories, primary_category 추출
   - PDF 다운로드 (arxiv_id 기반 파일명)
   - 오류 발생 시 해당 논문 건너뛰기
   - 수집한 논문 메타데이터 리스트 반환

3. `collect_by_keywords` 메서드 구현
   - 여러 키워드로 반복 수집
   - 각 키워드당 지정된 수만큼 논문 수집
   - 전체 논문 리스트 통합
   - 중복 제거 (제목 기준)
   - 최종 수집 결과 반환

4. `remove_duplicates` 메서드 구현
   - 제목을 소문자로 정규화하여 중복 확인
   - 중복되지 않은 논문만 유지

5. 실행 스크립트 작성
   - AI/ML 관련 키워드 리스트 정의
   - 키워드당 15편씩 수집 (총 ~100편)
   - 메타데이터를 JSON 파일로 저장

---

## Langchain Document Loader 구현

### 2. PDF → Langchain Document 변환

**파일 경로**: `src/data/document_loader.py`

**구현 방법**:
1. `PaperDocumentLoader` 클래스 정의
   - RecursiveCharacterTextSplitter 초기화
   - chunk_size: 1000 (청크 크기)
   - chunk_overlap: 200 (청크 간 중복, 맥락 유지)
   - separators: ["\n\n", "\n", ". ", " ", ""] (분할 우선순위)

2. `load_pdf` 메서드 구현
   - PyPDFLoader로 PDF 파일 로드
   - 추가 메타데이터가 있으면 각 문서에 업데이트
   - Langchain Document 리스트 반환

3. `load_and_split` 메서드 구현
   - PDF 파일을 로드하고 청크로 분할
   - load_pdf()로 PDF 로드
   - text_splitter.split_documents()로 청크 분할
   - 각 청크에 chunk_id 메타데이터 추가
   - 분할된 Document 리스트 반환

4. `load_all_pdfs` 메서드 구현
   - JSON 메타데이터 파일 로드
   - arXiv ID로 메타데이터 매핑 딕셔너리 생성
   - 디렉토리의 모든 PDF 파일 순회
   - 각 PDF에 대해 메타데이터 조회 및 로드
   - load_and_split()으로 청크 분할
   - 오류 발생 시 해당 파일 건너뛰기
   - 모든 청크를 통합하여 반환

---

## 임베딩 및 Vector DB 적재

### 3. OpenAI Embeddings 생성 및 pgvector 저장

**파일 경로**: `src/data/embeddings.py`

**구현 방법**:
1. `PaperEmbeddingManager` 클래스 정의
   - OpenAI Embeddings 초기화 (모델: text-embedding-3-small)
   - API 키를 환경변수에서 로드
   - PostgreSQL + pgvector VectorStore 초기화
   - 컬렉션명: "paper_chunks"
   - PostgreSQL 연결 문자열 설정

2. `add_documents` 메서드 구현
   - Document 리스트를 배치로 나누어 처리
   - OpenAI API 속도 제한 대응을 위한 배치 처리
   - 각 배치를 vectorstore.add_documents()로 저장
   - 진행 상황 출력
   - 오류 발생 시 해당 배치 건너뛰기

3. `add_documents_with_paper_id` 메서드 구현
   - 각 문서의 URL에서 arXiv ID 추출
   - paper_id_mapping에서 PostgreSQL paper_id 조회
   - 문서 메타데이터에 paper_id 추가
   - add_documents()로 Vector DB에 저장

4. 실행 스크립트 작성
   - PaperDocumentLoader로 PDF 로드 및 분할
   - PaperEmbeddingManager로 임베딩 및 Vector DB 저장
   - 배치 크기: 50

---

## PostgreSQL 데이터베이스 설정

### 4. 스키마 생성 및 메타데이터 저장

**파일 경로**: `scripts/setup_database.py`

**구현 방법**:
1. `create_tables` 함수 구현
   - PostgreSQL 연결 및 커서 생성
   - papers 테이블 생성 (paper_id, title, authors, publish_date, source, url, category, citation_count, abstract, created_at)
   - glossary 테이블 생성 (term_id, term, definition, easy_explanation, hard_explanation, category, difficulty_level, related_terms, examples, created_at)
   - 인덱스 생성 (papers의 title, category, date 및 glossary의 term)
   - 커밋 및 완료 메시지 출력

2. `insert_paper_metadata` 함수 구현
   - JSON 메타데이터 리스트를 순회
   - 각 논문 데이터를 papers 테이블에 INSERT
   - ON CONFLICT (url) DO NOTHING으로 중복 방지
   - RETURNING paper_id로 삽입된 ID 조회
   - arxiv_id와 paper_id 매핑 딕셔너리 생성
   - 오류 발생 시 해당 논문 건너뛰기
   - 매핑 딕셔너리 반환

3. `insert_glossary_data` 함수 구현
   - 용어집 초기 데이터 리스트 정의 (Attention Mechanism, Fine-tuning, BLEU Score 등)
   - 각 용어 데이터를 glossary 테이블에 INSERT
   - ON CONFLICT (term) DO NOTHING으로 중복 방지
   - 오류 발생 시 해당 용어 건너뛰기

4. 실행 스크립트 작성
   - PostgreSQL 연결
   - create_tables()로 테이블 및 인덱스 생성
   - JSON 파일에서 논문 메타데이터 로드
   - insert_paper_metadata()로 논문 데이터 삽입
   - insert_glossary_data()로 용어집 데이터 삽입
   - paper_id_mapping을 JSON 파일로 저장

---

## 인수인계 문서

### 완료 항목 체크리스트
- [ ] arXiv에서 최소 50편 논문 수집
- [ ] PDF → Langchain Document 변환 완료
- [ ] PostgreSQL 스키마 생성 (papers, glossary)
- [ ] PostgreSQL + pgvector 컬렉션 생성 (paper_chunks, glossary_embeddings)
- [ ] 논문 메타데이터 PostgreSQL 저장
- [ ] 논문 임베딩 pgvector 저장
- [ ] 용어집 초기 데이터 삽입
- [ ] paper_id_mapping.json 생성

### 인수인계 내용

#### 1. 데이터 위치
- **PDF 파일**: `data/raw/pdfs/`
- **메타데이터**: `data/raw/arxiv_papers_metadata.json`
- **paper_id 매핑**: `data/processed/paper_id_mapping.json`
- **Vector DB**: PostgreSQL + pgvector (connection: `postgresql://user:password@localhost:5432/papers`)

#### 2. 추가 데이터 수집 방법
```bash
# 추가 논문 수집
python scripts/collect_arxiv_papers.py --query "새로운 키워드" --max-results 50

# 임베딩 및 DB 저장
python src/data/embeddings.py
```

#### 3. DB 연결 정보
- PostgreSQL + pgvector: `postgresql://user:password@localhost:5432/papers`
  - 논문 메타데이터: `papers` 테이블
  - 용어집: `glossary` 테이블
  - 벡터 임베딩: pgvector extension 사용

#### 4. 주의사항
- OpenAI API 키 환경변수 설정 필수: `OPENAI_API_KEY`
- 임베딩 생성 시 배치 크기 조절 (API 속도 제한)
- PDF 다운로드 실패 시 재시도 로직 필요

---

## Feature 브랜치

- `feature/data-collection` - arXiv 데이터 수집
- `feature/document-processing` - Document Loader/Splitter
- `feature/database-setup` - PostgreSQL + pgvector 설정

---

## 참고 자료

- arXiv API: https://info.arxiv.org/help/api/index.html
- Langchain Document Loaders: https://python.langchain.com/docs/integrations/document_loaders/
- Langchain Text Splitters: https://python.langchain.com/docs/modules/data_connection/document_transformers/
- Langchain Embeddings: https://python.langchain.com/docs/integrations/text_embedding/
- pgvector 문서: https://github.com/pgvector/pgvector
