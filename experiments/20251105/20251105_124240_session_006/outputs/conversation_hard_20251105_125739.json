[
  {
    "role": "user",
    "content": "Reinforcement Learning이란 무엇이며, 어떻게 작동해?"
  },
  {
    "role": "user",
    "content": "GAN(Generative Adversarial Network)의 기본 개념 설명해줘"
  },
  {
    "role": "user",
    "content": "2022년 이후 Attention 메커니즘 관련 논문을 연도별로 보여줘"
  },
  {
    "role": "user",
    "content": "2023년 이후 Self-supervised learning 관련 논문 찾아줘"
  },
  {
    "role": "assistant",
    "content": "### 중급자용 (23-30세)\n\n아래는 2023년 이후(Self-supervised learning, SSL) 주요 논문들을 모달리티별로 정리한 큐레이션입니다. 각 방법의 핵심 아이디어(대조·마스킹·JEPA 등), 장단점, 실무 적용 포인트와 함께, 핵심 손실함수의 파이토치 예제 코드도 포함했습니다. 필요하시면 특정 도메인(비전·오디오·비디오·시계열·의료 등)으로 좁혀 상세 리뷰/비교표/재현용 스크립트를 드릴 수 있습니다.\n\n1) 이미지(비전) SSL\n- DINOv2: Learning Robust Visual Features without Supervision (Meta, 2023)\n  - 요지: ViT를 교사-학생 자가증류(EMA teacher, centering, sharpening, multi-crop)로 학습. 대규모 무라벨 데이터(LVD-142M) 큐레이션과 스케일링 레시피로 선형 프로빙/전이 성능 대폭 향상.\n  - 방법: 비대칭 teacher-student, 다중뷰 증강, non-contrastive(분산/정규화로 collapse 방지).\n  - 장점: 강력한 전이, 다운스트림에서 적은 라벨로도 우수. 단점: 증강/하이퍼 파고들기와 대규모 데이터·연산 필요.\n  - 코드: facebookresearch/dinov2 (공식)\n\n- I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (Meta/LeCun, 2023; ICLR 2024)\n  - 요지: JEPA(공동 임베딩 예측). 픽셀 복원 대신 잠재공간 예측. 컨텍스트 패치로부터 가려진 타깃 패치의 representation을 직접 예측.\n  - 방법: target encoder(gradient stop) + context encoder + predictor; 마스킹된 블록에 대해 L2/코사인 손실. 대조학습/픽셀 재구성 없이 collapse를 구조적으로 방지.\n  - 장점: 적은 증강 의존, 저주파 복원 편향 없음, 스케일 효율. 단점: 구현 복잡도와 마스킹 설계 감도.\n  - 코드: facebookresearch/ijepa\n\n- MAGE: Masked Generative Encoder (2023, NeurIPS)\n  - 요지: 마스킹 기반 생성적 프리텍스트와 판별적 인코더의 결합. MIM(픽셀/feature 복원)과 contrastive의 장점 절충.\n  - 장점: 다양한 다운스트림에 안정적. 단점: 학습 레시피가 다소 복잡.\n\n- 추가: Masked Feature Prediction 개선 계열(예: MaskFeat 계열 후속, SimMIM 계열 후속), SemMAE/MAE 변형(semantic-aware masking 등)도 2023~24에 다수 보고.\n\n2) 비디오 SSL\n- VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking and Efficient Sampling (2023)\n  - 요지: VideoMAE(2022)의 확장. 시공간 이중 마스킹, 효율적 프레임 샘플링, 더 큰 모델/데이터로 스케일.\n  - 장점: 영상 인식/탐지/행동분류 전반에 강함. 단점: GPU 메모리·I/O 부담 큼.\n  - 코드: MCG-NJU/VideoMAE\n\n- V-JEPA: Joint-Embedding Predictive Architecture for Video (Meta, 2024)\n  - 요지: 영상에 JEPA를 적용. 현재/미래 스패티오-템포럴 토큰 잠재를 직접 예측.\n  - 장점: 장면 동역학 학습에 유리, 픽셀 복원 대비 고수준 표현. 단점: 마스킹/뷰 설계가 성능에 민감.\n\n3) 오디오/음성 SSL\n- BEATs: Audio Pre-Training with Acoustic Tokenizers (Microsoft, 2023)\n  - 요지: teacher 모델이 만든 음향 토큰/feature를 예측하는 BERT-스타일 프리트레이닝.\n  - 장점: 라벨 없이 ASR/음향이벤트/스피커 태스크 전이 우수. 단점: teacher 의존성·토크나이저 편향.\n  - 코드: microsoft/BEATs\n\n- data2vec 2.0 (Meta, 2023)\n  - 요지: 음성·비전·텍스트 공통 JEPA류 프레임워크. teacher의 잠재표현을 학생이 예측.\n  - 장점: 멀티모달 범용 SSL, 효율적. 단점: 구현·튜닝 복잡.\n\n4) 멀티모달 SSL\n- ImageBind: One Embedding Space To Bind Them All (Meta, 2023)\n  - 요지: 이미지-텍스트-오디오-Depth-IMU-열화상 등 이질 모달을 라벨 없이 동시 임베딩 정렬. 공존(co-occurrence) 기반 대조/정렬.\n  - 장점: 제로샷 cross-modal retrieval/전이. 단점: 데이터 소스 품질·편향 관리 필요.\n  - 코드: facebookresearch/ImageBind\n\n5) 3D/포인트클라우드 SSL\n- Point-MAE v2 / 3D MAE 변형들 (2023)\n  - 요지: 포인트클라우드에 마스킹-복원 프레임워크 확장, sparse/voxel 혼합 토큰화, 인코더 경량화.\n  - 장점: 라벨 부족한 3D 도메인에 효과적. 단점: 데이터 전처리/샘플링 설계 난이도.\n\n핵심 알고리즘 분류와 비교\n- Contrastive(InfoNCE)\n  - 아이디어: 같은 인스턴스의 증강 뷰는 끌어당기고(positive), 다른 인스턴스는 밀어냄(negative).\n  - 장점: 강한 선형 프로빙, 시멘틱 클러스터링. 단점: 대배치/메모리뱅크 필요, 증강 의존도 높음.\n  - 손실: L = -log exp(sim(q,k+)/τ) / Σ_j exp(sim(q,k_j)/τ)\n\n- Masked Image/Video Modeling(MIM/MVM)\n  - 아이디어: 입력 일부 마스킹 후 픽셀/feature 복원. MAE는 경량 decoder, heavy masking(60~90%).\n  - 장점: 대배치/네거티브 불필요, 안정적 스케일링. 단점: 저수준 복원 편향, 고수준 의미 학습 한계.\n\n- Non-contrastive Distillation(BYOL/DINO/DINOv2)\n  - 아이디어: teacher-student로 같은 이미지 뷰 간 임베딩 일치. EMA/정규화로 collapse 방지.\n  - 장점: 네거티브 불필요, 강한 전이. 단점: 레시피 복잡, 증강 설계 민감.\n\n- JEPA(I-JEPA/V-JEPA, data2vec 계열)\n  - 아이디어: 보이는 컨텍스트로 가려진 타깃의 잠재표현을 직접 예측(픽셀 복원·네거티브 불필요).\n  - 장점: 의미 표현에 집중, 증강 의존 낮음, 스케일 효율. 단점: 마스킹/뷰/아키텍처 설계 중요.\n\n실무 적용 시 고려사항\n- 데이터: 대규모 무라벨 수집 시 중복 제거나 품질 필터링(DINOv2는 aesthetic/near-dup/NSFW 필터) 성능 영향 큼.\n- 증강: contrastive는 강한 컬러/크롭/멀티크롭, MIM/JEPA는 과도한 색 왜곡 지양, 공간·블록 마스킹 비율(이미지 40~75%, 비디오 시공간 혼합).\n- 모델/옵티마이저: ViT-B/L/H + AdamW, weight decay 0.05~0.2, cosine schedule, warmup; DINO류는 teacher EMA 0.996~0.9995.\n- 배치/분산: contrastive는 큰 글로벌 배치(>= 1024) 유리, MIM/JEPA는 상대적으로 덜 민감하나 I/O와 마스크 샘플링 효율 중요.\n- 평가: 선형 프로빙 vs 풀 파인튜닝, k-NN, 검출/세그/키포인트/비디오 전이. 도메인 특화(의료/위성/산업)일수록 파인지닝 전략이 성능 좌우.\n\n핵심 손실/학습 루프 미니멀 PyTorch 예시\n\n1) Contrastive InfoNCE (SimCLR식)\nimport torch\nimport torch.nn.functional as F\n\ndef info_nce_loss(z1, z2, temperature=0.1):\n    # z1, z2: [B, D], L2 normalize\n    z1 = F.normalize(z1, dim=-1)\n    z2 = F.normalize(z2, dim=-1)\n    B = z1.size(0)\n    reps = torch.cat([z1, z2], dim=0)                     # [2B, D]\n    sim = reps @ reps.t() / temperature                   # [2B, 2B]\n    mask = torch.eye(2*B, dtype=torch.bool, device=sim.device)\n    sim.masked_fill_(mask, -1e9)\n\n    # positives are on (i, i+B) and (i+B, i)\n    pos = torch.cat([torch.arange(B)+B, torch.arange(B)], dim=0).to(sim.device)\n    target = pos\n    loss = F.cross_entropy(sim, target)\n    return loss\n\n2) MAE식 마스킹 복원(MSE on pixels or features)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SimpleMAE(nn.Module):\n    def __init__(self, encoder, decoder, patch_embed, mask_ratio=0.75):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.patch_embed = patch_embed\n        self.mask_ratio = mask_ratio\n\n    def forward(self, imgs):\n        # patchify\n        patches = self.patch_embed(imgs)    # [B, N, D]\n        B, N, D = patches.shape\n\n        # random mask\n        num_mask = int(self.mask_ratio * N)\n        idx = torch.rand(B, N, device=imgs.device).argsort(dim=1)\n        keep_idx = idx[:, :-num_mask]\n        mask_idx = idx[:, -num_mask:]\n\n        patches_keep = torch.gather(patches, 1, keep_idx[..., None].expand(-1, -1, D))\n        enc = self.encoder(patches_keep)    # [B, Nk, De]\n\n        # decode to reconstruct masked tokens\n        dec_in = self._insert_mask_tokens(enc, keep_idx, N)\n        rec = self.decoder(dec_in)          # [B, N, patch_dim]\n        # compute loss only on masked tokens\n        target = self._patchify_pixels(imgs)  # [B, N, patch_dim]\n        target_masked = torch.gather(target, 1, mask_idx[..., None].expand(-1, -1, target.size(-1)))\n        rec_masked = torch.gather(rec, 1, mask_idx[..., None].expand(-1, -1, rec.size(-1)))\n        loss = F.mse_loss(rec_masked, target_masked)\n        return loss\n\n    def _insert_mask_tokens(self, enc, keep_idx, N):\n        # 구현 단순화: 자리 채우기용 mask token 생성 후 원래 순서로 복원\n        B, Nk, De = enc.shape\n        mask_tokens = enc.new_zeros(B, N - Nk, De)\n        tokens = torch.cat([enc, mask_tokens], dim=1)\n        # 복원 인덱스는 생략(학습 코드에서는 복원 permutation 필요)\n        return tokens\n\n3) I-JEPA식 잠재 예측(타깃 임베딩 예측, stop-grad)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass IJEPALoss(nn.Module):\n    def __init__(self, context_encoder, target_encoder, predictor, proj_dim=1024, mask_generator=None):\n        super().__init__()\n        self.context_encoder = context_encoder\n        self.target_encoder = target_encoder\n        self.predictor = predictor\n        self.mask_generator = mask_generator\n\n    @torch.no_grad()\n    def _stop_grad(self, x):\n        return x.detach()\n\n    def forward(self, img):\n        # generate block-wise masks: visible(ctx_mask) and target blocks(tgt_mask)\n        ctx_mask, tgt_mask = self.mask_generator(img)   # boolean masks over patch grid\n        # encode\n        z_ctx = self.context_encoder(img, ctx_mask)     # [B, N_ctx, D]\n        with torch.no_grad():\n            z_tgt = self._stop_grad(self.target_encoder(img, tgt_mask))  # [B, N_tgt, D]\n        # predict target embeddings from context tokens\n        pred = self.predictor(z_ctx, tgt_mask)          # [B, N_tgt, D]\n        # cosine or L2 loss on target blocks\n        pred = F.normalize(pred, dim=-1)\n        z_tgt = F.normalize(z_tgt, dim=-1)\n        loss = 2 - 2 * (pred * z_tgt).sum(dim=-1).mean()\n        return loss\n\n비교 요약\n- 대규모 이미지 표현: DINOv2(자가증류)와 I-JEPA(JEPA)가 2023~24 기준 강력한 축. 대조학습 대비 적은 증강 의존/스케일 효율이 장점.\n- 영상: VideoMAE v2(마스킹 복원) vs V-JEPA(잠재 예측). 동역학/미래예측이 중요하면 V-JEPA가 유리, 넓은 벤치마크 커버리지는 VideoMAE v2가 안전한 선택.\n- 오디오/음성: BEATs, data2vec 2.0이 범용 전이 성능 우수.\n- 멀티모달: ImageBind가 라벨 없이 크로스모달 임베딩 정렬의 대표작.\n\n참고(검색 키워드와 공식 저장소)\n- DINOv2 — “DINOv2: Learning robust visual features without supervision”, GitHub: facebookresearch/dinov2\n- I-JEPA — “Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture”, GitHub: facebookresearch/ijepa\n- V-JEPA — “Joint-Embedding Predictive Architecture for Video”, Meta AI resources\n- VideoMAE V2 — “Scaling Video Masked Autoencoders…”, GitHub: MCG-NJU/VideoMAE\n- BEATs — “Audio Pre-Training with Acoustic Tokenizers”, GitHub: microsoft/BEATs\n- data2vec 2.0 — fairseq examples: facebookresearch/fairseq\n- ImageBind — “One Embedding Space to Bind Them All”, GitHub: facebookresearch/ImageBind\n- MAGE — “Masked Generative Encoder”, 검색 키워드: “MAGE NeurIPS 2023”\n\n원하시면\n- 특정 모달리티(예: 의료 영상, 위성/원격탐사, 로보틱스) 중심으로 2023~2024 논문만 골라, 벤치마크 점수/학습 레시피/리소스 요구사항을 표로 정리하고\n- BibTeX와 함께 재현 코드 스캐폴드(train.py, config, 데이터 파이프라인)까지 제공해 드릴게요. 어느 분야를 더 깊게 보길 원하시나요?\n\n---\n\n### 고급자용 (30세 이상)\n\n요청 범위 메모: 제 지식은 2024-10까지의 공개 논문을 포함합니다. 아래는 2023년 이후(= 2023–2024 기간) Self-Supervised Learning(SSL)의 핵심 논문과 동향을 분야별로 정리한 것입니다. 2025년 최신 업데이트나 특정 하위분야(비전/음성/그래프/시계열 등)로 더 좁혀 드리길 원하시면 알려주세요.\n\n핵심 논문(2023–2024)\n- Vision (이미지)\n  - Assran et al., 2023, I-JEPA: Image-based Joint-Embedding Predictive Architecture. 예측적 자기지도(복원 대신 잠재표현 예측) 패러다임을 정립. 대규모 ViT에서 라벨 없이 강력한 선형 프로브/전이 성능 보고.\n  - Oquab et al., 2023, DINOv2: 대규모 비전 자기지도 사전학습의 강력한 베이스라인. 라벨 없는 데이터 큐레이션+ViT 스케일링으로 분류/검출/세그먼테이션 전이에서 SOTA급.\n  - Bao et al., 2023, CAE v2: Masked Autoencoder(MAE) 계열의 문맥 예측 강화. 재구성 편향을 줄이고 전이 성능 개선.\n- Vision (비디오)\n  - Wang et al., 2023, VideoMAE v2: 비디오용 MAE의 스케일링/효율화. Kinetics, Something-Something 등에서 비지도 사전학습 후 SOTA 전이 성능.\n  - Assran et al., 2024, V-JEPA: 비디오용 JEPA. 시간적 예측을 통해 단기 변형에는 불변, 장기 변화에는 민감한 표현을 학습.\n- 멀티모달(비전-언어; 약지도/자기지도 경계)\n  - Fang et al., 2023, EVA-CLIP: 대규모 이미지-텍스트 대조학습 스케일링과 로버스트니스 개선.\n  - Zhai et al., 2024, SigLIP 2: CLIP 대비 단순한 시그모이드 대조목적과 데이터/모델 스케일링으로 멀티모달 전반 성능 향상. (텍스트 짝 데이터를 쓰므로 “완전” 자기지도는 아님)\n- 음성/오디오\n  - Pratap et al., 2023, MMS: 초대규모 자기지도 음성 사전학습으로 다언어 인식/합성 기초모델. 레이블 희소 언어에서 강점.\n- 그래프\n  - Hou et al., 2023, GraphMAE2: 그래프용 마스킹-복원 기반 SSL의 안정성/전이 성능 강화(노드·링크 예측, 분류 전이).\n- 시계열\n  - (대표 동향) Masked reconstruction·변형-불변 대조학습·예측적(autoregressive/JEPA류) 목적의 스케일링. 도메인(의료/산업) 특화 마스킹 설계가 성능 좌우.\n\n핵심 목적함수 개요(수식)와 복잡도\n- 대조학습(InfoNCE; SimCLR/CLIP 계열)\n  - LInfoNCE = − Σi log [ exp(sim(z_i, z_i+)/τ) / Σj exp(sim(z_i, z_j)/τ) ], sim(u,v)=u^T v / (||u||·||v||)\n  - 배치당 복잡도: 유사도 계산 O(B^2 d), 정규화 O(B^2). 메모리 뱅크/모멘텀 인코더(MoCo류)로 음수 샘플 수를 확장.\n  - 장점: 불변 표현에 강함. 단점: 양/음수 정의 민감, O(B^2) 스케일 병목.\n- 마스킹-복원(MAE/CAE/VideoMAE)\n  - 입력 x를 마스크 M로 부분 관측 xM, 인코더 f로 잠재 h=f(xM), 디코더 g로 복원 x̂=g(h); Lrec = ||x − x̂||_p (대개 p=2) 또는 지각적 손실.\n  - 시퀀스 길이 N, 잠재 차원 d, 마스크율 m일 때, 인코더 FLOPs ~ O(B (1−m) N d^2) (ViT 기준), 디코더 O(B N d^2), m↑로 인코더 비용↓. 고해상도/장시퀀스에 효율적.\n  - 장점: 음수 샘플 불필요, 스케일에 유리. 단점: 픽셀/스펙트럼 복원 편향.\n- JEPA(I/V-JEPA)\n  - 컨텍스트 c와 타깃 t를 분할(공간/시간). 타깃 임베딩 y = stopgrad(ϕ(t)), 예측 p = ψ(f(c)); 손실 LJEPA = ||p − y||_2^2 (또는 cosine/MSE). 복원 대신 잠재예측이므로 저주파 수월성/저수준 복원 편향 감소.\n  - 복잡도: 인코더 두 번(컨텍스트/타깃) O(BNd^2) + 예측기 ψ의 O(Bd^2); 대조학습처럼 O(B^2) 유사도 계산이 없어 대규모 배치에 유리.\n  - 이론적 관점: mutual information 최대화 대신 예측가능한 부분만 보존하는 추상화 가설(LeCun, 2022 제안)을 실증.\n\n실험·벤치마크 요약(주요 경향)\n- DINOv2(Oquab et al., 2023): ImageNet-1K 선형 프로브·k-NN에서 강력, COCO/Cityscapes/ ADE20K 전이에서 감독 대비 경쟁력 보고. 데이터 큐레이션·학습 레시피가 결정적.\n- I-JEPA(Assran et al., 2023): ImageNet 선형 프로브에서 MAE 대비 우수, 적은 학습 스텝으로 수렴. 디코더가 없는 구조로 추론 효율성↑.\n- VideoMAE v2(Wang et al., 2023): Kinetics-400/600, Something-Something V2에서 자기지도 사전학습 후 미세조정 시 SOTA 혹은 준SOTA. 높은 마스크율과 sparse tube 샘플링이 효과적.\n- V-JEPA(Assran et al., 2024): 비디오에서 시간적 예측으로 행동인식·템포럴 전이에서 강함. 복원 없이 잠재예측이라 계산·스케일 효율.\n- EVA-CLIP(Fang et al., 2023), SigLIP 2(Zhai et al., 2024): 이미지-텍스트 대조에서 제로샷/리트리벌/VL 전이 개선. 완전 자기지도는 아니지만 대조목적 설계의 최신 추세를 대변.\n\n최근 연구 동향 비교\n- 복원(reconstruction) → 예측(prediction)으로: MAE 계열의 한계를 줄이고 JEPA류가 고수준 의미표현에 유리함을 보임(Assran et al., 2023/2024).\n- 불변(invariance) + 등변(equivariance) 공존: 순수 불변 대조의 한계를 보완하려 회전/기하 변환에 대한 등변 제약을 추가하는 SSL이 확산(예: equivariant regularization, 2023–2024 다수 후속).\n- 비디오·시계열로 확장: 높은 마스크율·튜브 마스킹, 장문맥 모델링(메모리/스트라이드) 등 효율 중심 설계.\n- 데이터 스케일·큐레이션의 중요성: DINOv2는 레시피·필터링이 핵심임을 보였고, 멀티모달(이미지-텍스트)도 캡션 품질/필터가 성능 좌우.\n- 평가 프로토콜의 다변화: 선형 프로브, 저샷, 분류 외 검출/세그/리트리벌/로버스트니스/OOD/공정성 등 다면 평가.\n\n알고리즘 복잡도 비교(요지)\n- 대조학습: O(B^2 d) 유사도 → 큰 B에서 병목. 메모리/시간 비용 큼.\n- MAE/CAE: O(B (1−m) N d^2) 인코더 우세, m↑로 비용 감소. 고해상도 시 효율적.\n- JEPA: O(B N d^2), pairwise 유사도 없음. 디코더 제거로 메모리·시간 효율. 비디오 확장 시 튜브 샘플링으로 시간축 비용 제어.\n\n미해결 문제와 향후 연구 방향\n- 자기지도 평가 표준화: 전이 세팅/데이터 누수/하이퍼 최적화 공정성.\n- 등변성 학습의 일반해: 변환군(G) 전반에 대한 이론적 보장과 실용적 레시피 결합.\n- 장문맥 시공간 모델링: 수만 프레임/초장기 시계열에서의 메모리-계산 트레이드오프(부분 관측, 압축 상태, 기호적 요약).\n- 데이터 큐레이션·저작권/편향: “라벨 없는” 데이터의 편향 제어와 합법성.\n- 멀티모달에서의 “진정한” 자기지도: 텍스트 의존 없이 다감각 정렬을 학습하는 방법(예: ImageBind 계열의 확장).\n- 경량화·온디바이스 SSL: 사전학습·어댑테이션의 효율적 저자원화.\n\n참고 문헌(선정)\n- Assran, M. et al., 2023. Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (I-JEPA).\n- Assran, M. et al., 2024. V-JEPA: Self-Supervised Learning from Video with Joint-Embedding Prediction.\n- Oquab, M. et al., 2023. DINOv2: Learning Robust Visual Features without Supervision.\n- Bao, H. et al., 2023. CAE v2: Context Autoencoders for Self-Supervised Representation Learning.\n- Wang, J. et al., 2023. VideoMAE v2: Scaling Video Masked Autoencoders with Transformers.\n- Fang, Y. et al., 2023. EVA-CLIP: Improved Training of CLIP at Scale.\n- Zhai, X. et al., 2024. SigLIP 2: Multimodal Contrastive Learning with a Sigmoid Loss at Scale.\n- Pratap, V. et al., 2023. MMS: Scaling Speech Self-Supervision to Massive Multilingual Data.\n- Hou, Z. et al., 2023. GraphMAE2: Revisiting Masked Autoencoders for Graphs.\n\n원하시면\n- 특정 하위 분야(예: 비전-이미지 vs 비디오 vs 음성 vs 그래프 vs 시계열)\n- 과제(분류/검출/세그/리트리벌/로버스트니스)\n- 형식(BibTeX/CSV)과 함께, 2024년 논문 위주로 더 많은 타이틀과 링크, 간단 요약, 데이터셋·성능 표, 연산량(FLOPs/Throughput)까지 정리해 드리겠습니다.\n\n---",
    "tool_choice": "general",
    "sources": null
  },
  {
    "role": "assistant",
    "content": "**질문**: 2022년 이후 Attention 메커니즘 관련 논문을 연도별로 보여줘\n\n**생성된 SQL**:\n```sql\nSELECT EXTRACT(YEAR FROM publish_date) AS year, COUNT(*) AS paper_count FROM papers WHERE publish_date >= DATE '2022-01-01' AND abstract ILIKE '%attention%' OR title ILIKE '%attention%' OR authors ILIKE '%attention%' GROUP BY EXTRACT(YEAR FROM publish_date) ORDER BY year DESC LIMIT 100;\n```\n**결과**:\n\nyear | paper_count\n--- | ---\n2025 | 1\n2024 | 6\n2023 | 3\n2022 | 4\n2021 | 4\n2017 | 1",
    "tool_choice": "text2sql",
    "sources": null
  },
  {
    "role": "user",
    "content": "\"Attention Is All You Need\" 논문의 내용을 요약해줘"
  },
  {
    "role": "user",
    "content": "요약한 내용을 마크다운 형식으로 깔끔하게 정리해줘"
  },
  {
    "role": "user",
    "content": "BERT와 GPT 논문 비교해서 분석하고 저장해줘"
  },
  {
    "role": "assistant",
    "content": "### 중급자용 (23-30세)\n\n아래는 BERT(BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018)와 GPT(Improving Language Understanding by Generative Pre-Training, 2018) 원 논문을 기준으로 한 비교·분석입니다. 마지막에 이 분석 결과를 파일로 저장하는 방법(파이썬 스크립트)도 제공합니다.\n\n1) 한눈에 보는 차이\n- 아키텍처\n  - BERT: Transformer Encoder 스택. 양방향 self-attention.\n  - GPT: Transformer Decoder 스택. 좌→우 causal(미래 차단) self-attention.\n- 사전학습(Objective)\n  - BERT: Masked Language Modeling(MLM) + Next Sentence Prediction(NSP).\n  - GPT: Causal Language Modeling(CLM; next-token prediction).\n- 입력/토크나이저\n  - BERT: [CLS], [SEP], segment(A/B) + position + WordPiece(≈30k).\n  - GPT: 단일 segment + position + BPE(원 GPT는 BooksCorpus 기반 BPE).\n- 사전학습 데이터(원 논문 시점)\n  - BERT: BooksCorpus + English Wikipedia.\n  - GPT: BooksCorpus.\n- 파인튜닝 전략\n  - BERT: 태스크별 작은 헤드 추가(분류, QA span 등).\n  - GPT: 통합된 생성 포맷으로 태스크 입력을 재구성하고, 동일 LM 헤드를 통해 학습(필요시 얕은 헤드 추가).\n- 강점/약점(요약)\n  - BERT: 이해/인코딩 중심 태스크에서 강함(GLUE, SQuAD 등), bidirectional 문맥으로 특징 추출 우수. 텍스트 생성은 약함.\n  - GPT: 생성/완성/요약/대화 등 생성형 태스크에서 자연스러움. 이해형 태스크는 2018 기준 BERT 대비 약함.\n\n2) 학습 목표(수식)와 마스킹 차이\n- GPT의 Causal LM\n  - 목표: 최대화 L_GPT = sum_{t=1..T} log pθ(x_t | x_1..x_{t-1})\n  - 어텐션 마스크: 엄격한 하삼각(causal) 마스크. 미래 토큰 접근 금지.\n- BERT의 MLM(+NSP)\n  - MLM 목표: L_MLM = sum_{i∈M} log pθ(x_i | x_{/M})\n    - M: 15% 정도 뽑힌 마스크 위치 집합\n    - 80% [MASK], 10% 랜덤 토큰, 10% 원토큰 유지\n  - NSP 목표: 두 문장(S_A, S_B)이 실제 이어지는지 이진 분류. L_NSP = log pθ(y | S_A, S_B)\n  - 총 손실: L = L_MLM + λ L_NSP (논문에서는 함께 학습)\n  - 어텐션: 완전 양방향. 모든 토큰이 서로를 참조(단, 마스킹된 위치의 라벨만 손실 계산).\n\n3) 입력 표현과 포맷\n- BERT\n  - [CLS] S_A [SEP] S_B [SEP]\n  - Segment embedding(E_A/E_B), position embedding, token embedding 합\n  - NSP 덕에 두 문장 관계학습 가능(후속 연구에서는 NSP 제거한 RoBERTa가 더 좋았음)\n- GPT\n  - 단일 시퀀스에 special token과 템플릿을 섞어 태스크 포맷팅\n  - 예: premise </delim> hypothesis </delim> [answer token] 식 구성 후, 다음 토큰 예측으로 분류 유도\n\n4) 왜 성능 차이가 났나(2018 시점)\n- 데이터/양방향성: BERT는 Wikipedia까지 사용하고 양방향 문맥을 활용하여 문장 이해가 크게 향상.\n- 태스크 정렬: BERT는 분류/QA 등의 discriminative 태스크에 바로 맞는 head와 손실 설계. GPT는 생성 프레임에 태스크를 매핑하므로 2018 기준에는 약간의 간접성 존재.\n- 모델 규모: BERT Large(340M)가 GPT(117M)보다 규모가 커 파인튜닝 성능 우위.\n\n5) 알고리즘 의사코드\n- BERT(MLM+NSP) 미니배치 학습\n  ```\n  for batch in loader:\n      tokens_a, tokens_b = batch.sent_a, batch.sent_b\n      input_ids, segment_ids = pack_with_special_tokens(tokens_a, tokens_b)\n      positions = arange(len(input_ids))\n      # MLM 마스킹\n      mask_positions = sample_15_percent(input_ids, avoid_special=True)\n      labels = input_ids.clone()\n      labels[~mask_positions] = -100  # ignore index\n\n      input_ids_masked = apply_80_10_10_masking(input_ids, mask_positions, vocab)\n\n      h = Encoder(input_ids_masked, segment_ids, positions)  # bidirectional attention\n      mlm_logits = MLMHead(h)\n      mlm_loss = cross_entropy(mlm_logits, labels, ignore_index=-100)\n\n      nsp_logits = NSPHead(h[CLS_index])\n      nsp_loss = binary_cross_entropy_with_logits(nsp_logits, batch.is_next)\n\n      loss = mlm_loss + nsp_loss\n      loss.backward(); optimizer.step()\n  ```\n- GPT(Causal LM) 미니배치 학습\n  ```\n  for batch in loader:\n      input_ids = batch.input_ids  # x_1..x_T\n      positions = arange(len(input_ids))\n      attn_mask = causal_triangle_mask(T)  # t는 <= t 위치만 볼 수 있음\n\n      h = Decoder(input_ids, positions, attn_mask)  # masked self-attention\n      logits = LMHead(h)\n      loss = cross_entropy(logits[:, :-1], input_ids[:, 1:])  # next-token\n      loss.backward(); optimizer.step()\n  ```\n\n6) PyTorch 예시 코드\n- BERT-스타일 MLM 마스크와 손실\n  ```python\n  import torch\n  from transformers import BertTokenizerFast, BertForMaskedLM\n\n  tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n  model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n\n  texts = [\"Machine learning is [MASK] fun.\", \"Transformers are [MASK] powerful.\"]\n  enc = tokenizer(texts, padding=True, return_tensors=\"pt\")\n\n  # 이미 [MASK]가 들어간 예시이므로 그대로 사용\n  labels = enc[\"input_ids\"].clone()\n  # [MASK]가 아닌 위치는 -100으로 ignore\n  mask_token_id = tokenizer.mask_token_id\n  mask_positions = enc[\"input_ids\"].eq(mask_token_id)\n  labels[~mask_positions] = -100\n\n  out = model(**enc, labels=labels)\n  loss = out.loss\n  loss.backward()\n  ```\n- GPT-스타일 Causal LM 손실\n  ```python\n  import torch\n  from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n\n  tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n  model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n\n  texts = [\"Today I went to the\", \"The capital of France is\"]\n  enc = tokenizer(texts, padding=True, return_tensors=\"pt\")\n  input_ids = enc[\"input_ids\"]\n  attention_mask = enc[\"attention_mask\"]\n\n  # GPT2 모델은 내부에서 causal mask를 적용\n  out = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n  loss = out.loss\n  loss.backward()\n  ```\n- 직접 causal 마스크 만들기(커스텀 모델용)\n  ```python\n  def causal_mask(sz: int):\n    # [T,T] 상삼각을 -inf로, 하삼각(포함)은 0으로\n    mask = torch.full((sz, sz), float(\"-inf\"))\n    mask = torch.triu(mask, diagonal=1)\n    return mask\n\n  T = 10\n  attn_bias = causal_mask(T)  # 이를 attention score에 더해 softmax 전에 적용\n  ```\n\n7) 성능·평가(원 논문 기준)\n- BERT\n  - GLUE, SQuAD 1.1/2.0, SWAG 등에서 당시 SOTA 다수 달성. 특히 QA span 예측이 강함.\n- GPT\n  - 다양한 NLU 벤치마크에서 기존 RNN/ELMo 기반 대비 큰 향상. 단, GLUE·QA에서 BERT Large 대비 열세.\n- 이후 발전\n  - BERT 계열: RoBERTa(NSP 제거, 대규모 데이터/배치), ALBERT(파라미터 공유), DeBERTa(상대적 position).\n  - GPT 계열: GPT-2/3(스케일업), InstructGPT/ChatGPT(지도/강화학습 기반 지시 따르기).\n\n8) 장단점 비교\n- BERT(Encoder, 양방향)\n  - 장점: 문장·문서 인코딩, 분류/매칭/추출 기반 태스크에 강함. feature extractor로 유용.\n  - 단점: 생성 태스크에 직접 쓰기 어려움. [MASK] 토큰 mismatch(사전학습-추론 차이) 존재. 길이 확장 어려움.\n- GPT(Decoder, causal)\n  - 장점: 자연스러운 생성. 단일 포맷으로 다양한 태스크를 텍스트-생성 문제로 통일.\n  - 단점: 2018 기준 이해형 벤치마크에서 BERT 대비 약함. 분류 등에선 학습 샘플/포맷 설계의 민감도.\n\n9) 실무 적용 가이드\n- 태스크 성격으로 선택\n  - 생성/요약/대화/작성: GPT 계열 선호.\n  - 분류/탐지/엔티티추출/문장쌍 유사도: BERT 계열 선호.\n- 파인튜닝 팁\n  - BERT: 학습률 2e-5~5e-5 범위, 워밍업(10% 내외), weight decay(0.01), seq length 128→512 점진 확대.\n  - GPT: 프롬프트 설계 품질이 성능에 크게 영향. 분류도 텍스트-완성으로 포맷하면 라벨 누수 방지 필요.\n- 도메인 적응\n  - Continual pretraining(특정 도메인 코퍼스로 MLM/CLM 추가 사전학습) 후 파인튜닝이 효과적.\n- 길이/리소스\n  - 양쪽 모두 O(n^2) self-attention 비용. 긴 문서는 Longformer/FlashAttention/Sliding Window 등 고려.\n- 최신 대체\n  - BERT 계열: RoBERTa나 DeBERTa가 기본 선택지로 많이 사용.\n  - GPT 계열: GPT-2/Neo/OPT/LLaMA 등 공개 모델 활용. 지시 미세조정(SFT)·RLHF는 추가 비용.\n\n10) 간단 결론\n- 이해 중심 태스크에는 BERT 계열이 여전히 강력한 베이스라인.\n- 생성 중심 태스크에는 GPT 계열이 자연스럽고 범용적.\n- 2018 원 논문 시점의 비교에서는 BERT가 더 큰 데이터/양방향성/태스크 일치 덕에 NLU 벤치마크에서 앞섰음.\n\n11) 참고(원 논문)\n- BERT: arXiv:1810.04805 (Devlin et al., 2018)\n- GPT: Radford et al., 2018, Improving Language Understanding by Generative Pre-Training\n\n저장 방법 안내\n- 아래 파이썬 스크립트를 실행하면 분석 내용을 두 가지 형식으로 저장합니다.\n  - bert_vs_gpt_분석.md: 사람이 읽기 쉬운 요약\n  - bert_vs_gpt_분석.json: 구조화된 메타데이터 포함\n\n파이썬 스크립트\n```python\nimport json\nfrom datetime import datetime\nanalysis_md = \"\"\"[BERT vs GPT 논문 비교·분석 요약]\n- 아키텍처: BERT(Encoder, 양방향) vs GPT(Decoder, causal)\n- 사전학습: BERT(MLM+NSP) vs GPT(Causal LM)\n- 데이터: BERT(BooksCorpus+Wikipedia) vs GPT(BooksCorpus)\n- 입력: BERT([CLS]/[SEP]/segment) vs GPT(단일 시퀀스)\n- 장단점: BERT-이해형 강, GPT-생성형 강\n- 실무: 태스크 성격, 도메인 적응, 리소스/길이 고려\n(상세 내용은 본 문서 상단 분석을 참조)\n\"\"\"\nanalysis_json = {\n  \"title\": \"BERT와 GPT 원 논문 비교 분석\",\n  \"updated_at\": datetime.utcnow().isoformat() + \"Z\",\n  \"papers\": [\n    {\"name\": \"BERT\", \"arxiv\": \"1810.04805\", \"year\": 2018,\n     \"objective\": [\"MLM\", \"NSP\"], \"arch\": \"Transformer Encoder (bidirectional)\"},\n    {\"name\": \"GPT\", \"reference\": \"Radford et al., 2018\", \"year\": 2018,\n     \"objective\": [\"Causal LM\"], \"arch\": \"Transformer Decoder (causal)\"},\n  ],\n  \"key_differences\": {\n    \"attention\": \"BERT: bidirectional vs GPT: causal mask\",\n    \"pretraining\": \"BERT uses MLM(+NSP) vs GPT uses next-token prediction\",\n    \"inputs\": \"BERT uses [CLS]/[SEP]/segment embeddings vs GPT uses single stream\",\n    \"data\": \"BERT: BooksCorpus+Wikipedia vs GPT: BooksCorpus\"\n  },\n  \"practical_guide\": {\n    \"choose\": {\n      \"generation\": \"GPT-family\",\n      \"classification_qa\": \"BERT-family\"\n    },\n    \"finetune_hints\": {\n      \"bert\": \"lr 2e-5~5e-5, warmup, weight decay, seq length schedule\",\n      \"gpt\": \"prompt formatting, label leakage prevention\"\n    }\n  }\n}\n\nwith open(\"bert_vs_gpt_분석.md\", \"w\", encoding=\"utf-8\") as f:\n    f.write(analysis_md)\n\nwith open(\"bert_vs_gpt_분석.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(analysis_json, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved: bert_vs_gpt_분석.md, bert_vs_gpt_분석.json\")\n```\n\n원하시면 위 전체 분석 본문을 그대로 포함한 긴 Markdown 버전으로 저장해 드릴 수 있습니다. 파일명/형식을 지정해 주시면 그에 맞춰 출력 포맷을 바꿔 드리겠습니다.\n\n---\n\n### 고급자용 (30세 이상)\n\n요청하신 BERT와 GPT의 핵심 논문들을 비교·분석한 정리입니다. 마지막에 머신-리더블(JSON) 요약을 포함했으니 그대로 저장해 사용할 수 있습니다. 추가로 특정 형식(예: .md, .tex, .pdf)으로 파일을 생성해 드리길 원하시면 형식을 알려주세요.\n\n1) 문제 설정과 사전학습 목적 함수의 차이\n- BERT(Devlin et al., 2019): 양방향 Encoder 전용 Transformer. 마스크드 언어모델(MLM)과 Next Sentence Prediction(NSP) 목적.\n  - MLM 손실: L_MLM(θ) = E_{x∼D, M}[ Σ_{i∈M} −log p_θ(x_i | x_{¬i}) ]. 여기서 M은 마스크 위치 집합, x_{¬i}는 마스크된 토큰을 제외한 컨텍스트.\n  - NSP 손실: L_NSP(θ) = E_{(s1,s2)}[ −y log p_θ(IsNext | s1, s2) − (1−y) log(1−p_θ(IsNext | s1, s2)) ].\n  - 이론적 관점: MLM은 합성곱적(autoreg) 요인화를 직접 최적화하지 않고 pseudo-likelihood 최대화와 유사(= 데이터를 구성하는 각 좌표의 조건부분포 합)하며, 마스크 비율과 패턴에 따른 추정 편향이 존재. RoBERTa(Liu et al., 2019)는 NSP를 제거하고 더 큰 데이터·긴 학습으로 성능 향상 보고.\n\n- GPT(Radford et al., 2018; 2019; Brown et al., 2020): 단방향 Decoder 전용 Transformer. 자기회귀 언어모델(AR-LM) 목적.\n  - AR 손실: L_AR(θ) = E_{x∼D}[ Σ_{t=1}^T −log p_θ(x_t | x_{<t}) ].\n  - 이론적 관점: AR은 데이터의 실제 결합분포 log p(x) = Σ_t log p(x_t | x_{<t})를 직접 최대화. 예측과 사용 시나리오(생성)가 정합적. 스케일이 커질수록 In-context Learning, 제로/소샷 능력이 출현(Kaplan et al., 2020; Brown et al., 2020).\n\n간단한 정리: AR은 결합우도 직접최대화(정합성 높음), MLM은 pseudo-likelihood 근사(양방향 표현력 높고 판별 과제에 유리).\n\n2) 아키텍처적 비교\n- 공통: Transformer 기반(Vaswani et al., 2017), 다중 헤드 자기어텐션, 포지셔널 임베딩, GELU 활성화 등.\n- BERT: Encoder 블록만 사용, 양방향 self-attention, [MASK]/[CLS]/[SEP] 토큰, absolute position(원 논문). 변형: Whole-word/Span masking(SpanBERT), disentangled attention(DeBERTa), factorized/parameter sharing(ALBERT), dynamic masking(RoBERTa).\n- GPT: Decoder 블록(마스크드 self-attention + causal), layer norm 배치 위치 차이(프리/포스트), KV 캐시를 통한 효율적 오토리그레시브 추론. 대형화와 데이터 확장(GPT-2, GPT-3), 지시튜닝/정렬(RLHF; Ouyang et al., 2022), MoE(Switch Transformer, Fedus et al., 2021), 장문맥(FlashAttention, Dao et al., 2022; GPT-4/Claude 계열).\n\n3) 계산 복잡도와 자원\n- 한 레이어 당 시간복잡도(히든 크기 d, 시퀀스 길이 L):\n  - Self-attention: O(L^2 d)\n  - FFN: O(L d^2)\n  - 총합 O(L^2 d + L d^2) × 레이어수 H. 메모리: 어텐션맵 O(L^2), KV 캐시 O(H L d) (추론 시).\n- BERT 추론: 전토큰 양방향 주의 → 문장/문서 인코딩 시 병렬 처리 유리. 토큰 분류, 문장 분류에 고효율.\n- GPT 추론: 오토리그레시브 생성으로 토큰별 순차 디코딩 필요(지연 증가). 다만 KV 캐시와 speculative decoding 등으로 실무 지연 완화.\n- 학습 FLOPs(밀집 Transformer 근사): ≈ 6 N T (N=파라미터 수, T=학습 토큰수) (Hoffmann et al., 2022). 스케일법칙에 따라 성능은 파라미터·데이터·컴퓨트 균형에 민감(Kaplan et al., 2020; Hoffmann et al., 2022).\n\n4) 이론적 성질(간단한 정당화)\n- 정리(AR 일치성): 모델 용량이 충분하고 최적화가 전역최적에 도달하면 AR 학습은 데이터의 진짜 결합분포 p(x)를 회복한다. 증명: log p_θ(x)=Σ_t log p_θ(x_t|x_{<t})를 최대화하는 θ*가 p(x_t|x_{<t})=p*(x_t|x_{<t})를 만족하면 p_θ=p*.\n- 정리(MLM과 pseudo-likelihood): 충분히 다양한 마스킹 분포 q(M)로 모든 위치가 독립적으로 양의 확률로 마스킹되고, 모델이 조건분포를 정확히 표현 가능하면, L_MLM 최대화는 Besag(1975) 의미의 pseudo-likelihood 최대화이며, 특정 정규성 조건 하에서 일치 추정량이 된다. 단, 실제 BERT는 부분 마스킹(15%), [MASK] 토큰 노이즈, 마스크-미스매치 등으로 편향이 잔존(ELECTRA, Clark et al., 2020가 이를 완화).\n\n5) 학습 데이터와 스케일\n- BERT: BooksCorpus(800M 단어) + Wikipedia(2.5B 단어) 중심. 비교적 정제된 코퍼스.\n- GPT 계열:\n  - GPT-1: BooksCorpus 위주(약 1B 토큰 수준).\n  - GPT-2: WebText(수십 GB; 크롤링·필터링).\n  - GPT-3: 약 300B 토큰( Common Crawl, WebText2, Books, Wikipedia 혼합) (Brown et al., 2020).\n- 동향: 데이터 규모·다양성·필터링 품질이 성능에 핵심. 데이터 커리큘럼, 라이선스/윤리, 데이터 누수 검출이 중요 이슈.\n\n6) 벤치마크와 실험적 차이(요점 중심)\n- BERT:\n  - 발표 당시 GLUE, SQuAD 1.1/2.0, SWAG 등 판별형 벤치마크 SOTA. 파인튜닝 효율이 높음.\n  - 후속: RoBERTa는 더 긴 학습·큰 배치·NSP 제거·동적 마스킹으로 재학습하여 광범위한 개선. ELECTRA는 Replaced Token Detection으로 적은 compute에 높은 효율.\n- GPT:\n  - GPT-2: LAMBADA, Wikitext-103 등에서 제로·소샷 생성 능력 부각.\n  - GPT-3: 광범위한 과제에서 In-context Learning으로 소·제로샷 SOTA 다수 달성, 특히 추론·지시 따르기 능력이 스케일로 개선.\n  - 이후 RLHF(InstructGPT)로 안전성·유용성 대폭 향상.\n- 정리: 판별형 태스크(분류, NER, 랭킹)에서 BERT류 파인튜닝이 경량·강력. 생성·지시추종·지식호라이즌에서는 대규모 GPT류가 우위.\n\n7) 실제 사용상의 차이와 파인튜닝\n- BERT:\n  - 장점: 인코더 벡터화를 통한 빠른 서빙(배치 가능), 검색/재현/문서 인코딩, 클래시피케이션. 파라미터 효율형 튜닝(LoRA, prompt-tuning, adapters) 유용.\n  - 약점: 텍스트 생성·다양한 제로샷 능력은 제한. 다운스트림 별로 태스크 전용 헤드 필요.\n- GPT:\n  - 장점: 범용 텍스트 생성, 제로·소샷/ICL, 지시튜닝·RLHF로 범용 비서화. 도메인 적응은 컨텍스트·RAG로 실시간 가능.\n  - 약점: 생성 지연·비용, 컨텍스트 길이에 따른 O(L^2) 제약, 최신성·사실성 이슈(정렬 필요).\n\n8) 최신 연구 동향 대비\n- Encoder 계열: RoBERTa, ELECTRA, ALBERT, DeBERTa, SpanBERT, SimCSE(대조학습) 등으로 판별형 성능·효율 개선. 여전히 검색(DPR), 랭킹, 문서 임베딩에서 주력.\n- Decoder 계열: 지시튜닝(FLAN/FLAN-T5; Chung et al., 2022), 거대 모델(LLaMA, GPT-4), 장문맥(FlashAttention 2, Hyena/SSM, LongRoPE), MoE(Switch/Mixtral), 멀티모달(GPT-4V, LLaVA).\n- 혼합/대안: T5/UL2(denoising mixture로 encoder-decoder), Prefix/Span-Infilling(Filled-in-the-Middle; GELU), Retrieval-Augmented Generation(RAG), State Space Models(Mamba; Gu & Dao, 2024)로 장문맥·선형 복잡도 탐색.\n\n9) 미해결 문제와 향후 연구\n- 장문맥 확장: O(L^2) 어텐션의 근본적 한계. 근사·커널화(Performer, Linformer), 압축, 메모리 모듈, SSM, 하이브리드 구조 설계.\n- 목적 함수 정합성: MLM의 마스크-미스매치와 pseudo-likelihood 편향을 줄이는 대안(Span corruption, ELECTRA, denoising mixtures)의 이론적 보장 강화.\n- 데이터 거버넌스: 누수 방지, 라이선싱 준수, 데이터 밸런싱·디덕플리케이션, 장기적 지식 업데이트.\n- 정렬·안전성: RLHF/RLAIF의 신뢰성, 보상 해킹, 다단계 평가 자동화. 다국어·멀티모달 정렬.\n- 효율적 튜닝·서빙: PEFT(LoRA, IA3), 저정밀(8/4/2-bit), 지연 감소(speculative decoding, Medusa), 에너지 효율.\n- 해석가능성·견고성: In-context 작동 원리 이론화, 반사실성·역추적, 적대적 강건성.\n\n10) 선택 가이드\n- 판별형(분류/추출/검색/재현) + 제한된 컴퓨트/지연 민감: 고성능 BERT류(RoBERTa/DeBERTa) 파인튜닝이 합리적.\n- 생성/요약/에이전트/제로샷: 대규모 GPT류 + 지시튜닝/RLHF + RAG 권장.\n- 도메인 한정 생성: 중형 Decoder + RAG/PEFT로 비용-성능 균형.\n\n참고 수식 요약\n- AR-LM: maximize L_AR = E[Σ_t log p_θ(x_t|x_{<t})], perplexity = exp(−(1/T) Σ_t log p_θ(x_t|x_{<t})).\n- MLM(denoising): maximize L_MLM = E_{M}[Σ_{i∈M} log p_θ(x_i|x_{¬i})], pseudo-likelihood PL(θ)=Σ_i log p_θ(x_i|x_{¬i}); L_MLM ≈ E_M[PL(θ; M⊆{1..T})].\n\n핵심 비교 요약\n- 목적: BERT는 양방향 표현 학습(MLM/NSP), GPT는 생성 정합 AR.\n- 구조: Encoder-only vs Decoder-only.\n- 성능 경향: BERT류는 판별형 SOTA(효율적 파인튜닝), GPT류는 생성/지시/제로샷 SOTA(스케일 의존).\n- 복잡도: 둘 다 레이어당 O(L^2 d + L d^2); GPT는 생성 시 순차 디코딩으로 지연 부담, BERT는 인코딩 병렬화 유리.\n- 최신: Encoder는 RoBERTa/DeBERTa/ELECTRA로 성숙, Decoder는 지시튜닝·RLHF·RAG·장문맥·MoE로 급진적 진화.\n\n대표 인용(연도)\n- Vaswani et al., 2017. Attention Is All You Need.\n- Devlin et al., 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\n- Radford et al., 2018. Improving Language Understanding by Generative Pre-Training (GPT-1).\n- Radford et al., 2019. Language Models are Unsupervised Multitask Learners (GPT-2).\n- Brown et al., 2020. Language Models are Few-Shot Learners (GPT-3).\n- Liu et al., 2019. RoBERTa.\n- Lan et al., 2019. ALBERT.\n- Clark et al., 2020. ELECTRA.\n- He et al., 2021. DeBERTa.\n- Raffel et al., 2020. T5.\n- Kaplan et al., 2020. Scaling Laws for Neural Language Models.\n- Hoffmann et al., 2022. Training Compute-Optimal Large Language Models (Chinchilla).\n- Ouyang et al., 2022. InstructGPT (RLHF).\n- Dao et al., 2022. FlashAttention.\n- Fedus et al., 2021. Switch Transformers.\n- Gu & Dao, 2024. Mamba.\n\n머신-리더블 요약(JSON)\n{\n  \"title\": \"BERT_vs_GPT_analytic_comparison\",\n  \"version\": \"2025-11-05\",\n  \"core_difference\": {\n    \"objective\": {\n      \"BERT\": \"Masked LM (pseudo-likelihood approximation) + NSP\",\n      \"GPT\": \"Autoregressive LM (joint likelihood maximization)\"\n    },\n    \"architecture\": {\n      \"BERT\": \"Encoder-only, bidirectional self-attention\",\n      \"GPT\": \"Decoder-only, causal self-attention\"\n    },\n    \"use_cases\": {\n      \"BERT\": [\"classification\", \"NER\", \"retrieval/reranking\", \"embedding\"],\n      \"GPT\": [\"text generation\", \"instruction following\", \"zero-/few-shot\", \"agents\"]\n    }\n  },\n  \"losses\": {\n    \"MLM\": \"E_{x,M}[ sum_{i in M} -log p(x_i | x_{¬i}) ]\",\n    \"AR\": \"E_{x}[ sum_{t=1}^T -log p(x_t | x_{<t}) ]\"\n  },\n  \"complexity\": {\n    \"per_layer_time\": \"O(L^2 d + L d^2)\",\n    \"memory\": \"O(L^2) attention map; KV cache O(H L d)\",\n    \"training_flops_rule\": \"~6 N T (dense Transformer)\"\n  },\n  \"data_scale\": {\n    \"BERT\": \"BooksCorpus + Wikipedia (~3B words)\",\n    \"GPT\": \"from ~1B tokens (GPT-1) to ~300B (GPT-3)\"\n  },\n  \"benchmarks\": {\n    \"BERT\": [\"GLUE\", \"SQuAD 1.1/2.0\", \"SWAG\"],\n    \"GPT\": [\"LAMBADA\", \"Wikitext-103\", \"zero-/few-shot tasks across many benchmarks\"]\n  },\n  \"trends\": [\"RoBERTa/ELECTRA/DeBERTa improvements\", \"Instruction tuning, RLHF\", \"RAG\", \"Long-context attention/SSM\", \"MoE\"],\n  \"open_problems\": [\"long-context efficiency\", \"objective mismatch for MLM\", \"data governance\", \"alignment/safety\", \"efficient PEFT/serving\", \"interpretability\"],\n  \"citations\": [\n    \"Devlin et al., 2019\",\n    \"Radford et al., 2018; 2019\",\n    \"Brown et al., 2020\",\n    \"Liu et al., 2019\",\n    \"Lan et al., 2019\",\n    \"Clark et al., 2020\",\n    \"He et al., 2021\",\n    \"Raffel et al., 2020\",\n    \"Kaplan et al., 2020\",\n    \"Hoffmann et al., 2022\",\n    \"Ouyang et al., 2022\",\n    \"Dao et al., 2022\",\n    \"Fedus et al., 2021\",\n    \"Gu & Dao, 2024\"\n  ]\n}\n\n저장 관련 안내\n- 위 JSON 블록을 파일로 저장하려면 파일명 예: BERT_vs_GPT_analytic_comparison_2025-11-05.json 로 저장하세요.\n- 인간 가독성 요약을 .md나 .tex로 원하시면 형식을 지정해 주세요. 원하시는 파일명과 포맷을 주시면 해당 형식으로 출력해 드리겠습니다.\n\n---",
    "tool_choice": "general",
    "sources": null
  },
  {
    "role": "user",
    "content": "최신 AI 기술 찾아서 정리하고 분석해서 저장해줘"
  },
  {
    "role": "assistant",
    "content": "### 중급자용 (23-30세)\n\n요청 요약\n- 제한: 제 지식은 2024-10 기준의 최신 동향까지 반영됩니다. 그 이후 소식은 직접 웹을 조회할 수 없으니, 주기적 업데이트나 특정 도메인(예: 멀티모달·코드·음성·RAG·추론향상·경량화·배포) 우선순위를 알려주시면 맞춤 정리본을 유지 보수해 드릴 수 있습니다.\n- 제공물: 실무 적용을 위한 기술 지도, 알고리즘 해설, 장단점, 코드 스니펫(PyTorch/Transformers), 저장용 Markdown/JSON, 저장 스크립트.\n\n실무 중심 요약(2024년 최신 트렌드 스냅샷)\n- LLM 추론강화: o1 스타일의 “테스트-타임 컴퓨트”(self-verify, deliberate sampling, verifier-guided decoding), ToT/GoT/SC(자기일관성) 재부상. 장점: 논리적·계산적 문제에서 급격한 향상. 단점: 지연시간/비용 상승, 안정적 검증기 필요.\n- 효율 추론: FlashAttention 2/3, vLLM(PagedAttention), speculative decoding(EAGLE/Medusa), MoE(예: Mixtral), KV-cache 최적화/압축, TensorRT-LLM. 장점: 토큰당 비용 대폭 절감. 단점: 엔지니어링 난이도↑, 하드웨어별 튜닝 필요.\n- 롱컨텍스트: RoPE 스케일링(NTK/YaRN), Sparse/Sliding-window, RetNet/State-Space(Mamba-2), KV de/rehash. 장점: 100K+ 컨텍스트 실용화. 단점: 품질 유지 위한 데이터/학습 레시피 중요.\n- RAG 2.0: 멀티벡터(ColBERTv2), 하이브리드 BM25+dense, reranker(BGE), Graph RAG, agentic RAG(계획/툴사용). 장점: 최신성·사실성 확보. 단점: 파이프라인 복잡도, 관찰가능성 필요.\n- 멀티모달: GPT-4o 계열(실시간 음성/영상 이해), LLaVA/Idefics2/Qwen2-VL, Stable Video Diffusion/Sora류(텍스트→동영상), 대화형 음성(TTS/ASR) 통합. 장점: 제품경험 혁신. 단점: 지연·비용·안전성 이슈.\n- 파인튜닝: QLoRA/DoRA/LoRA+/ReLoRA, DPO/IPO/KTO/ORPO(선호학습), 코드/툴사용·함수호출·구조화 출력 튜닝. 장점: 소형모델로 도메인 성능↑. 단점: 데이터 정제와 안전튜닝 중요.\n- 양자화/경량화: AWQ/GPTQ/RPTQ/SmoothQuant, KV-양자화, gguf/llama.cpp/TensorRT-LLM/ONNX Runtime GenAI. 장점: 엣지·온프레미스 배포. 단점: 정밀도-품질 trade-off.\n- 에이전트/툴사용: ReAct/Reflexion/ToT, LangGraph, 함수호출/JSON schema/검증기, 샌드박스 실행. 장점: 복합업무 자동화. 단점: 신뢰성·관찰성·비용 관리 과제.\n- 평가/안전: SWE-bench, GPQA, MATH/AGIEval, LLM-as-a-judge/베리파이어, 프롬프트·컨텐츠 안전가드, 레드팀·모니터링. 장점: 회귀방지/품질보증. 단점: 벤치마크 과적합 리스크.\n\n핵심 기술 상세와 알고리즘(코드 포함)\n\n1) 추론강화(Reasoning-time compute, Verifier-guided decoding)\n- 개념: 생성 단계에서 추가 샘플링/검증을 통해 최종 답 변별력 향상. 예: Self-consistency(다중 샘플→투표), Verifier 모델로 후보 스코어링.\n- 간단 의사코드\n```\ndef reason_with_verifier(model, verifier, prompt, n=8, beta=0.7):\n    candidates = [model.generate(prompt, temperature=1.0) for _ in range(n)]\n    scores = [verifier.score(prompt, c) for c in candidates]  # logit/prob\n    i = np.argmax(scores)\n    # 가중 보정(샘플링 확률 * 검증점수)도 가능\n    return candidates[i], scores[i]\n```\n- 장단점\n  - 장점: 수학/코딩/논리에서 큰 폭의 정확도 향상. 모델 파라미터 변경 없이 효과.\n  - 단점: 지연시간 선형 증가, 검증기 학습 필요, 실패모드 모니터링 필수.\n\n2) 효율 추론 스택(vLLM, FlashAttention, Speculative decoding, MoE)\n- FlashAttention 2/3: attention 계산을 타일링해 HBM 왕복 최소화. 시간·메모리 O(BHL^2) 상수항 대폭 축소.\n- vLLM(PagedAttention): KV cache를 가상 메모리처럼 페이징, 프롬프트 병렬화 극대화로 토큰/초 향상.\n- Speculative decoding: 빠른 draft 모델이 여러 토큰 제안→느린 target 모델이 여러 토큰 한번에 검증/수용.\n  - 기대 이론치: 수용율 p일 때 속도 향상 ~ 1/(1 - p·k) 근사(k는 제안 토큰 길이).\n- MoE: Top-k expert 활성화로 FLOPs/파라미터 분리. 추론 시 활성 expert만 계산.\n- PyTorch: 라우팅형 MoE 레이어 예시\n```\nimport torch, torch.nn as nn, torch.nn.functional as F\n\nclass Top2Router(nn.Module):\n    def __init__(self, d_model, n_expert):\n        super().__init__()\n        self.w = nn.Linear(d_model, n_expert)\n\n    def forward(self, x):\n        logits = self.w(x)                  # [B, L, E]\n        top2 = logits.topk(2, dim=-1)\n        idx = top2.indices                  # expert indices\n        gate = F.softmax(top2.values, -1)   # gates\n        return idx, gate\n\nclass ExpertFFN(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.w1, self.w2 = nn.Linear(d_model, d_ff), nn.Linear(d_ff, d_model)\n\n    def forward(self, x): return self.w2(F.gelu(self.w1(x)))\n\nclass MoE(nn.Module):\n    def __init__(self, d_model, d_ff, n_expert):\n        super().__init__()\n        self.router = Top2Router(d_model, n_expert)\n        self.experts = nn.ModuleList([ExpertFFN(d_model, d_ff) for _ in range(n_expert)])\n\n    def forward(self, x):                   # x: [B, L, D]\n        idx, gate = self.router(x)          # [B, L, 2], [B, L, 2]\n        y = torch.zeros_like(x)\n        for k in range(2):\n            expert_ids = idx[..., k]        # [B, L]\n            g = gate[..., k].unsqueeze(-1)  # [B, L, 1]\n            # gather tokens per expert (naive; 실제론 토큰-버킷팅으로 효율화)\n            for e in range(len(self.experts)):\n                mask = (expert_ids == e).unsqueeze(-1)\n                if mask.any():\n                    y = y + self.experts[e](x * mask.float()) * g\n        return y\n```\n- Speculative decoding(HF Transformers) 예시\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntok = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\ndraft = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.float16).cuda()\ntarget = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-70B-Instruct\", torch_dtype=torch.float16, device_map=\"auto\")\n\nprompt = \"Solve: 24*17-19=\"\ninputs = tok(prompt, return_tensors=\"pt\").to(\"cuda\")\n# transformers>=4.41 기준: assistant(=draft) 모델 지정\nout = target.generate(**inputs, max_new_tokens=64, do_sample=True, assistant_model=draft)\nprint(tok.decode(out[0], skip_special_tokens=True))\n```\n- vLLM로 서빙\n```\n# 설치: pip install vllm\nfrom vllm import LLM, SamplingParams\nllm = LLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", tensor_parallel_size=2)\nparams = SamplingParams(temperature=0.7, max_tokens=128)\noutputs = llm.generate([\"Explain KV cache in LLMs\"], params)\nprint(outputs[0].outputs[0].text)\n```\n- 실무 고려: 프롬프트 병렬화, paged KV, 텐서/파이프라인 병렬, INT4/FP8 혼합, 배치 어그리게이션, 샘플링 파라미터 표준화.\n\n3) 롱컨텍스트(Attention/State-Space/Mixed)\n- Attention O(L^2) 비용. 최적화:\n  - RoPE 스케일링(NTK/YaRN): 로터리 주파수 재매핑으로 장문 일반화 향상.\n  - Sliding-window/Block-sparse: 최근 토큰에 집중해 계산 절감(Longformer/BigBird 계열).\n  - KV-cache 재활용/압축: 반복 패턴 중복 제거, 8bit/4bit KV, 그룹화.\n  - State-space/RetNet/Mamba-2: O(L) 순차 스캔, 롱-레인지 상호작용은 파라미터화된 필터로 표현.\n- RoPE 스케일링(Transformers 예시)\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nm = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", rope_scaling={\"type\":\"yarn\",\"factor\":4.0})\n```\n- 트레이드오프: 롱컨텍스트 튜닝 시 훈련 데이터의 장문 비율·분포가 품질 좌우. 단순 컨텍스트 확장만으로 사실성/추론력이 보장되지 않음.\n\n4) RAG 2.0(멀티벡터/Graph/에이전트형)\n- 파이프라인: Query transform(MQ/MR), retrieve(BM25+dense), rerank(cross-encoder), synthesize(CoT), cite.\n- 멀티벡터(ColBERTv2): 토큰 단위 임베딩으로 부분정합 강화.\n- Graph RAG: 엔티티/관계 그래프 위의 서브그래프 탐색→근거 집약.\n- Agentic RAG: 계획-실행-검증 루프(ReAct), 툴사용(Web/API/코드 실행).\n- Python 예시(FAISS + BGE reranker)\n```\n# pip install faiss-cpu sentence-transformers rank-bm25\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom rank_bm25 import BM25Okapi\nimport faiss, numpy as np\n\ndocs = [...]  # list of str\nbm25 = BM25Okapi([d.split() for d in docs])\ndense = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\nindex = faiss.IndexFlatIP(dense.get_sentence_embedding_dimension())\nembs = dense.encode(docs, normalize_embeddings=True)\nindex.add(embs)\nreranker = CrossEncoder(\"BAAI/bge-reranker-base\")\n\ndef retrieve(q, k=20):\n    bm = bm25.get_top_n(q.split(), docs, n=k)\n    qv = dense.encode([q], normalize_embeddings=True)\n    D, I = index.search(qv, k)\n    dv = [docs[i] for i in I[0]]\n    cand = list({*bm, *dv})\n    pairs = [(q, d) for d in cand]\n    scores = reranker.predict(pairs)\n    reranked = [d for _, d in sorted(zip(scores, cand), reverse=True)]\n    return reranked[:10]\n```\n- 실무 고려: 소스 신뢰도/버전 관리, 인용(grounding), 캐시 전략, 프롬프트 가드, 관찰가능성(질의→문서→답 경로).\n\n5) 멀티모달(텍스트+비전/오디오/비디오)\n- VLM: LLaVA-1.6, Idefics2, Qwen2-VL 등. 이미지→토큰 매핑(비전 인코더 CLIP/ViT) + LLM 디코더, 투-타워/프로젝터 설계.\n- 실시간 대화형 모델: GPT-4o 계열(음성·시각 스트리밍) 접근. 지연 줄이기 위해 어텐션 윈도우, 오디오 특화 토큰화, 서버-클라이언트 비동기 채널.\n- 비디오 생성: Latent video diffusion, rectified flow/flow matching, 다중스케일 시간 어텐션. OpenAI Sora(2024)로 품질 상한 인식 상승.\n- 예시(Transformers로 VLM 추론)\n```\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nfrom PIL import Image\nproc = AutoProcessor.from_pretrained(\"llava-hf/llava-1.6-mistral-7b-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"llava-hf/llava-1.6-mistral-7b-hf\").eval()\nimg = Image.open(\"chart.png\")\nprompt = \"이 차트의 주요 추세를 요약해줘.\"\ninputs = proc(text=prompt, images=img, return_tensors=\"pt\")\nout = model.generate(**inputs, max_new_tokens=128)\nprint(proc.batch_decode(out, skip_special_tokens=True)[0])\n```\n- 고려사항: 프라이버시/저작권, 스트리밍 전송량, 실시간 안전가드.\n\n6) 파인튜닝/선호학습(LoRA 계열, QLoRA, DPO/IPO/KTO/ORPO)\n- QLoRA: 4bit NF4로 베이스 가중치 고정, LoRA만 학습→VRAM 절감.\n- DoRA/LoRA+: 랭크 분해·스케일 재파라미터화로 성능 향상.\n- DPO: 정책과 참조모델 대비로 선호도 직접 최적화.\n- 코드 예시(QLoRA + TRL DPO)\n```\n# pip install transformers peft accelerate bitsandbytes trl datasets\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\nfrom trl import DPOTrainer, DPOConfig\n\nmodel_name = \"meta-llama/Meta-Llama-3-8B\"\ntok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True, device_map=\"auto\")\n\npeft_cfg = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"], lora_dropout=0.05)\nmodel = get_peft_model(model, peft_cfg)\n\ndataset = load_dataset(\"my/dpo-pairs\")  # fields: prompt, chosen, rejected\ntrainer = DPOTrainer(\n    model=model, ref_model=None, args=DPOConfig(output_dir=\"dpo-out\", per_device_train_batch_size=2),\n    beta=0.1, train_dataset=dataset[\"train\"], tokenizer=tok\n)\ntrainer.train()\n```\n- 주의: 레퍼런스(참조) 모델 고정, KL 제어, 분포 drift 감시, 안전/해루프롬프트 포함한 튜닝.\n\n7) 양자화/배포(gguf, AWQ/GPTQ, TensorRT-LLM, ONNX Runtime GenAI)\n- 정적 양자화(AWQ/GPTQ/RPTQ): 가중치 단정도↓, 에러 최소화로 중요채널 보호.\n- 활성/KV 양자화(SmoothQuant, KV-Int4/8): 메모리 절감.\n- llama.cpp/ggml/gguf: CPU/모바일 배포. TensorRT-LLM: NVIDIA GPU 최적화, FP8/INT8, fused kernels.\n- 비교\n  - 장점: 하드웨어 범위 확대·비용 절감.\n  - 단점: 지표 손실·추론 안정성 이슈, 레이어별 스케일링·outlier 대응 필요.\n\n8) 에이전트·툴사용(ReAct/Reflexion/ToT, LangGraph)\n- ReAct: 추론(Thought)과 행동(Action) 인터리브, 도구 호출을 체계화.\n- Reflexion: 실패 시 자기피드백 메모리 업데이트.\n- LangGraph: 상태머신 기반 에이전트 워크플로우 선언적 정의.\n- 간단 ReAct 루프\n```\ntools = {\"search\": google_api, \"code\": exec_sandbox}\n\ndef react(agent, question):\n    ctx = []\n    for _ in range(8):\n        prompt = build_prompt(question, ctx)\n        step = agent.generate(prompt)\n        if \"Action:\" in step:\n            name, arg = parse_action(step)\n            obs = tools[name](arg)\n            ctx += [step, f\"Observation: {obs}\"]\n        if \"Final Answer:\" in step:\n            return step.split(\"Final Answer:\")[-1].strip()\n    return \"I don't know\"\n```\n- 운영: 샌드박스/리소스할당, 타임아웃/회로차단기, 추적(Span/Token/Tool logs), 안전가드.\n\n9) 평가/옵스\n- 벤치: HumanEval/SWE-bench(코드), GSM8K/MATH/GPQA(수학/과학), MMLU/IFEval, LVLM(ScienceQA), LLM-as-a-judge+Verificaton.\n- 옵스: Canary prompt, 샘플링 지표(토큰/초, 수용율, 장문 품질), 회귀 테스트, 데이터 drift·프롬프트 drift 관측.\n\n기술 간 비교 요점\n- MoE vs Dense: 같은 FLOPs에서 더 큰 파라미터 용량 확보 vs 라우팅/로드밸런싱 복잡성.\n- Speculative vs KV 최적화: 전자는 지연 단축에 즉효(모델 2개 필요), 후자는 서버 이용률↑과 메모리절감.\n- RAG vs 파인튜닝: 최신성·사실성은 RAG, 스타일·사소한 변환은 FT가 효율. 하이브리드가 실무 최적.\n- 롱컨텍스트 vs 검색: 초장문 질의는 롱컨텍스트, 광범위 코퍼스는 검색(RAG) 우위. 비용·지연 기준으로 스위칭.\n\n실무 적용 체크리스트\n- 요구사항: 지연/비용/정확도 목표 수치화(SLO).\n- 모델선정: 공개/상용, 라이선스, 언어/도메인.\n- 파이프라인: 프롬프트·RAG·툴사용·검증기·가드 체인.\n- 성능: vLLM+Paged KV, FlashAttn, SpecDec, quant.\n- 품질: Reranker·Verifier·Self-consistency, 회귀벤치.\n- 안전/규정: PII 마스킹, 우회 방지, 로깅/감사.\n- 운영: Canary, A/B, 샤딩·오토스케일, 비용 모니터.\n\n저장 아티팩트 제공(JSON/Markdown)\n- 아래 문자열을 파일로 저장하면 바로 문서·데이터베이스로 활용 가능합니다.\n\n파일 1: latest_ai_tech_2024Q4.md\n```\n# 2024-Q4 AI 기술 지도\n[요약/장단점/코드/실무 고려사항 — 본 답변 상단 섹션 그대로 포함]\n```\n\n파일 2: latest_ai_tech_2024Q4.json\n```\n{\n  \"version\": \"2024Q4\",\n  \"categories\": [\n    {\n      \"category\": \"reasoning\",\n      \"items\": [\n        {\n          \"name\": \"Verifier-guided decoding\",\n          \"what\": \"생성 단계에서 검증기로 후보 응답 점수화하여 최종 선택\",\n          \"algorithms\": [\"self-consistency\", \"verifier scoring\", \"majority vote\"],\n          \"pros\": [\"정확도 향상\", \"모델 교체 없이 적용\"],\n          \"cons\": [\"지연 증가\", \"검증기 학습 필요\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"HF Transformers\", \"LangGraph\"],\n          \"refs\": [\"arXiv:2305.10601\", \"arXiv:2203.11171\"]\n        }\n      ]\n    },\n    {\n      \"category\": \"inference_efficiency\",\n      \"items\": [\n        {\n          \"name\": \"vLLM PagedAttention\",\n          \"what\": \"KV cache 페이징으로 프롬프트 병렬화 극대화\",\n          \"algorithms\": [\"paged KV\", \"continuous batching\"],\n          \"pros\": [\"토큰/초↑\", \"메모리 파편화↓\"],\n          \"cons\": [\"엔진 의존\", \"디버깅 러닝커브\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"vLLM\", \"TGI\"]\n        },\n        {\n          \"name\": \"Speculative decoding\",\n          \"what\": \"draft/target 이중모델로 멀티토큰 수용\",\n          \"algorithms\": [\"Medusa\", \"EAGLE\"],\n          \"pros\": [\"지연↓\"],\n          \"cons\": [\"모델 2개 필요\", \"수용율 튜닝\"],\n          \"maturity\": \"beta\",\n          \"tooling\": [\"HF generate(assistant_model)\", \"vLLM\"]\n        },\n        {\n          \"name\": \"MoE routing\",\n          \"what\": \"Top-k expert 활성화로 파라미터 용량 대비 FLOPs 효율\",\n          \"algorithms\": [\"top-2 gating\", \"load balancing loss\"],\n          \"pros\": [\"스케일 효율\"],\n          \"cons\": [\"라우팅 불안정\", \"통신비용\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"Megablocks\", \"DeepSpeed-MoE\"]\n        }\n      ]\n    },\n    {\n      \"category\": \"long_context\",\n      \"items\": [\n        {\n          \"name\": \"RoPE scaling (NTK/YaRN)\",\n          \"what\": \"로터리 주파수 재매핑으로 장문 일반화\",\n          \"pros\": [\"간단/효과적\"],\n          \"cons\": [\"튜닝 민감\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"Transformers rope_scaling\"]\n        },\n        {\n          \"name\": \"Mamba-2\",\n          \"what\": \"state-space 기반 O(L) 시퀀스 모델\",\n          \"pros\": [\"롱컨텍스트 비용↓\"],\n          \"cons\": [\"생성 품질 편차\"],\n          \"maturity\": \"research-to-prod\",\n          \"tooling\": [\"mamba-ssm\"]\n        }\n      ]\n    },\n    {\n      \"category\": \"rag\",\n      \"items\": [\n        {\n          \"name\": \"Hybrid+Rerank RAG\",\n          \"what\": \"BM25+dense 멀티스테이지 검색과 cross-encoder reranker\",\n          \"pros\": [\"정확/근거 강화\"],\n          \"cons\": [\"지연↑\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"FAISS\", \"sentence-transformers\", \"BGE reranker\"]\n        },\n        {\n          \"name\": \"Graph RAG\",\n          \"what\": \"지식그래프 기반 서브그래프 증거 집약\",\n          \"pros\": [\"복잡 쿼리 강함\"],\n          \"cons\": [\"그래프 구축비용\"],\n          \"maturity\": \"beta\",\n          \"tooling\": [\"Neo4j\", \"NetworkX\"]\n        }\n      ]\n    },\n    {\n      \"category\": \"multimodal\",\n      \"items\": [\n        {\n          \"name\": \"VLM (LLaVA/Idefics2/Qwen2-VL)\",\n          \"what\": \"비전 인코더 + LLM 디코더로 이미지 이해/대화\",\n          \"pros\": [\"다양한 태스크\"],\n          \"cons\": [\"리소스/지연\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"Transformers\"]\n        },\n        {\n          \"name\": \"Video diffusion (Sora class)\",\n          \"what\": \"잠재공간 비디오 확산/플로우 매칭\",\n          \"pros\": [\"고품질 생성\"],\n          \"cons\": [\"대규모 연산/데이터\"],\n          \"maturity\": \"research-to-prod\",\n          \"tooling\": [\"Diffusers\"]\n        }\n      ]\n    },\n    {\n      \"category\": \"finetune_pref\",\n      \"items\": [\n        {\n          \"name\": \"QLoRA + DPO\",\n          \"what\": \"4bit 베이스 + LoRA 어댑터, 선호학습으로 응답 품질 향상\",\n          \"pros\": [\"VRAM 절감\", \"도메인 적합\"],\n          \"cons\": [\"데이터 품질 민감\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"PEFT\", \"TRL\"]\n        }\n      ]\n    },\n    {\n      \"category\": \"quantization_deploy\",\n      \"items\": [\n        {\n          \"name\": \"AWQ/GPTQ\",\n          \"what\": \"가중치 양자화로 추론 메모리/지연 감소\",\n          \"pros\": [\"엣지 배포\"],\n          \"cons\": [\"정확도 손실\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"auto-gptq\", \"awq\"]\n        },\n        {\n          \"name\": \"TensorRT-LLM\",\n          \"what\": \"NVIDIA GPU 최적화 커널/그래프\",\n          \"pros\": [\"최고 성능\"],\n          \"cons\": [\"GPU 종속\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"tensorrt-llm\"]\n        }\n      ]\n    },\n    {\n      \"category\": \"agents_tools\",\n      \"items\": [\n        {\n          \"name\": \"ReAct + LangGraph\",\n          \"what\": \"상태머신 기반 도구연계 워크플로우\",\n          \"pros\": [\"복합업무 자동화\"],\n          \"cons\": [\"관찰성/안전설계 필요\"],\n          \"maturity\": \"production-ready\",\n          \"tooling\": [\"LangGraph\", \"LangChain\"]\n        }\n      ]\n    }\n  ]\n}\n```\n\n파일 저장 스크립트(save_latest_ai.py)\n```\ncontent_md = \"\"\"[여기에 latest_ai_tech_2024Q4.md 문자열 붙여넣기]\"\"\"\ncontent_json = \"\"\"[여기에 latest_ai_tech_2024Q4.json 문자열 붙여넣기]\"\"\"\n\nimport json, os\nos.makedirs(\"latest_ai_tech_2024Q4\", exist_ok=True)\nwith open(\"latest_ai_tech_2024Q4/README.md\", \"w\", encoding=\"utf-8\") as f: f.write(content_md)\nwith open(\"latest_ai_tech_2024Q4/index.json\", \"w\", encoding=\"utf-8\") as f: f.write(content_json)\nprint(\"Saved to latest_ai_tech_2024Q4/\")\n```\n\n다음 단계 제안\n- 범위 확정: 특정 도메인(예: 법률/헬스/금융/코딩/제조)과 배포타깃(온프레미스 vs 클라우드, 지연/비용 SLO)을 알려주시면 맞춤형 로드맵과 벤치 구성으로 재정리합니다.\n- 업데이트 주기: 월간/분기별 자동 업데이트(원하시면 Git 리포 구조와 CI 스크립트까지 드립니다).\n- 데모: 귀사의 데이터로 RAG 2.0 파이프라인과 vLLM 서빙 PoC 코드를 바로 제공 가능합니다.\n\n원하시면 위 아카이브를 바로 붙여넣을 수 있게 완성된 Markdown/JSON 본문을 전달해 드리겠습니다.\n\n---\n\n### 고급자용 (30세 이상)\n\n아래는 2024년 10월까지의 최신 AI 기술을 학술적으로 정리·분석한 요약본입니다. 지식 최신성 한계로 2025년 발표·성능 수치는 포함하지 않습니다. 마지막에 저장용 요약(JSON)도 제공합니다. 저장 형식/위치(예: 파일, 노트 앱, 벤치마크 트래커)가 있으면 알려주시면 변환해 드리겠습니다.\n\n1) 대규모 언어모델(LLM): 아키텍처, 훈련, 추론 가속\n- 핵심 동향\n  - 스케일링 법칙 재정립: 데이터 우선(Chinchilla; Hoffmann et al., 2022), 장문맥·멀티모달로의 확장, 테스트타임 컴퓨트 증대(o1; OpenAI, 2024).\n  - 희소화/전문화: Mixture-of-Experts(MoE)로 파라미터 대형화·계산량 제한(Fedus et al., 2021; Mistral Mixtral, 2024).\n  - 효율화: FlashAttention(DAO et al., 2022, 2023), Speculative decoding(Leviathan et al., 2023), 양자화 GPTQ/ AWQ/ SmoothQuant(Frantar et al., 2022; Lin et al., 2023; Xiao et al., 2023), vLLM의 PagedAttention(Kwon et al., 2023).\n  - 장문맥: RoPE 변형(YaRN; Peng et al., 2023, NTK-aware), StreamingLLM(Xiao et al., 2023), Infini-attention(Munkhdalai et al., 2024).\n- 수식·복잡도\n  - 어텐션: Att(Q,K,V)=softmax(QK^T/√d)V. 시간복잡도 O(n^2 d), 메모리 O(n^2). FlashAttention는 타일링으로 정확도 손실 없이 메모리 O(n d)로 감소(시간은 여전히 O(n^2 d)이나 IO 최적화로 대폭 가속; Dao et al., 2022).\n  - MoE(top-k 게이팅): 토큰별 k개 전문가만 활성. 한 스텝 연산량 O(k·d^2) (dense 대비 k/E 비율로 감소, E는 전문가 수), 통신 오버헤드가 병목(샤딩·토큰 라우팅 필요).\n  - Speculative decoding: 작은 초안 모델 p_draft로 길이 τ 초안 생성, 큰 모델 p_large가 검증. 토큰당 기대 비용 ~ C_draft/τ + p_acc·C_large. 속도향상 S ≈ C_large/(p_acc·C_large + C_draft/τ). p_acc는 KL(p_large||p_draft)가 작을수록 증가(Leviathan et al., 2023; Medusa: Cai et al., 2023).\n  - 장문맥 포지셔닝: RoPE에서 위치 t에 대해 복소회전 R_θ(t); 스케일링/보간으로 유효 맥시멈 길이 확대(Peng et al., 2023).\n- 선호최적화(RLHF 계열)\n  - PPO-KL 기반 RLHF(Christiano et al., 2017): J(θ)=E[A^{π}(s,a)·r_θ(a|s)] − β·D_KL(π_θ||π_ref).\n  - DPO(Rafailov et al., 2023): 쌍별 선호(y^+≻y^−) 가정하에 브래들리–테리 모델로 유도된 폐형식 목적:\n    L_DPO(θ)=E[log σ(β(Δ log π_θ − Δ log π_ref))], Δ log π = log π(y^+|x) − log π(y^−|x).\n    스케치 증명: P(y^+≻y^−|x)=σ(β(R(y^+)-R(y^-))). 최대우도에서 R(y)=log π_θ(y|x)−log π_ref(y|x)+const로 귀결되어 위 로지스틱 회귀 형태가 됨. KL 제약 하의 Lagrangian과 등가.\n  - 변형: IPO(Azar et al., 2023), ORPO(Hong et al., 2023), KTO/EIPO 등(라벨 효율·안정성 개선).\n- 대표 모델·결과(일부)\n  - LLaMA 2/3(Touvron et al., 2023; Meta, 2024), Qwen2/2.5(Bai et al., 2024), Mixtral(Mistral, 2024), o1(OpenAI, 2024). 벤치마크: MMLU(Hendrycks et al., 2020), GSM8K(Cobbe et al., 2021), HumanEval(Chen et al., 2021). 2024년 중반 기준 70B급 오픈모델이 MMLU 80%± 성능 보고, 코드·수학은 사전학습/미세조정 레시피에 민감.\n\n2) 시퀀스 모델 대안: 상태공간모델(SSM)·RNN 르네상스\n- S4(Gu et al., 2022): 연속-이산 선형 SSM의 컨볼루션 커널 k를 FFT로 계산해 O(n log n) 시퀀스 처리.\n- Mamba(Gu & Dao, 2023): 데이터-의존 선택 스캔(selective scan)으로 선형 시간·상수 메모리. 길이 n, 채널 d에서 O(n d^2) 또는 적절한 저랭크화로 O(n d r).\n- 비교: Transformer는 장거리 상호작용에 유리, SSM은 매우 긴 n에서 우수한 스루풋·메모리. 하이브리드(Attention+SSM) 구조가 활발.\n- 벤치마크: Long Range Arena, LongBench(Bai et al., 2023) 등에서 SSM 변형이 긴 입력에서 경쟁력 확인.\n\n3) 멀티모달 모델: 비전·비디오·오디오\n- 비전-언어\n  - Flamingo(Alayrac et al., 2022), LLaVA(Liu et al., 2023), Idefics2(Laurençon et al., 2024), Qwen2-VL(Bai et al., 2024), InternVL2(Chen et al., 2024). SigLIP(Zhai et al., 2023)로 텍스트-이미지 정렬 개선.\n  - 벤치마크: MMMU(Yue et al., 2024), MathVista(Lu et al., 2024), TextVQA 등. 복합추론·차트이해에서 여전히 취약.\n- 세그멘테이션·기초 비전\n  - SAM(Kirillov et al., 2023), SAM 2(2024): 프롬프트 가능한 범용 세그멘터(비디오·스트리밍 확장).\n- 비디오 생성\n  - Diffusion-Transformer(DiT; Peebles & Xie, 2023), 3D/시공간 어텐션. Sora(OpenAI, 2024), Lumiere(Google, 2024) 등으로 고품질 장면 일관성. 평가: VBench·FVD.\n- 오디오/음성\n  - 음성 인식·번역: Whisper-v3(Radford et al., 2023), SeamlessM4T(Meta, 2023).\n  - 오디오 생성: AudioLM(Borsos et al., 2023), MusicGen(Copet et al., 2023).\n- 생성모델 기법\n  - DDPM(Ho et al., 2020), Latent Diffusion(Rombach et al., 2022), DiT(Peebles & Xie, 2023).\n  - Flow/ODE류: Rectified Flow(Liu et al., 2022), Flow Matching(Lipman et al., 2023).\n    · 정식화: 데이터 분포 p_1에서 노이즈 p_0로의 확률 경로 ρ_t, 연속 변환 φ_t가 ∂_t φ_t = v_t(φ_t)를 따른다고 할 때, v_t를 최소제곱 회귀로 근사: L = E_{t,x∼ρ_t} ||s_θ(x,t) − v_t(x)||^2. 최적 s_θ는 진정한 벡터장 v_t에 일치(증명 스케치: Fokker–Planck 연계 및 최소제곱의 일의성).\n  - Consistency Models(Song et al., 2023), CTM(Kim et al., 2023): 적은 스텝에서의 샘플링 가속.\n  - 복잡도: U-Net 대비 DiT는 어텐션 O(n^2) 병목이 있으나 TPU/GPU에서 더 나은 병렬성.\n\n4) RAG(검색확장생성)·지식접근\n- 검색기술\n  - Dense retriever: DPR(Karpukhin et al., 2020), Contriever(Izacard et al., 2021), E5(Wang et al., 2023), BGE(Xiao et al., 2023).\n  - 토크나이즈 후 행 단위 압축: SPLADE(Formal et al., 2021). 세밀 매칭: ColBERTv2(Santhanam et al., 2022).\n- 생성+검색 통합\n  - RAG(Lewis et al., 2020), REALM(Guu et al., 2020), RETRO(Borgeaud et al., 2022), Atlas(Izacard et al., 2022).\n  - 자가검증형: Self-RAG(Asai et al., 2023), Corrective-RAG/GraphRAG(Microsoft, 2024).\n- 평가: BEIR(Thakur et al., 2021), MTEB(Muennighoff et al., 2023), FreshQA/Temporal RAG. 여전히 최신성·충실성 트레이드오프 존재.\n- 복잡도: 인퍼런스에서 k개 문서 검색 시 O(k·L) 재랭킹 비용(크로스인코더), 토큰 비용은 입력 길이에 선형.\n\n5) 추론·도구사용 에이전트\n- 체인·트리 추론\n  - CoT(Wei et al., 2022), Self-Consistency(Wang et al., 2022), ReAct(Yao et al., 2023), Tree-of-Thought(Yao et al., 2023), Graph-of-Thought(Zhou et al., 2024).\n  - 과정감독 PRM(Uesato et al., 2022; Lightman et al., 2023). o1(OpenAI, 2024)은 테스트타임 컴퓨트 확대로 복잡문제 성능 향상 보고.\n- 코드·툴\n  - StarCoder2(Lozanov et al., 2024), DeepSeek-Coder(2024), Code Llama(Touvron et al., 2023). 툴포밍: Toolformer(Schick et al., 2023), Gorilla(Patil et al., 2023).\n- 벤치마크: SWE-bench(Jimenez et al., 2023), MBPP(Austin et al., 2021), HumanEval(Chen et al., 2021), AIDER계열. 함수호출 정확도, 실행안전성 병행 평가가 중요.\n\n6) 효율화: 양자화·서빙·메모리\n- 양자화\n  - GPTQ(Frantar et al., 2022), AWQ(Lin et al., 2023), SmoothQuant(Xiao et al., 2023), NF4 QLoRA(Dettmers et al., 2023). 4~8bit 가중치/액티베이션 실사용 단계.\n- 서빙\n  - vLLM의 PagedAttention(Kwon et al., 2023): KV 캐시를 페이징해 프래그멘테이션 최소화. 동시성·스루풋 향상.\n  - 텐서·커널: FlashAttention-2/3(DAO et al., 2023/2024), TensorRT-LLM(NVIDIA, 2023), DeepSpeed-Inference.\n- 컨텍스트 확장\n  - LongRoPE/YaRN/NTK-aware 스케일링(Peng et al., 2023), Sliding Window(Longformer; Beltagy et al., 2020), Linear/Performer(Katharopoulos et al., 2020; Choromanski et al., 2021), Infini-attention(Munkhdalai et al., 2024).\n- 복잡도 요약\n  - KV 캐시 메모리: O(n_layers·d·seq_len). 페이징/압축으로 상수 계수 절감.\n  - Speculative/멀티드래프트: 병렬화로 지연 감소, 수용률-드래프트 비용의 최적화 필요.\n\n7) 안전성·견고성·프라이버시\n- 정렬·선호\n  - Constitutional AI(Bai et al., 2022), RLAIF(Bai et al., 2022), DPO/IPO/ORPO 계열.\n- 프롬프트 공격·탈옥\n  - 평가: AdvBench(Zou et al., 2023), RealToxicityPrompts(Gehman et al., 2020). Robust prompting 방어, 콘텐츠 필터링, 정책 강건화 연구 활발.\n- 출처표시·워터마킹\n  - GMM watermark(Kirchenbauer et al., 2023), SynthID(Google, 2023). 강적 대항성·재샘플링 내구성은 개방문제.\n- 프라이버시·언러닝\n  - DP-SGD(Abadi et al., 2016) 대규모 모델 적용의 효율-성능 균형 난제. LLM unlearning은 이론적 보장과 실용적 성능 사이 간극 존재(미해결).\n\n8) 로보틱스·체화지능(Embodied AI)\n- 데이터·스케일\n  - Open-X-Embodiment/RT-X(2023), DROID(Chi et al., 2024): 로봇 데이터 집성·표준화.\n- 모델\n  - RT-1/RT-2(Brohan et al., 2022/2023): VLA(비전-언어-행동) 스케일링. Diffusion Policy(Chi et al., 2023), Octo(2023).\n- 평가: CALVIN, LIBERO, RealRobotBench. 시뮬-실세계 갭, 안전·실패 복구가 핵심 과제.\n\n실험·벤치마크 총평\n- 지식·종합: MMLU·MMMU가 포괄적 지표로 활용되나 데이터 누수·튜닝 민감도 문제. 더 강건한 오픈 평가 세트 요구.\n- 수학·코딩: GSM8K·MATH·HumanEval·SWE-bench 등에서 과정감독, 테스트타임 탐색 확대가 유의미한 향상.\n- 멀티모달: MathVista·MMMU에서 추론형 VLM의 격차가 두드러짐(차트, 과학도해).\n- RAG: BEIR·MTEB 상에서도 도메인·신선성에 따라 랭커 선택(E5/BGE+크로스리랭커)이 성능 좌우.\n\n미해결 문제·향후 연구\n- 장문맥의 진정한 활용: 단순 길이 확장 대비 선택적 주의/메모리(RMT, 외부 메모리)와 학습 안정성.\n- 추론의 신뢰성: PRM/Verifier의 확장과 증명가능한 오류 상한, 자기검증(Self-Consistency+Verifier)의 비용-성능 최적화.\n- 데이터 커링·합성: 고품질 장문맥·멀티스텝 데이터의 자동 생성과 평가 기준. 오표식 레이블 정정.\n- 효율성과 정확도의 동시달성: 4bit 양자화에서의 수치불안정, KV 캐시 압축·공유의 이론적 분석.\n- 안전성: 워터마킹의 강적 내구성, 탈옥 방어의 일반화 보장, 언러닝의 PAC-보장.\n- 에이전트: 도구 선택·순서 계획의 합리성 증명, 환경 상호작용에서의 오프폴리시 안정 학습.\n\n간단한 이론 스케치 두 가지\n- DPO의 BT(Bradley–Terry) 유도\n  가정: P(y^+≻y^-|x)=σ(β(R(y^+)−R(y^-))). 정합 제약 max_π E[log σ(β(R^π_Δ))] s.t. KL(π||π_ref)≤ε. 라그랑주 이중화로 R(y)=log π(y|x)−log π_ref(y|x)+const가 최적. 대입하면 L_DPO(θ)=E[log σ(β(Δ log π_θ − Δ log π_ref))].\n- FlashAttention의 정확성\n  어텐션 A=softmax(QK^T/√d)V의 원-패스 타일링 계산에서 각 타일의 정규화 상수 Z를 로그-서밍으로 누적하면 전역 softmax와 동치. 따라서 메모리 O(n d)로 정확 계산이 가능(DAO et al., 2022).\n\n참고문헌(발췌)\n- Hoffmann et al., 2022, Training Compute-Optimal LMs. Fedus et al., 2021, Switch Transformer. Touvron et al., 2023, LLaMA 2. Bai et al., 2024, Qwen2/2-VL. Mistral, 2024, Mixtral. OpenAI, 2024, o1. Dao et al., 2022/2023, FlashAttention. Leviathan et al., 2023, Speculative Decoding. Cai et al., 2023, Medusa. Kwon et al., 2023, vLLM/PagedAttention. Peng et al., 2023, YaRN. Munkhdalai et al., 2024, Infini-attention. Gu et al., 2022, S4. Gu & Dao, 2023, Mamba. Alayrac et al., 2022, Flamingo. Liu et al., 2023, LLaVA. Laurençon et al., 2024, Idefics2. Kirillov et al., 2023, SAM. Peebles & Xie, 2023, DiT. Ho et al., 2020, DDPM. Rombach et al., 2022, LDM. Liu et al., 2022, Rectified Flow. Lipman et al., 2023, Flow Matching. Song et al., 2023, Consistency Models. Kim et al., 2023, CTM. Lewis et al., 2020, RAG. Guu et al., 2020, REALM. Borgeaud et al., 2022, RETRO. Izacard et al., 2022, Atlas. Asai et al., 2023, Self-RAG. Santhanam et al., 2022, ColBERTv2. Wang et al., 2023, E5. Xiao et al., 2023, BGE/SmoothQuant. Frantar et al., 2022, GPTQ. Dettmers et al., 2023, QLoRA. Christiano et al., 2017, RLHF. Rafailov et al., 2023, DPO. Bai et al., 2022, Constitutional AI/RLAIF. Gehman et al., 2020, RealToxicityPrompts. Thakur et al., 2021, BEIR. Muennighoff et al., 2023, MTEB. Cobbe et al., 2021, GSM8K. Hendrycks et al., 2020/2021, MMLU/MATH. Chen et al., 2021, HumanEval. Jimenez et al., 2023, SWE-bench. Brohan et al., 2022/2023, RT-1/RT-2. Chi et al., 2023/2024, Diffusion Policy/DROID.\n\n저장용 컴팩트 요약(JSON)\n{\n  \"timestamp\": \"2024-10\",\n  \"areas\": [\n    {\n      \"area\": \"LLM\",\n      \"trends\": [\"MoE\", \"테스트타임 컴퓨트(o1)\", \"장문맥\", \"효율화(FlashAttention, SpecDec)\", \"선호최적화(DPO 계열)\"],\n      \"algorithms\": [\n        {\"name\": \"FlashAttention\", \"complexity\": {\"time\": \"O(n^2 d)\", \"memory\": \"O(n d)\"}, \"refs\": [\"Dao 2022\",\"Dao 2023\"]},\n        {\"name\": \"MoE(top-k)\", \"complexity\": {\"per-token\": \"O(k d^2)\"}, \"refs\": [\"Fedus 2021\",\"Mistral 2024\"]},\n        {\"name\": \"Speculative Decoding\", \"speedup\": \"S ≈ C_large/(p_acc·C_large + C_draft/τ)\", \"refs\": [\"Leviathan 2023\",\"Cai 2023\"]},\n        {\"name\": \"DPO\", \"objective\": \"E[log σ(β(Δ log π_θ − Δ log π_ref))]\", \"refs\": [\"Rafailov 2023\"]}\n      ],\n      \"benchmarks\": [\"MMLU\",\"GSM8K\",\"HumanEval\"]\n    },\n    {\n      \"area\": \"SSM/RNN\",\n      \"trends\": [\"S4/Mamba\", \"하이브리드 Attention+SSM\"],\n      \"algorithms\": [\n        {\"name\": \"S4\", \"complexity\": {\"time\": \"O(n log n)\"}, \"refs\": [\"Gu 2022\"]},\n        {\"name\": \"Mamba\", \"complexity\": {\"time\": \"O(n d^2)\"}, \"refs\": [\"Gu&Dao 2023\"]}\n      ],\n      \"benchmarks\": [\"Long Range Arena\",\"LongBench\"]\n    },\n    {\n      \"area\": \"Multimodal\",\n      \"trends\": [\"VLM 확대\", \"비디오 생성 품질 향상\", \"SAM2 비디오 세그멘트\"],\n      \"algorithms\": [\n        {\"name\": \"DiT\", \"note\": \"Transformer 기반 확산\", \"refs\": [\"Peebles&Xie 2023\"]},\n        {\"name\": \"Flow Matching/Rectified Flow\", \"refs\": [\"Lipman 2023\",\"Liu 2022\"]},\n        {\"name\": \"SAM/SAM2\", \"refs\": [\"Kirillov 2023\",\"Meta 2024\"]}\n      ],\n      \"benchmarks\": [\"MMMU\",\"MathVista\",\"FVD/VBench\"]\n    },\n    {\n      \"area\": \"RAG\",\n      \"trends\": [\"Self-RAG/GraphRAG\", \"조합형 retriever+reranker\"],\n      \"algorithms\": [\n        {\"name\": \"E5/BGE\", \"refs\": [\"Wang 2023\",\"Xiao 2023\"]},\n        {\"name\": \"ColBERTv2\", \"refs\": [\"Santhanam 2022\"]}\n      ],\n      \"benchmarks\": [\"BEIR\",\"MTEB\"]\n    },\n    {\n      \"area\": \"Agents/Reasoning\",\n      \"trends\": [\"CoT→ToT/GoT\", \"PRM/Verifier\"],\n      \"algorithms\": [\n        {\"name\": \"ReAct/ToT\", \"refs\": [\"Yao 2023\"]},\n        {\"name\": \"PRM\", \"refs\": [\"Uesato 2022\",\"Lightman 2023\"]}\n      ],\n      \"benchmarks\": [\"SWE-bench\",\"GSM8K\",\"MATH\"]\n    },\n    {\n      \"area\": \"Efficiency\",\n      \"trends\": [\"4–8bit 양자화\", \"PagedAttention 서빙\", \"KV 캐시 최적화\"],\n      \"algorithms\": [\n        {\"name\": \"GPTQ/AWQ/SmoothQuant\", \"refs\": [\"Frantar 2022\",\"Lin 2023\",\"Xiao 2023\"]},\n        {\"name\": \"vLLM PagedAttention\", \"refs\": [\"Kwon 2023\"]}\n      ],\n      \"benchmarks\": [\"Latency/Throughput\",\"Energy per token\"]\n    },\n    {\n      \"area\": \"Safety/Privacy\",\n      \"trends\": [\"Constitutional AI\", \"워터마킹/출처표시\", \"언러닝\"],\n      \"benchmarks\": [\"AdvBench\",\"RealToxicityPrompts\"]\n    },\n    {\n      \"area\": \"Robotics\",\n      \"trends\": [\"대규모 다기관 데이터\", \"VLA/디퓨전 정책\"],\n      \"algorithms\": [\n        {\"name\": \"RT-2\", \"refs\": [\"Brohan 2023\"]},\n        {\"name\": \"Diffusion Policy\", \"refs\": [\"Chi 2023\"]}\n      ],\n      \"benchmarks\": [\"LIBERO\",\"RealRobotBench\"]\n    }\n  ]\n}\n\n원하시면 위 JSON을 파일(.json/.md)로 저장하거나, 분야별로 더 자세한 수식·알고리즘(예: 장문맥 RoPE 보정의 정밀 도출, Flow Matching의 엄밀한 편미분 방정식 유도, Speculative 수용률의 KL 상계)을 확장해 드리겠습니다. 저장 위치와 형식을 알려주세요.\n\n---",
    "tool_choice": "general",
    "sources": null
  }
]