#!/usr/bin/env python
"""
ê¸°ì¡´ ì²­í¬ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”©ë§Œ ì¬ìƒì„±

Usage:
    python scripts/data/reembed_from_existing.py --paper-id 1
"""

import os
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import argparse
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv
from langchain_core.documents import Document

from src.database.embeddings import get_embeddings

load_dotenv()


def reembed_paper_from_existing(paper_id: int, collection_name: str = "paper_chunks"):
    """
    ê¸°ì¡´ ì²­í¬ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ì¬ìƒì„±
    
    Args:
        paper_id: ë…¼ë¬¸ ID
        collection_name: ì»¬ë ‰ì…˜ëª…
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“ paper_id={paper_id} ì„ë² ë”© ì¬ìƒì„± (ê¸°ì¡´ í…ìŠ¤íŠ¸ ì‚¬ìš©)")
    print(f"{'='*80}")
    
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    cursor = conn.cursor(cursor_factory=RealDictCursor)
    
    # 1. ì»¬ë ‰ì…˜ UUID ê°€ì ¸ì˜¤ê¸°
    cursor.execute("""
        SELECT uuid FROM langchain_pg_collection WHERE name = %s;
    """, (collection_name,))
    collection_result = cursor.fetchone()
    
    if not collection_result:
        print(f"âŒ ì»¬ë ‰ì…˜ '{collection_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        cursor.close()
        conn.close()
        return False
    
    collection_uuid = collection_result['uuid']
    
    # 2. ê¸°ì¡´ ì²­í¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    print(f"\nğŸ“¦ ê¸°ì¡´ ì²­í¬ ë°ì´í„° ë¡œë“œ ì¤‘...")
    cursor.execute("""
        SELECT id, document, cmetadata
        FROM langchain_pg_embedding
        WHERE collection_id = %s
        AND cmetadata->>'paper_id' = %s
        ORDER BY (cmetadata->>'chunk_index')::int;
    """, (collection_uuid, str(paper_id)))
    
    existing_chunks = cursor.fetchall()
    
    if not existing_chunks:
        print(f"âŒ paper_id={paper_id}ì˜ ì²­í¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        cursor.close()
        conn.close()
        return False
    
    print(f"   âœ… {len(existing_chunks)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    
    # 3. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘ (ëª¨ë¸: text-embedding-3-small)...")
    embeddings = get_embeddings()
    
    # 4. ê° ì²­í¬ì˜ ì„ë² ë”© ì¬ìƒì„±
    updated_count = 0
    for i, chunk_data in enumerate(existing_chunks, 1):
        chunk_id = chunk_data['id']
        text = chunk_data['document']
        metadata = chunk_data['cmetadata']
        
        # ìƒˆ ì„ë² ë”© ìƒì„±
        new_embedding = embeddings.embed_query(text)
        
        # DB ì—…ë°ì´íŠ¸
        cursor.execute("""
            UPDATE langchain_pg_embedding
            SET embedding = %s
            WHERE id = %s;
        """, (str(new_embedding), chunk_id))
        
        updated_count += 1
        
        if i % 5 == 0 or i == len(existing_chunks):
            print(f"   ì§„í–‰: {i}/{len(existing_chunks)} ({i/len(existing_chunks)*100:.1f}%)")
    
    conn.commit()
    print(f"   âœ… {updated_count}ê°œ ì„ë² ë”© ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    
    # 5. ê²€ì¦
    print(f"\nğŸ” ì¬ìƒì„± ê²€ì¦ ì¤‘...")
    
    cursor.execute("""
        SELECT COUNT(*)
        FROM langchain_pg_embedding
        WHERE collection_id = %s
        AND cmetadata->>'paper_id' = %s;
    """, (collection_uuid, str(paper_id)))
    after_count = cursor.fetchone()['count']
    
    print(f"   ì €ì¥ëœ ì²­í¬ ìˆ˜: {after_count}ê°œ")
    
    if after_count == len(existing_chunks):
        print(f"   âœ… ê²€ì¦ ì„±ê³µ!")
    else:
        print(f"   âš ï¸  ì²­í¬ ìˆ˜ ë¶ˆì¼ì¹˜")
    
    cursor.close()
    conn.close()
    
    # 6. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘...")
    from src.database.vector_store import get_pgvector_store
    
    store = get_pgvector_store(collection_name)
    results = store.similarity_search_with_score("Attention is all you need", k=10)
    
    found = False
    for rank, (doc, score) in enumerate(results, 1):
        if doc.metadata.get('paper_id') == paper_id:
            found = True
            print(f"   âœ… [{rank}ìœ„] paper_id={paper_id} ë°œê²¬! (L2 ê±°ë¦¬: {score:.4f})")
            break
    
    if not found:
        print(f"   âš ï¸  paper_id={paper_id}ê°€ Top-10ì— ì—†ìŠµë‹ˆë‹¤.")
        print(f"   Top-3 ê²°ê³¼:")
        for rank, (doc, score) in enumerate(results[:3], 1):
            pid = doc.metadata.get('paper_id')
            print(f"      [{rank}ìœ„] paper_id={pid}, L2={score:.4f}")
    
    print(f"\n{'='*80}")
    print(f"âœ… paper_id={paper_id} ì„ë² ë”© ì¬ìƒì„± ì™„ë£Œ!")
    print(f"{'='*80}\n")
    
    return True


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ê¸°ì¡´ ì²­í¬ë¡œ ì„ë² ë”© ì¬ìƒì„±")
    parser.add_argument("--paper-id", type=int, required=True, help="ë…¼ë¬¸ ID")
    parser.add_argument("--collection", type=str, default="paper_chunks", help="ì»¬ë ‰ì…˜ëª…")
    
    args = parser.parse_args()
    
    try:
        success = reembed_paper_from_existing(
            paper_id=args.paper_id,
            collection_name=args.collection
        )
        
        if success:
            print("ğŸ‰ ì„±ê³µ!")
            sys.exit(0)
        else:
            print("âŒ ì‹¤íŒ¨!")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
