# 시나리오: 논문 요약 (Summarize)

## 📋 도구 설명

**도구명**: `summarize`
**목적**: 특정 논문의 전문을 요약하여 핵심 내용 제공

### 주요 기능
- 논문 전문 요약
- 구조화된 요약 (배경, 방법론, 결과, 결론)
- 난이도별 요약 스타일
- 핵심 기여도 강조

---

## 🎯 사용 시나리오

### 시나리오 1: 논문 전체 요약

**사용자 상황**: 논문을 빠르게 파악하고 싶음

**사용자 질문**:
```
"Attention Is All You Need 논문을 요약해줘"
```

**Agent 동작**:
1. Router가 "요약" 키워드 감지
2. `summarize` 도구 선택
3. 논문 제목으로 DB 검색
4. 논문 내용 추출
5. 구조화된 요약 생성

**예상 답변** (Easy 모드):
```
📄 **논문 요약: Attention Is All You Need**
👥 저자: Vaswani et al.
📅 연도: 2017
🏢 기관: Google Research

---

## 🎯 연구 목적
기존 RNN/CNN 없이 오직 Attention만으로 시퀀스 모델을 만들 수 있을까?

## 💡 핵심 아이디어
**Transformer 아키텍처** 제안:
- RNN 대신 Self-Attention 사용
- 순차 처리 불필요 → 병렬 처리 가능
- 긴 문장도 효과적으로 처리

## 🔬 주요 기술
1. **Multi-Head Attention**: 여러 관점에서 정보 수집
2. **Positional Encoding**: 단어 순서 정보 추가
3. **Feed-Forward Network**: 각 위치별 독립 처리

## 📊 실험 결과
- 기계 번역: BLEU 28.4 (기존 최고 대비 +2.0)
- 학습 속도: 기존 모델 대비 10배 빠름
- 영어-독일어, 영어-프랑스어 번역 모두 최고 성능

## 🚀 영향력
- GPT, BERT, T5 등 모든 현대 LLM의 기반
- NLP 분야 혁명적 변화
- Vision, Speech 등 다른 분야로 확산

## 🔑 핵심 기여
1. RNN 없는 시퀀스 모델 성공
2. 병렬 처리로 학습 속도 향상
3. Attention의 범용성 입증
```

**예상 답변** (Hard 모드):
```
📄 **논문 요약: Attention Is All You Need**
👥 저자: Ashish Vaswani, Noam Shazeer, Niki Parmar, et al.
📅 발표: NIPS 2017
🏢 Google Brain & Google Research

---

## 📌 Abstract
Sequence transduction 모델의 주류인 RNN/CNN 기반 encoder-decoder 구조를 완전히 배제하고,
오직 attention mechanism만으로 구성된 Transformer 아키텍처를 제안.

## 🎯 Motivation
- **RNN의 한계**: 순차적 계산으로 인한 병렬화 불가
- **Long-range dependency**: 거리가 먼 단어 간 관계 학습 어려움
- **계산 효율성**: 학습 시간 과다

## 🏗️ Architecture

### Encoder
- N=6개 레이어 스택
- 각 레이어: Multi-Head Self-Attention + Feed-Forward Network
- Residual Connection + Layer Normalization

### Decoder
- N=6개 레이어 스택
- Masked Multi-Head Self-Attention (자기회귀적 생성)
- Encoder-Decoder Attention
- Feed-Forward Network

### Attention Mechanism
```
Attention(Q, K, V) = softmax(QK^T / √d_k)V
```
- Scaled Dot-Product Attention
- Multi-Head Attention (h=8 heads)
- d_model = 512, d_k = d_v = 64

### Positional Encoding
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

## 📊 Experiments

### Datasets
- WMT 2014 English-German (4.5M sentence pairs)
- WMT 2014 English-French (36M sentence pairs)

### Results
| Model | BLEU (EN-DE) | BLEU (EN-FR) | Training Cost |
|-------|--------------|--------------|---------------|
| GNMT | 24.6 | 39.9 | - |
| ConvS2S | 25.1 | 40.4 | - |
| **Transformer (base)** | **27.3** | **38.1** | **3.5 days** |
| **Transformer (big)** | **28.4** | **41.8** | **12 hrs** |

### Ablation Studies
- Layer 수: 6이 최적
- Head 수: 8이 최적
- Attention dropout: 0.1
- Residual dropout: 0.1

## 🔬 Analysis
- Attention 가시화: 구문 구조 학습 확인
- Self-Attention: Anaphora resolution 성공
- 계산 복잡도: O(n²d) vs RNN O(nd²)

## 💡 Key Contributions
1. **순수 Attention 기반 모델**: RNN/CNN 완전 배제
2. **병렬화 가능**: 학습 속도 대폭 향상
3. **SOTA 달성**: 기계 번역 벤치마크 최고 성능
4. **일반화 성능**: English constituency parsing 등 다른 작업에도 성공

## 🌟 Impact
- **Citations**: 100,000+ (2024 기준)
- **파생 연구**: BERT, GPT, T5, Vision Transformer 등
- **산업 응용**: Google Translate, ChatGPT 등

---

**요약 생성 시각**: 2024-11-03 10:30
**참고**: 논문 전문 기반 요약
```

---

### 시나리오 2: 특정 섹션 요약

**사용자 질문**:
```
"BERT 논문의 방법론 부분만 요약해줘"
```

**Agent 동작**:
1. `summarize` 도구 선택
2. BERT 논문 검색
3. "방법론" 섹션 추출
4. 해당 부분만 요약

**예상 답변**:
```
📄 **BERT 방법론 요약**

## 🏗️ Pre-training Tasks

### 1. Masked Language Model (MLM)
- 입력 토큰의 15%를 랜덤 마스킹
- 마스킹 전략:
  * 80%: [MASK] 토큰으로 대체
  * 10%: 랜덤 토큰으로 대체
  * 10%: 원본 유지
- 양방향 컨텍스트 활용 가능

### 2. Next Sentence Prediction (NSP)
- 두 문장이 연속된 문장인지 예측
- 50% IsNext, 50% NotNext
- QA, NLI 등 문장 관계 이해 작업에 유용

## 🎯 Fine-tuning
1. Task-specific 입력/출력 레이어 추가
2. 모든 파라미터 end-to-end fine-tuning
3. 대부분의 작업: 3 epochs 내 수렴

## ⚙️ Model Configurations
- BERT-Base: L=12, H=768, A=12, 110M params
- BERT-Large: L=24, H=1024, A=16, 340M params
```

---

### 시나리오 3: 다수 논문 비교 요약

**사용자 질문**:
```
"GPT-2와 GPT-3의 차이점을 요약해줘"
```

**Agent 동작**:
1. `summarize` 도구 선택
2. GPT-2, GPT-3 논문 모두 검색
3. 비교 중심 요약

**예상 답변**:
```
📊 **GPT-2 vs GPT-3 비교 요약**

## 📈 규모 차이
| | GPT-2 | GPT-3 |
|---|-------|-------|
| **파라미터** | 1.5B | 175B |
| **레이어 수** | 48 | 96 |
| **학습 데이터** | 40GB | 570GB |

## 🎯 주요 차이점

### 1. Few-Shot Learning
- **GPT-2**: Fine-tuning 필요
- **GPT-3**: Few-shot, One-shot, Zero-shot 가능

### 2. In-Context Learning
- **GPT-3의 핵심**: 예제를 프롬프트로 제시
- Task 설명 없이도 작업 수행

### 3. 성능
- GPT-3가 대부분의 NLP 벤치마크에서 SOTA
- 번역, QA, 코드 생성 등 다양한 작업

## 💡 패러다임 변화
- GPT-2: 전통적 Transfer Learning (Pre-train → Fine-tune)
- GPT-3: Prompt-based Learning (In-context)

## 📊 실험 결과
- SuperGLUE: GPT-3 71.8% (Fine-tuned SOTA: 89.8%)
- TriviaQA: GPT-3 71.2% (Zero-shot)
```

---

## 📊 성능 지표

- **요약 품질**: 높음 (구조화된 요약)
- **응답 속도**: 중간 (논문 검색 + LLM)
- **커버리지**: DB에 논문이 있어야 함

---

## 🔧 내부 구현

### 요약 프롬프트
```python
system_prompt = """당신은 논문 요약 전문가입니다.
다음 논문을 구조화하여 요약하세요:
- 연구 목적
- 방법론
- 실험 결과
- 핵심 기여
"""
```

### 로깅
- **tools/summarize.log**: 요약 과정
- **outputs/summary.md**: 생성된 요약 저장

---

## ⚠️ 제한사항

1. **논문 전문 필요**: Abstract만으로는 상세 요약 어려움
2. **DB 의존**: 논문이 DB에 저장되어 있어야 함
3. **수식 처리**: 복잡한 수식은 텍스트로 근사

---

## 🔄 다른 도구로의 전환

| 상황 | 추천 도구 |
|------|----------|
| 논문 검색 필요 | `search_paper` |
| 최신 논문 요약 | `web_search` → `summarize` |
| 요약 저장 필요 | `save_file` |

---

## 💡 활용 팁

1. **논문 제목 정확히**: 제목을 정확히 입력하면 검색 정확도 향상
2. **난이도 활용**: Easy는 개요, Hard는 기술적 상세
3. **섹션 지정**: "방법론만", "결과만" 등 특정 섹션 요청 가능

---

## 📈 예상 질문 리스트

### 단일요청 (1개)
1. "Attention Is All You Need" 논문의 내용을 요약해줘

### 다중요청 (다른 도구와 결합)
- `search_paper` → `summarize`: Transformer 논문들에 대해 정리해서 쉽게 설명해줘
- `search_paper` → `summarize`: GPT 계열 모델의 발전 과정을 논문 기준으로 정리해줘
- `search_paper` → `summarize`: BERT와 RoBERTa의 차이점을 관련 논문들을 바탕으로 비교해줘
- `text2sql` → `summarize`: 2024년 발표된 BERT 계열 논문의 수를 통계로 보여주고, 대표 논문을 하나 요약해줘

---

**작성일**: 2025-11-05
**버전**: 2.0
