# 제목: 성능 평가 시스템 개선 - 구체적 평가 기준 추가

---

## 📋 작업 개요
**작업 주제:** LLM-as-a-Judge 평가 프롬프트 개선 - 구체적이고 일관성 있는 평가 기준 추가
**작성자:** 최현화[팀장]
**담당자:** @최현화
**마감일:** 2025-11-05

## 📅 기간
- 시작일: 2025-11-04
- 종료일: 2025-11-05

---

## 📌 이슈 목적

현재 LLM-as-a-Judge 평가 시스템의 평가 프롬프트가 너무 간단하여 **주관적이고 일관성 없는 평가**가 이루어지고 있습니다. 각 평가 기준(정확도, 관련성, 난이도 적합성, 출처 명시)에 대해 **구체적인 점수 부여 기준**을 추가하여 평가의 일관성과 신뢰성을 향상시킵니다.

**핵심 목표:**
- 각 평가 기준별 구체적인 점수 부여 기준 정의
- 평가 프롬프트에 상세 기준 추가
- 평가 일관성 검증 테스트 수행
- 평가 결과 비교 분석 (개선 전/후)

---

## 🔍 현재 문제점 분석

### 1. **현재 평가 프롬프트 (src/evaluation/evaluator.py:36-40)**

```
[평가 기준]
1. 정확도 (0-10점): 참고 문서의 내용과 일치하는가?
2. 관련성 (0-10점): 질문과 답변이 관련있는가?
3. 난이도 적합성 (0-10점): 난이도 모드에 맞는 답변인가?
4. 출처 명시 (0-10점): 논문 제목, 저자를 명시했는가?
```

### 2. **주요 문제점**

#### 문제 1: **정확도 (Accuracy) 기준 모호**
- **현재**: "참고 문서의 내용과 일치하는가?"만 물어봄
- **문제**:
  - "일치"의 기준이 불명확 (100% 일치? 핵심만 일치?)
  - "부분적 일치"의 범위가 모호
  - 오류가 포함된 경우 감점 기준 없음
- **결과**: LLM이 주관적으로 판단하여 같은 답변도 평가 시마다 점수 변동

**예시:**
```
질문: "Transformer 모델의 핵심 구조는?"
답변: "Transformer는 Self-Attention 메커니즘을 사용합니다."
참고 문서: "Transformer는 Self-Attention과 Feed-Forward Neural Network로 구성됩니다."

→ 이 경우 몇 점? 5점? 7점? 8점? 기준 없음!
```

#### 문제 2: **관련성 (Relevance) 기준 모호**
- **현재**: "질문과 답변이 관련있는가?"만 물어봄
- **문제**:
  - "관련있다"의 기준이 불명확
  - 직접 답변 vs 간접 답변의 차이 모호
  - 질문에서 벗어난 정도 측정 불가
- **결과**: 부분적으로 답변한 경우 점수 편차 큼

**예시:**
```
질문: "BERT의 학습 방법은?"
답변: "BERT는 자연어 처리에 사용되는 모델입니다."

→ 관련은 있지만 질문에 직접 답변하지 않음. 몇 점?
```

#### 문제 3: **난이도 적합성 (Difficulty) 기준 모호**
- **현재**: "난이도 모드에 맞는 답변인가?"만 물어봄
- **문제**:
  - Easy 모드의 "쉬운 설명" 기준 불명확
  - Hard 모드의 "전문적 설명" 기준 불명확
  - 용어 사용, 설명 깊이 기준 없음
- **결과**: 같은 답변도 평가자마다 다르게 판단

**예시 (Easy 모드):**
```
답변 A: "Attention은 중요한 부분에 집중하는 메커니즘입니다."
답변 B: "Attention은 Query, Key, Value의 내적으로 계산됩니다."

→ 둘 다 Easy 모드에 적합? 답변 B는 너무 어려운가?
```

#### 문제 4: **출처 명시 (Citation) 기준 불명확**
- **현재**: "논문 제목, 저자를 명시했는가?"만 물어봄
- **문제**:
  - 제목만 있으면 몇 점? 저자만 있으면?
  - 발행 연도는 필수인가 선택인가?
  - URL, DOI는 평가에 포함되는가?
- **결과**: 부분적 출처 명시 시 점수 편차 큼

**예시:**
```
답변 A: "Attention Is All You Need 논문에서..."
답변 B: "Vaswani et al.의 연구에 따르면..."
답변 C: "Attention Is All You Need (Vaswani et al., 2017)에서..."

→ 각각 몇 점? 10점, 7점, 10점? 기준 없음!
```

---

## 💡 개선 방안

### 1. **구체적 점수 구간 정의**

각 평가 기준마다 **점수 구간별 명확한 기준**을 제시합니다.

**점수 구간:**
- **10점**: 완벽 (Perfect)
- **7-9점**: 우수 (Excellent)
- **4-6점**: 보통 (Fair)
- **1-3점**: 미흡 (Poor)
- **0점**: 매우 미흡 (Very Poor)

### 2. **평가 항목별 세부 기준**

#### (1) 정확도 (Accuracy) - 참고 문서와의 일치도

**평가 기준:**
```
10점 (완벽):
- 참고 문서의 핵심 내용을 모두 정확히 반영
- 사실 관계 오류 없음
- 추가 정보도 정확함

7-9점 (우수):
- 참고 문서의 핵심 내용 80% 이상 정확히 반영
- 사소한 표현 차이는 있으나 의미는 동일
- 사실 관계 오류 없음

4-6점 (보통):
- 참고 문서의 핵심 내용 50% 이상 반영
- 일부 내용 누락 또는 단순화
- 경미한 오류 1-2개 포함 가능

1-3점 (미흡):
- 참고 문서의 내용 일부만 정확
- 중요한 내용 누락
- 사실 관계 오류 여러 개 포함

0점 (매우 미흡):
- 참고 문서와 무관한 내용
- 완전히 틀린 정보
- 거짓 정보 (Hallucination)
```

**평가 예시:**

| 질문 | 참고 문서 | 답변 | 정확도 점수 | 이유 |
|-----|----------|------|------------|------|
| Transformer의 핵심 구조는? | "Self-Attention과 Feed-Forward Neural Network로 구성" | "Self-Attention 메커니즘 사용" | 6점 | 핵심 내용 50% 반영 (FFN 누락) |
| BERT의 학습 방법은? | "MLM과 NSP 사전학습" | "Masked Language Modeling과 Next Sentence Prediction으로 사전학습" | 10점 | 완벽히 정확 |
| GPT의 특징은? | "Transformer Decoder 기반의 자기회귀 모델" | "Transformer Encoder 기반" | 1점 | 사실 관계 오류 (Decoder→Encoder) |

#### (2) 관련성 (Relevance) - 질문과 답변의 관련도

**평가 기준:**
```
10점 (완벽):
- 질문에 직접적이고 완전하게 답변
- 질문이 요구하는 모든 정보 포함
- 불필요한 내용 없음

7-9점 (우수):
- 질문에 직접 답변했으나 일부 정보 누락
- 핵심은 다루었으나 세부 사항 부족
- 약간의 불필요한 내용 포함 가능

4-6점 (보통):
- 질문과 관련은 있으나 핵심을 일부 벗어남
- 답변이 간접적이거나 우회적
- 불필요한 내용이 다수 포함

1-3점 (미흡):
- 질문과 부분적으로만 관련
- 질문의 핵심을 다루지 않음
- 대부분 불필요한 내용

0점 (매우 미흡):
- 질문과 완전히 무관한 답변
- 엉뚱한 주제로 답변
```

**평가 예시:**

| 질문 | 답변 | 관련성 점수 | 이유 |
|-----|------|-----------|------|
| BERT의 학습 방법은? | "BERT는 MLM과 NSP로 사전학습합니다" | 10점 | 질문에 직접 답변 |
| BERT의 학습 방법은? | "BERT는 자연어 처리 모델입니다" | 2점 | 질문과 부분적으로만 관련 |
| Transformer의 장점은? | "Transformer는 2017년 Google이 발표했습니다" | 1점 | 질문의 핵심(장점)을 다루지 않음 |

#### (3) 난이도 적합성 (Difficulty) - 모드별 설명 수준

**Easy 모드 평가 기준:**
```
10점 (완벽):
- 일상 용어 사용 (전문 용어 최소화)
- 비유, 예시를 통한 설명
- 단계별, 쉬운 설명
- 중학생도 이해 가능한 수준

7-9점 (우수):
- 대부분 쉬운 용어 사용
- 일부 전문 용어 포함하나 설명 추가
- 비유나 예시 일부 포함
- 고등학생 수준

4-6점 (보통):
- 쉬운 용어와 전문 용어 혼재
- 비유나 예시 부족
- 대학생 수준

1-3점 (미흡):
- 전문 용어 다수 사용
- 설명이 어렵고 추상적
- 대학원생 수준

0점 (매우 미흡):
- Hard 모드 수준의 설명
- 수식, 논문 원문 수준
```

**Hard 모드 평가 기준:**
```
10점 (완벽):
- 전문 용어 정확히 사용
- 수식, 알고리즘 포함
- 논문 원문 수준의 설명
- 세부 구현 내용 포함

7-9점 (우수):
- 대부분 전문적 설명
- 일부 간단한 설명 포함
- 주요 개념은 전문적으로 설명

4-6점 (보통):
- 전문 용어와 쉬운 설명 혼재
- 중간 수준의 설명

1-3점 (미흡):
- 쉬운 용어 다수 사용
- 비유나 예시 위주
- 피상적 설명

0점 (매우 미흡):
- Easy 모드 수준의 설명
- 전문성 부족
```

**평가 예시:**

| 모드 | 질문 | 답변 | 난이도 점수 | 이유 |
|-----|-----|------|-----------|------|
| Easy | Attention이란? | "중요한 부분에 집중하는 메커니즘. 시험 공부할 때 핵심 부분에 집중하는 것과 비슷" | 10점 | 비유 사용, 쉬운 설명 |
| Easy | Attention이란? | "Query, Key, Value의 내적으로 계산되는 메커니즘" | 2점 | 전문 용어, 어려운 설명 |
| Hard | Transformer 구조는? | "Multi-head Self-Attention과 Position-wise FFN으로 구성. Attention(Q,K,V) = softmax(QK^T/√d_k)V" | 10점 | 수식 포함, 전문적 설명 |
| Hard | Transformer 구조는? | "여러 부분으로 나눠서 중요한 걸 찾는 구조" | 1점 | 쉬운 설명, 전문성 부족 |

#### (4) 출처 명시 (Citation) - 참고 문헌 인용

**평가 기준:**
```
10점 (완벽):
- 논문 제목 + 저자 + 발행 연도 모두 명시
- 예: "Attention Is All You Need (Vaswani et al., 2017)"
- URL, DOI 추가 시 추가 점수는 없으나 긍정적

7-9점 (우수):
- 논문 제목 + 저자 명시 (연도 누락)
- 예: "Attention Is All You Need (Vaswani et al.)"
- 또는 제목 + 연도 (저자 누락)

4-6점 (보통):
- 논문 제목만 명시
- 예: "Attention Is All You Need 논문에서..."
- 또는 저자만 명시
- 예: "Vaswani et al.의 연구에 따르면..."

1-3점 (미흡):
- 출처를 언급했으나 구체적이지 않음
- 예: "한 논문에 따르면...", "연구에 의하면..."
- 출처가 모호하거나 불완전

0점 (매우 미흡):
- 출처 명시 전혀 없음
- 마치 자신의 의견처럼 서술
```

**평가 예시:**

| 답변 | 출처 점수 | 이유 |
|------|---------|------|
| "Attention Is All You Need (Vaswani et al., 2017)에서 제안한 Transformer는..." | 10점 | 제목 + 저자 + 연도 완벽 |
| "Vaswani et al.의 Attention Is All You Need에서..." | 8점 | 제목 + 저자 (연도 누락) |
| "Attention Is All You Need 논문에서..." | 5점 | 제목만 명시 |
| "한 연구에 따르면..." | 2점 | 출처 모호 |
| "Transformer는..." (출처 없음) | 0점 | 출처 명시 없음 |

---

## 🛠️ 구현 가이드

### 1. **평가 프롬프트 수정**

**파일**: `src/evaluation/evaluator.py`

**수정 위치**: 21-51줄 `EVALUATION_PROMPT_TEMPLATE`

**수정 전:**
```python
[평가 기준]
1. 정확도 (0-10점): 참고 문서의 내용과 일치하는가?
2. 관련성 (0-10점): 질문과 답변이 관련있는가?
3. 난이도 적합성 (0-10점): 난이도 모드에 맞는 답변인가?
4. 출처 명시 (0-10점): 논문 제목, 저자를 명시했는가?
```

**수정 후:**
```python
[평가 기준]

1. 정확도 (Accuracy) - 참고 문서와의 일치도 (0-10점)
   - 10점: 참고 문서의 핵심 내용을 모두 정확히 반영, 사실 관계 오류 없음
   - 7-9점: 핵심 내용 80% 이상 정확히 반영, 사소한 표현 차이만 있음
   - 4-6점: 핵심 내용 50% 이상 반영, 일부 내용 누락 또는 경미한 오류 1-2개
   - 1-3점: 일부만 정확, 중요 내용 누락, 사실 오류 여러 개
   - 0점: 참고 문서와 무관하거나 완전히 틀린 정보

2. 관련성 (Relevance) - 질문과 답변의 관련도 (0-10점)
   - 10점: 질문에 직접적이고 완전하게 답변, 불필요한 내용 없음
   - 7-9점: 질문에 직접 답변했으나 일부 정보 누락, 약간의 불필요한 내용 가능
   - 4-6점: 질문과 관련은 있으나 핵심 일부 벗어남, 답변이 간접적
   - 1-3점: 질문과 부분적으로만 관련, 핵심을 다루지 않음
   - 0점: 질문과 완전히 무관한 답변

3. 난이도 적합성 (Difficulty) - 모드별 설명 수준 (0-10점)

   [Easy 모드 기준]
   - 10점: 일상 용어, 비유/예시 사용, 단계별 쉬운 설명 (중학생 수준)
   - 7-9점: 대부분 쉬운 용어, 일부 전문 용어는 설명 추가 (고등학생 수준)
   - 4-6점: 쉬운 용어와 전문 용어 혼재 (대학생 수준)
   - 1-3점: 전문 용어 다수, 어렵고 추상적 설명 (대학원생 수준)
   - 0점: Hard 모드 수준의 전문적 설명

   [Hard 모드 기준]
   - 10점: 전문 용어 정확히 사용, 수식/알고리즘 포함, 논문 원문 수준
   - 7-9점: 대부분 전문적, 일부 간단한 설명 포함
   - 4-6점: 전문 용어와 쉬운 설명 혼재
   - 1-3점: 쉬운 용어 다수, 비유 위주, 피상적 설명
   - 0점: Easy 모드 수준의 쉬운 설명

4. 출처 명시 (Citation) - 참고 문헌 인용 (0-10점)
   - 10점: 논문 제목 + 저자 + 발행 연도 모두 명시
   - 7-9점: 논문 제목 + 저자 명시 (연도 누락) 또는 제목 + 연도 (저자 누락)
   - 4-6점: 논문 제목만 명시 또는 저자만 명시
   - 1-3점: 출처 언급했으나 구체적이지 않음 (예: "한 논문에 따르면")
   - 0점: 출처 명시 전혀 없음

[중요 지침]
- 각 기준별로 정확한 점수를 부여하세요
- 점수 부여 이유를 comment에 간단히 설명하세요
- total_score는 4개 항목의 합계입니다 (0-40점)
```

### 2. **코드 수정 사항**

**파일 경로**: `src/evaluation/evaluator.py`

**수정 내용**:
1. `EVALUATION_PROMPT_TEMPLATE` (21-51줄) 전체 교체
2. 기존 코드 구조는 유지 (변경 없음)
3. LLM 호출 부분 변경 없음

### 3. **테스트 케이스 작성**

**파일**: `scripts/test_evaluation_improvement.py` (신규 생성)

**테스트 내용**:
1. 동일 답변 10회 평가 → 점수 편차 확인 (표준편차 1점 이내 목표)
2. 다양한 품질의 답변 평가 → 점수 분포 확인
3. 개선 전/후 비교 → 일관성 개선도 측정

**예시 테스트 케이스**:
```python
test_cases = [
    {
        "question": "Transformer의 핵심 구조는?",
        "answer": "Self-Attention 메커니즘을 사용합니다.",
        "reference_docs": "Self-Attention과 Feed-Forward Neural Network로 구성됩니다.",
        "difficulty": "easy",
        "expected_accuracy": "4-7점",  # 핵심 내용 50% 반영
        "expected_relevance": "9-10점",  # 질문에 직접 답변
        "expected_difficulty": "8-10점",  # Easy 모드 적합
        "expected_citation": "0점"  # 출처 없음
    },
    # ... 추가 테스트 케이스
]
```

---

## 📊 평가 일관성 검증 방법

### 1. **일관성 테스트**

**목적**: 같은 답변을 여러 번 평가했을 때 점수 편차 확인

**방법**:
```python
# 동일 답변 10회 평가
results = []
for i in range(10):
    result = evaluator.evaluate(
        question="Transformer의 핵심 구조는?",
        answer="Self-Attention과 Feed-Forward Neural Network로 구성됩니다.",
        reference_docs="...",
        difficulty="easy"
    )
    results.append(result)

# 점수 편차 분석
import numpy as np
accuracy_scores = [r['accuracy_score'] for r in results]
print(f"평균: {np.mean(accuracy_scores):.2f}")
print(f"표준편차: {np.std(accuracy_scores):.2f}")  # 목표: 1.0 이하
print(f"범위: {np.min(accuracy_scores)} ~ {np.max(accuracy_scores)}")
```

**목표 기준**:
- 표준편차 ≤ 1.0점 (우수)
- 표준편차 ≤ 1.5점 (양호)
- 표준편차 > 2.0점 (개선 필요)

### 2. **점수 분포 검증**

**목적**: 다양한 품질의 답변이 적절히 구분되는지 확인

**테스트 케이스**:
```python
# 품질별 답변 샘플
quality_levels = {
    "excellent": "완벽한 답변 (예상: 35-40점)",
    "good": "좋은 답변 (예상: 28-34점)",
    "fair": "보통 답변 (예상: 20-27점)",
    "poor": "나쁜 답변 (예상: 10-19점)",
    "very_poor": "매우 나쁜 답변 (예상: 0-9점)"
}
```

**검증 기준**:
- 각 품질 레벨이 명확히 구분되는가?
- 예상 점수 범위에 포함되는가?

### 3. **개선 전/후 비교**

**측정 지표**:
1. **일관성**: 표준편차 감소율
2. **정확성**: 예상 점수와 실제 점수 차이
3. **평가 시간**: LLM 호출 시간 변화

**비교 표**:

| 지표 | 개선 전 | 개선 후 | 개선율 |
|-----|--------|--------|--------|
| 표준편차 (정확도) | 2.5점 | 0.8점 | 68% 개선 |
| 표준편차 (관련성) | 2.1점 | 1.0점 | 52% 개선 |
| 표준편차 (난이도) | 3.2점 | 1.2점 | 63% 개선 |
| 표준편차 (출처) | 1.8점 | 0.5점 | 72% 개선 |
| 평균 평가 시간 | 3.2초 | 3.5초 | -9% (허용) |

---

## ✅ 작업 항목 체크리스트

### Phase 1: 프롬프트 개선 (1시간)
- [ ] `src/evaluation/evaluator.py` 파일 수정
  - [ ] `EVALUATION_PROMPT_TEMPLATE` 교체
  - [ ] 구체적 평가 기준 추가
  - [ ] 각 점수 구간별 상세 기준 명시
  - [ ] Easy/Hard 모드별 기준 구분

### Phase 2: 테스트 코드 작성 (1-2시간)
- [ ] `scripts/test_evaluation_improvement.py` 생성
  - [ ] 일관성 테스트 함수
  - [ ] 점수 분포 검증 함수
  - [ ] 개선 전/후 비교 함수
  - [ ] 다양한 품질의 테스트 케이스 추가 (10개 이상)

### Phase 3: 테스트 수행 및 검증 (1시간)
- [ ] 일관성 테스트 수행
  - [ ] 동일 답변 10회 평가
  - [ ] 표준편차 계산 및 분석
  - [ ] 목표 기준 달성 여부 확인
- [ ] 점수 분포 검증
  - [ ] 다양한 품질 레벨 답변 평가
  - [ ] 점수 분포가 예상 범위에 포함되는지 확인
- [ ] 개선 전/후 비교
  - [ ] 표준편차 감소율 계산
  - [ ] 평가 시간 변화 측정

### Phase 4: 문서화 및 커밋 (30분)
- [ ] 평가 결과 문서화
  - [ ] 개선 효과 정리
  - [ ] 테스트 결과 스크린샷
- [ ] 코드 커밋
  - [ ] `feat: 평가 프롬프트 구체적 기준 추가`
  - [ ] `test: 평가 일관성 테스트 추가`
- [ ] 관련 문서 업데이트
  - [ ] `docs/modularization/11_성능평가시스템.md` 업데이트

---

## 🎯 예상 개선 효과

### 1. **일관성 향상**
- **기대 효과**: 표준편차 60% 이상 감소
- **근거**: 구체적 기준 제시로 LLM의 주관적 판단 최소화

### 2. **신뢰성 향상**
- **기대 효과**: 같은 답변의 평가 점수 편차 1점 이내
- **근거**: 명확한 점수 구간 정의

### 3. **평가 속도**
- **기대 효과**: 평가 시간 10% 이내 증가 (허용 범위)
- **근거**: 프롬프트가 길어지지만 LLM이 더 명확히 판단 가능

### 4. **품질 구분**
- **기대 효과**: 우수/보통/미흡 답변 명확히 구분
- **근거**: 품질 레벨별 점수 범위 정의

---

## 📝 참고 자료

### 관련 문서
- **[docs/modularization/11_성능평가시스템.md](../modularization/11_성능평가시스템.md)** - 성능 평가 시스템 개요
- **[docs/PRD/09_평가_기준.md](../PRD/09_평가_기준.md)** - RAG 평가 지표
- **[docs/issues/05-2_성능평가시스템_구현.md](./05-2_성능평가시스템_구현.md)** - 초기 구현 이슈

### 참고 논문
- **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena** (Zheng et al., 2023)
  - LLM-as-a-Judge 평가 방법론
  - 평가 일관성 향상 기법
- **G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment** (Liu et al., 2023)
  - 구체적 평가 기준의 중요성
  - 프롬프트 엔지니어링 기법

---

## 🚀 다음 단계

이 작업 완료 후:

1. **Streamlit UI 개선**
   - 평가 결과를 더 상세히 표시
   - 각 항목별 점수와 이유 표시
   - 평가 히스토리 차트 추가

2. **평가 기준 추가**
   - 응답 속도 평가
   - 답변 길이 적절성 평가
   - 사용자 만족도 평가 (선택)

3. **자동 개선 시스템**
   - 평가 결과 기반 프롬프트 자동 개선
   - 낮은 점수 항목에 대한 자동 피드백

---

## 📌 작성자 메모

**작성일**: 2025-11-04
**작성자**: 최현화[팀장]

**핵심 포인트**:
- 평가 프롬프트의 구체성이 평가 일관성에 직접적 영향
- 점수 구간별 명확한 기준 필수
- Easy/Hard 모드 구분이 특히 중요
- 테스트를 통한 검증 필수

**주의사항**:
- 프롬프트가 너무 길어지면 LLM 성능 저하 가능 → 적절한 길이 유지
- 개선 후 반드시 일관성 테스트 수행
- 평가 시간이 너무 늘어나지 않도록 주의
