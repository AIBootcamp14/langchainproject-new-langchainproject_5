#!/usr/bin/env python
"""
ì˜¬ë°”ë¥¸ ë°©ì‹ìœ¼ë¡œ ì„ë² ë”© ì¬ìƒì„± (langchain PGVector ì‚¬ìš©)
"""

import os
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import argparse
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv
from langchain_core.documents import Document

from src.database.vector_store import get_pgvector_store

load_dotenv()


def reembed_paper_proper(paper_id: int, collection_name: str = "paper_chunks"):
    """
    langchain PGVectorë¥¼ ì‚¬ìš©í•œ ì˜¬ë°”ë¥¸ ì„ë² ë”© ì¬ìƒì„±
    
    Args:
        paper_id: ë…¼ë¬¸ ID
        collection_name: ì»¬ë ‰ì…˜ëª…
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“ paper_id={paper_id} ì„ë² ë”© ì¬ìƒì„± (PGVector ë°©ì‹)")
    print(f"{'='*80}")
    
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    cursor = conn.cursor(cursor_factory=RealDictCursor)
    
    # 1. ì»¬ë ‰ì…˜ UUID ê°€ì ¸ì˜¤ê¸°
    cursor.execute("""
        SELECT uuid FROM langchain_pg_collection WHERE name = %s;
    """, (collection_name,))
    collection_result = cursor.fetchone()
    
    if not collection_result:
        print(f"âŒ ì»¬ë ‰ì…˜ '{collection_name}'ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        cursor.close()
        conn.close()
        return False
    
    collection_uuid = collection_result['uuid']
    
    # 2. ê¸°ì¡´ ì²­í¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    print(f"\nğŸ“¦ ê¸°ì¡´ ì²­í¬ ë°ì´í„° ë¡œë“œ ì¤‘...")
    cursor.execute("""
        SELECT id, document, cmetadata
        FROM langchain_pg_embedding
        WHERE collection_id = %s
        AND cmetadata->>'paper_id' = %s
        ORDER BY (cmetadata->>'chunk_index')::int NULLS LAST;
    """, (collection_uuid, str(paper_id)))
    
    existing_chunks = cursor.fetchall()
    
    if not existing_chunks:
        print(f"âŒ paper_id={paper_id}ì˜ ì²­í¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        cursor.close()
        conn.close()
        return False
    
    print(f"   âœ… {len(existing_chunks)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ")
    
    cursor.close()
    conn.close()
    
    # 3. Document ê°ì²´ ì¬êµ¬ì„±
    print(f"\nğŸ“¦ Document ê°ì²´ ì¬êµ¬ì„± ì¤‘...")
    documents = []
    chunk_ids = []
    
    for chunk_data in existing_chunks:
        chunk_ids.append(chunk_data['id'])
        doc = Document(
            page_content=chunk_data['document'],
            metadata=chunk_data['cmetadata']
        )
        documents.append(doc)
    
    print(f"   âœ… {len(documents)}ê°œ Document ìƒì„±")
    
    # 4. ê¸°ì¡´ ì²­í¬ ì‚­ì œ
    print(f"\nğŸ—‘ï¸  ê¸°ì¡´ ì²­í¬ ì‚­ì œ ì¤‘...")
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    cursor = conn.cursor()
    
    cursor.execute("""
        DELETE FROM langchain_pg_embedding
        WHERE collection_id = %s
        AND cmetadata->>'paper_id' = %s;
    """, (collection_uuid, str(paper_id)))
    
    deleted_count = cursor.rowcount
    conn.commit()
    cursor.close()
    conn.close()
    
    print(f"   âœ… {deleted_count}ê°œ ì²­í¬ ì‚­ì œ ì™„ë£Œ")
    
    # 5. PGVectorë¡œ ì¬ë“±ë¡ (ì„ë² ë”© ìë™ ìƒì„±)
    print(f"\nğŸ”„ PGVectorë¡œ ì¬ë“±ë¡ ì¤‘ (ì„ë² ë”© ìë™ ìƒì„±)...")
    print(f"   ì˜ˆìƒ ì‹œê°„: ì•½ {len(documents)}ì´ˆ")
    
    store = get_pgvector_store(collection_name)
    store.add_documents(documents)
    
    print(f"   âœ… {len(documents)}ê°œ ì²­í¬ ì¬ë“±ë¡ ì™„ë£Œ")
    
    # 6. ê²€ì¦
    print(f"\nğŸ” ì¬ìƒì„± ê²€ì¦ ì¤‘...")
    
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT uuid FROM langchain_pg_collection WHERE name = %s;
    """, (collection_name,))
    collection_uuid = cursor.fetchone()[0]
    
    cursor.execute("""
        SELECT COUNT(*)
        FROM langchain_pg_embedding
        WHERE collection_id = %s
        AND cmetadata->>'paper_id' = %s;
    """, (collection_uuid, str(paper_id)))
    after_count = cursor.fetchone()[0]
    
    cursor.close()
    conn.close()
    
    print(f"   ì €ì¥ëœ ì²­í¬ ìˆ˜: {after_count}ê°œ")
    
    # 7. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘...")
    
    results = store.similarity_search_with_score("Attention is all you need", k=10)
    
    found = False
    for rank, (doc, score) in enumerate(results, 1):
        if doc.metadata.get('paper_id') == paper_id:
            found = True
            print(f"   âœ… [{rank}ìœ„] paper_id={paper_id} ë°œê²¬! (L2 ê±°ë¦¬: {score:.4f})")
            print(f"   ì²­í¬ ë‚´ìš©: {doc.page_content[:100]}...")
            break
    
    if not found:
        print(f"   âš ï¸  paper_id={paper_id}ê°€ Top-10ì— ì—†ìŠµë‹ˆë‹¤.")
        print(f"\n   Top-5 ê²°ê³¼:")
        for rank, (doc, score) in enumerate(results[:5], 1):
            pid = doc.metadata.get('paper_id')
            print(f"      [{rank}ìœ„] paper_id={pid}, L2={score:.4f}")
            print(f"             {doc.page_content[:80]}...")
    
    print(f"\n{'='*80}")
    print(f"âœ… paper_id={paper_id} ì„ë² ë”© ì¬ìƒì„± ì™„ë£Œ!")
    print(f"{'='*80}\n")
    
    return True


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="PGVector ë°©ì‹ìœ¼ë¡œ ì„ë² ë”© ì¬ìƒì„±")
    parser.add_argument("--paper-id", type=int, required=True, help="ë…¼ë¬¸ ID")
    parser.add_argument("--collection", type=str, default="paper_chunks", help="ì»¬ë ‰ì…˜ëª…")
    
    args = parser.parse_args()
    
    try:
        success = reembed_paper_proper(
            paper_id=args.paper_id,
            collection_name=args.collection
        )
        
        if success:
            print("ğŸ‰ ì„±ê³µ!")
            sys.exit(0)
        else:
            print("âŒ ì‹¤íŒ¨!")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
