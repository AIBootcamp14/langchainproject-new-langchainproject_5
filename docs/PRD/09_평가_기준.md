# 09. 평가 기준

## 문서 정보
- **작성일**: 2025-10-30
- **프로젝트명**: 논문 리뷰 챗봇 (AI Agent + RAG)
- **팀명**: 연결의 민족
- **작성자**: 연결의 민족 팀

---

## 1. 평가 개요

### 1.1 평가 목적

- RAG 시스템의 검색 정확도 및 답변 품질 측정
- 데이터베이스 성능 및 무결성 검증
- 시스템 전체 성능 및 사용자 경험 평가

### 1.2 평가 프로세스 흐름

```mermaid
graph TB
    subgraph Prepare["🔸 평가 준비"]
        direction LR
        A[테스트 데이터<br/>준비<br/>50~200개] --> B[Ground Truth<br/>작성<br/>정답 문서 ID]
        B --> C[평가 메트릭<br/>선택<br/>Recall, F1 등]
    end

    subgraph Execute["🔹 평가 실행"]
        direction TB
        D[시스템 실행<br/>테스트 쿼리] --> E[결과 수집<br/>검색 문서, 답변]
        E --> F[메트릭 계산<br/>Recall@K, F1, MRR]
        F --> G[성능 지표<br/>지연 시간, 비용]
    end

    subgraph Analyze["🔺 분석 & 개선"]
        direction LR
        H[결과 분석<br/>목표 대비 비교] --> I{기준<br/>만족?}
        I -->|No| J[시스템 개선<br/>파라미터 튜닝]
        I -->|Yes| K[✅ 평가 통과]
        J --> D
    end

    Prepare --> Execute
    Execute --> Analyze

    %% Subgraph 스타일
    style Prepare fill:#e1f5ff,stroke:#01579b,stroke-width:3px,color:#000
    style Execute fill:#f3e5f5,stroke:#4a148c,stroke-width:3px,color:#000
    style Analyze fill:#e8f5e9,stroke:#1b5e20,stroke-width:3px,color:#000

    %% 노드 스타일
    style A fill:#90caf9,stroke:#1976d2,color:#000
    style B fill:#64b5f6,stroke:#1976d2,color:#000
    style C fill:#42a5f5,stroke:#1565c0,color:#000
    style D fill:#ce93d8,stroke:#7b1fa2,color:#000
    style E fill:#ba68c8,stroke:#7b1fa2,color:#fff
    style F fill:#ab47bc,stroke:#4a148c,color:#fff
    style G fill:#9c27b0,stroke:#4a148c,color:#fff
    style H fill:#a5d6a7,stroke:#388e3c,color:#000
    style I fill:#ba68c8,stroke:#7b1fa2,color:#fff
    style J fill:#ffcc80,stroke:#f57c00,color:#000
    style K fill:#66bb6a,stroke:#2e7d32,color:#fff
```

### 1.2 평가 대상

1. **RAG 시스템**: 검색 정확도, 답변 충실성, 응답 속도
2. **RDB 시스템**: 쿼리 성능, 데이터 무결성, 백업/복구
3. **AI Agent**: 도구 선택 정확도, 라우팅 성능
4. **UI/UX**: 사용자 인터페이스 및 경험

---

## 2. RAG 평가 기준

### 2.1 정답성/충실성 (Offline)

#### 2.1.1 Recall@K (문서 검색)

**정의:** 정답 근거가 상위 K 문서에 포함되는 비율

**측정 방법:**
```python
from typing import List

def calculate_recall_at_k(
    ground_truth_docs: List[str],  # 정답 문서 ID 리스트
    retrieved_docs: List[str],      # 검색된 문서 ID 리스트
    k: int
) -> float:
    """
    Recall@K 계산

    Args:
        ground_truth_docs: 정답 문서 ID
        retrieved_docs: 검색된 문서 ID (상위 K개)
        k: 상위 K개 문서

    Returns:
        Recall@K 값 (0.0 ~ 1.0)
    """
    top_k_docs = retrieved_docs[:k]
    relevant_retrieved = len(set(ground_truth_docs) & set(top_k_docs))
    total_relevant = len(ground_truth_docs)

    return relevant_retrieved / total_relevant if total_relevant > 0 else 0.0

# 사용 예시
ground_truth = ["doc_1", "doc_3"]
retrieved = ["doc_1", "doc_2", "doc_3", "doc_5"]

recall_3 = calculate_recall_at_k(ground_truth, retrieved, k=3)
recall_5 = calculate_recall_at_k(ground_truth, retrieved, k=5)
recall_10 = calculate_recall_at_k(ground_truth, retrieved, k=10)

print(f"Recall@3: {recall_3:.2f}")
print(f"Recall@5: {recall_5:.2f}")
print(f"Recall@10: {recall_10:.2f}")
```

**평가 기준:**
- K ∈ {3, 5, 10}
- **목표**: Recall@5 ≥ 0.6

#### 2.1.2 Precision@K

**정의:** 상위 K 문서 중 정답 문서의 비율

```python
def calculate_precision_at_k(
    ground_truth_docs: List[str],
    retrieved_docs: List[str],
    k: int
) -> float:
    """Precision@K 계산"""
    top_k_docs = retrieved_docs[:k]
    relevant_retrieved = len(set(ground_truth_docs) & set(top_k_docs))

    return relevant_retrieved / k if k > 0 else 0.0
```

#### 2.1.3 MRR (Mean Reciprocal Rank)

**정의:** 첫 번째 정답 문서의 순위의 역수 평균

```python
def calculate_mrr(
    queries: List[dict]  # [{"ground_truth": [...], "retrieved": [...]}, ...]
) -> float:
    """MRR 계산"""
    reciprocal_ranks = []

    for query in queries:
        ground_truth = set(query["ground_truth"])
        retrieved = query["retrieved"]

        for rank, doc_id in enumerate(retrieved, start=1):
            if doc_id in ground_truth:
                reciprocal_ranks.append(1.0 / rank)
                break
        else:
            reciprocal_ranks.append(0.0)

    return sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0.0
```

#### 2.1.4 EM (Exact Match) & F1 Score

**정의:** 기준 정답 대비 답변 일치도

```python
def calculate_em(prediction: str, ground_truth: str) -> float:
    """Exact Match 계산"""
    return 1.0 if prediction.strip().lower() == ground_truth.strip().lower() else 0.0

def calculate_f1(prediction: str, ground_truth: str) -> float:
    """F1 Score 계산 (토큰 단위)"""
    pred_tokens = prediction.lower().split()
    truth_tokens = ground_truth.lower().split()

    common = set(pred_tokens) & set(truth_tokens)

    if len(common) == 0:
        return 0.0

    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(truth_tokens)

    return 2 * (precision * recall) / (precision + recall)
```

**평가 기준:**
- **목표**: EM ≥ 0.4 또는 F1 ≥ 0.6

#### 2.1.5 Faithfulness (환각률)

**정의:** 답변이 제공된 컨텍스트에 근거하는 비율 (= 1 - 환각률)

```python
from langchain.evaluation import load_evaluator

def calculate_faithfulness(
    query: str,
    answer: str,
    context: str,
    llm
) -> float:
    """
    Faithfulness 계산 (RAGAS 방식)

    Returns:
        0.0 ~ 1.0 (1.0이 가장 충실함)
    """
    evaluator = load_evaluator("qa", llm=llm)

    result = evaluator.evaluate_strings(
        prediction=answer,
        reference=context,
        input=query
    )

    return result.get("score", 0.0)
```

**평가 기준:**
- **목표**: Faithfulness ≥ 0.9 (환각률 ≤ 10%)

#### 2.1.6 Context Utilization (%)

**정의:** 답변이 실제로 어느 컨텍스트 조각을 참조했는지 매칭 비율

```python
def calculate_context_utilization(
    answer: str,
    context_chunks: List[str]
) -> float:
    """
    답변이 컨텍스트를 얼마나 활용했는지 계산

    Returns:
        0.0 ~ 1.0
    """
    used_chunks = 0

    for chunk in context_chunks:
        # 청크의 주요 키워드가 답변에 포함되어 있는지 확인
        chunk_keywords = set(chunk.lower().split())
        answer_keywords = set(answer.lower().split())

        overlap = len(chunk_keywords & answer_keywords)
        if overlap / len(chunk_keywords) > 0.3:  # 30% 이상 겹치면 사용
            used_chunks += 1

    return used_chunks / len(context_chunks) if context_chunks else 0.0
```

---

### 2.2 효율성 (Online/Latency)

#### 2.2.1 End-to-End 지연 (ms)

**정의:** 검색 + 생성 총 소요시간

```python
import time

def measure_end_to_end_latency(query: str, agent) -> dict:
    """E2E 지연 측정"""
    start_time = time.time()

    response = agent.invoke({"question": query})

    end_time = time.time()
    latency_ms = (end_time - start_time) * 1000

    return {
        "latency_ms": latency_ms,
        "response": response
    }

# 여러 쿼리에 대해 측정
latencies = []
for query in test_queries:
    result = measure_end_to_end_latency(query, agent)
    latencies.append(result["latency_ms"])

# p50, p95 계산
import numpy as np
p50 = np.percentile(latencies, 50)
p95 = np.percentile(latencies, 95)

print(f"p50 지연: {p50:.2f}ms")
print(f"p95 지연: {p95:.2f}ms")
```

**평가 기준:**
- **목표**: p95 지연 ≤ 6000ms (6초)

#### 2.2.2 토큰 비용/호출 비용 (원)

**정의:** 질문당 평균 프롬프트/출력 토큰 수, API 비용 추정

```python
from langchain.callbacks import get_openai_callback

def calculate_cost(queries: List[str], agent) -> dict:
    """토큰 비용 계산"""
    total_tokens = 0
    total_cost = 0.0

    with get_openai_callback() as cb:
        for query in queries:
            agent.invoke({"question": query})

        total_tokens = cb.total_tokens
        total_cost = cb.total_cost

    avg_tokens = total_tokens / len(queries)
    avg_cost = total_cost / len(queries)

    return {
        "total_tokens": total_tokens,
        "avg_tokens_per_query": avg_tokens,
        "total_cost_usd": total_cost,
        "avg_cost_per_query_usd": avg_cost,
        "avg_cost_per_query_krw": avg_cost * 1300  # USD to KRW
    }
```

---

### 2.3 강건성

#### 2.3.1 No-Answer Handling

**정의:** 근거 부재 시 "근거 없음" 응답을 정확히 반환하는 비율

```python
def evaluate_no_answer_handling(
    queries_without_answer: List[str],
    agent
) -> float:
    """No-Answer 처리 정확도"""
    correct_no_answer = 0

    for query in queries_without_answer:
        response = agent.invoke({"question": query})

        # "근거 없음", "답변할 수 없습니다" 등의 키워드 확인
        no_answer_keywords = ["근거 없음", "답변할 수 없", "정보가 없", "찾을 수 없"]

        if any(keyword in response for keyword in no_answer_keywords):
            correct_no_answer += 1

    return correct_no_answer / len(queries_without_answer)
```

#### 2.3.2 길이/잡음 민감도

**정의:** 매우 긴 요약/문서 섞임/중복 컨텐츠에서 성능 저하 정도

**측정 방법:**
- 정상 문서 vs 긴 문서 (10배 길이)에서 Recall@5 비교
- 노이즈 문서 추가 시 성능 저하율 측정

---

### 2.4 권장 툴/프로토콜

#### 2.4.1 데이터셋

**준비:**
- 소규모 GT (Ground Truth) 쿼리-정답-근거 세트 준비
- 규모: 50~200개
- 형식: CSV 또는 JSON

**예시 (CSV):**
```csv
query,ground_truth_answer,ground_truth_docs,difficulty
Transformer 논문의 핵심 아이디어는?,"Self-Attention 메커니즘, Encoder-Decoder 구조, Positional Encoding","doc_1,doc_3",easy
BERT의 pre-training 방법은?,"Masked Language Model (MLM), Next Sentence Prediction (NSP)","doc_5,doc_7",hard
```

#### 2.4.2 도구

**사용 라이브러리:**
- `ragas`: RAG 평가 전용 라이브러리
- `langchain.evaluation`: Langchain 내장 평가 도구
- 수동 판정 템플릿 (Streamlit 내 표출)

**예시:**
```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall

# RAGAS 평가
results = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_recall
    ]
)

print(results)
```

#### 2.4.3 절차

1. **인덱스 고정**: VectorDB 인덱스 빌드 후 변경 금지
2. **쿼리 배치 실행**: 테스트 쿼리 전체 실행
3. **메트릭 산출**: Recall@K, Faithfulness 등 계산
4. **리그레션 추적**: 변경 전후 성능 비교

---

### 2.5 권장 수용 기준 (초기 PoC)

| 메트릭 | 목표 값 |
|--------|---------|
| **Recall@5** | ≥ 0.6 |
| **EM** | ≥ 0.4 |
| **F1** | ≥ 0.6 |
| **Faithfulness** | ≥ 0.9 |
| **p95 지연** | ≤ 6000ms (6초) |

---

## 3. RDB 평가 기준

### 3.1 성능/효율

#### 3.1.1 Query Latency (ms)

**정의:** 주요 쿼리의 p50/p95 지연 시간

**측정 대상:**
- `papers` 테이블 최신 N건 조회
- `term` 상위 N개 조회
- 복잡한 JOIN 쿼리

```python
import psycopg2
import time

def measure_query_latency(query: str, conn, iterations: int = 100) -> dict:
    """쿼리 지연 측정"""
    latencies = []

    cursor = conn.cursor()

    for _ in range(iterations):
        start_time = time.time()
        cursor.execute(query)
        cursor.fetchall()
        end_time = time.time()

        latency_ms = (end_time - start_time) * 1000
        latencies.append(latency_ms)

    cursor.close()

    import numpy as np
    return {
        "p50_ms": np.percentile(latencies, 50),
        "p95_ms": np.percentile(latencies, 95),
        "avg_ms": np.mean(latencies)
    }

# 사용 예시
conn = psycopg2.connect("postgresql://user:password@localhost/papers")

# 최신 10건 조회
query = "SELECT * FROM papers ORDER BY created_at DESC LIMIT 10"
result = measure_query_latency(query, conn)
print(f"p50: {result['p50_ms']:.2f}ms, p95: {result['p95_ms']:.2f}ms")
```

**평가 기준:**
- **목표**: p95 조회 지연 ≤ 50ms

#### 3.1.2 Insert/Upsert Throughput

**정의:** 초당 처리 건수, 배치 삽입 시 평균 소요

```python
def measure_insert_throughput(data: List[dict], conn) -> dict:
    """삽입 처리량 측정"""
    cursor = conn.cursor()

    start_time = time.time()

    for row in data:
        cursor.execute("""
            INSERT INTO papers (title, authors, publish_date, url, abstract)
            VALUES (%s, %s, %s, %s, %s)
            ON CONFLICT (url) DO NOTHING
        """, (row['title'], row['authors'], row['publish_date'], row['url'], row['abstract']))

    conn.commit()
    end_time = time.time()

    duration = end_time - start_time
    throughput = len(data) / duration

    cursor.close()

    return {
        "total_rows": len(data),
        "duration_sec": duration,
        "throughput_per_sec": throughput,
        "avg_latency_ms": (duration / len(data)) * 1000
    }
```

**평가 기준:**
- **목표**: p95 삽입 지연 ≤ 30ms

#### 3.1.3 파일 크기/성장률

**정의:** `papers.db` 크기, 주당 증가량, VACUUM 후 절감률

```bash
# PostgreSQL 데이터베이스 크기 확인
psql -U user -d papers -c "
SELECT
    pg_size_pretty(pg_database_size('papers')) AS database_size;
"

# 테이블별 크기
psql -U user -d papers -c "
SELECT
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
"

# VACUUM 실행
psql -U user -d papers -c "VACUUM FULL ANALYZE;"
```

---

### 3.2 무결성/품질

#### 3.2.1 스키마 일관성

**정의:** NULL/타입 위반 0건, 제약조건 위반 0건

```sql
-- NULL 체크
SELECT COUNT(*) AS null_count
FROM papers
WHERE title IS NULL OR authors IS NULL;

-- 타입 위반 체크 (예: 날짜 형식)
SELECT COUNT(*) AS invalid_dates
FROM papers
WHERE publish_date > CURRENT_DATE;

-- 제약조건 위반 체크
SELECT constraint_name, table_name
FROM information_schema.table_constraints
WHERE constraint_type = 'CHECK';
```

**평가 기준:**
- **목표**: NULL/타입 위반 0건

#### 3.2.2 중복률

**정의:** 동일 URL/ID 중복 삽입 비율 (UPSERT 정책으로 0에 근접)

```sql
-- URL 중복 체크
SELECT url, COUNT(*) AS duplicate_count
FROM papers
GROUP BY url
HAVING COUNT(*) > 1;

-- 중복률 계산
SELECT
    (SELECT COUNT(*) FROM (
        SELECT url FROM papers GROUP BY url HAVING COUNT(*) > 1
    ) AS duplicates) * 100.0 / COUNT(*) AS duplicate_rate_percent
FROM papers;
```

**평가 기준:**
- **목표**: 중복률 ≤ 1%

#### 3.2.3 백업/복구 테스트

**정의:** 주 1회 백업 후 복원 성공률 100%

```bash
# 백업
pg_dump -U user -d papers -F c -f backup_$(date +%Y%m%d).dump

# 복원 (테스트 DB로)
createdb papers_test
pg_restore -U user -d papers_test backup_20251030.dump

# 복원 검증
psql -U user -d papers_test -c "SELECT COUNT(*) FROM papers;"
```

**평가 기준:**
- **목표**: 복구 성공률 100%

---

### 3.3 유지보수성

#### 3.3.1 인덱스 활용도

**정의:** `EXPLAIN QUERY PLAN`으로 풀스캔 회피 확인

```sql
-- 쿼리 실행 계획 확인
EXPLAIN ANALYZE
SELECT * FROM papers WHERE title LIKE '%Transformer%';

-- 인덱스 사용 여부 확인
-- "Index Scan" 또는 "Bitmap Index Scan"이 나오면 OK
-- "Seq Scan"이 나오면 풀스캔 (인덱스 미사용)
```

#### 3.3.2 마이그레이션 용이성

**정의:** 컬럼 추가 시 기존 쿼리 호환성 유지율

```sql
-- 컬럼 추가 (예: tags)
ALTER TABLE papers ADD COLUMN tags TEXT[];

-- 기존 쿼리 호환성 테스트
-- 기존 쿼리가 여전히 동작하는지 확인
SELECT title, authors FROM papers LIMIT 10;
```

---

### 3.4 권장 툴/프로토콜

- **CLI**: `psql`, `pg_dump`, `pg_restore`
- **EXPLAIN QUERY PLAN**: 쿼리 최적화
- **ANALYZE**: 통계 정보 갱신
- **Streamlit 진단 탭** (선택): 최근 레코드 수, 파일 크기, 인덱스 유무 표시

---

### 3.5 권장 수용 기준 (초기 PoC)

| 메트릭 | 목표 값 |
|--------|---------|
| **p95 조회 지연** | ≤ 50ms |
| **p95 삽입 지연** | ≤ 30ms |
| **중복률** | ≤ 1% |
| **복구 성공률** | 100% |

---

## 4. AI Agent 평가 기준

### 4.1 도구 선택 정확도

**정의:** 사용자 질문에 대해 올바른 도구를 선택한 비율

```python
def evaluate_tool_selection(test_cases: List[dict], agent) -> float:
    """
    도구 선택 정확도 평가

    test_cases: [
        {"query": "...", "expected_tool": "search_paper"},
        ...
    ]
    """
    correct = 0

    for case in test_cases:
        # Agent 실행 (도구 선택만 확인)
        result = agent.invoke({"question": case["query"]})

        # 실제 사용된 도구 확인
        used_tool = result.get("tool_used")

        if used_tool == case["expected_tool"]:
            correct += 1

    return correct / len(test_cases)
```

**평가 기준:**
- **목표**: 도구 선택 정확도 ≥ 0.85

### 4.2 라우팅 지연

**정의:** 라우팅 결정에 소요되는 시간

**평가 기준:**
- **목표**: 평균 라우팅 지연 ≤ 500ms

---

## 5. UI/UX 평가 기준

### 5.1 사용자 만족도

**정의:** 사용자 설문 조사 (1~5점)

**평가 항목:**
1. 인터페이스 직관성
2. 응답 속도 만족도
3. 답변 품질 만족도
4. 전체 만족도

**평가 기준:**
- **목표**: 평균 만족도 ≥ 4.0 / 5.0

### 5.2 UI 응답성

**정의:** UI 렌더링 및 상호작용 지연

**평가 기준:**
- **목표**: 버튼 클릭 ~ 응답 시작 ≤ 1초

---

## 6. 통합 평가 시나리오

### 6.1 시나리오 기반 테스트 (10개)

| 번호 | 시나리오 | 기대 도구 | 기대 결과 |
|------|----------|-----------|----------|
| 1 | "Transformer 논문 설명해줘" (Easy) | RAG 검색 | 쉬운 설명 제공 |
| 2 | "BERT와 GPT의 차이는?" (Hard) | RAG 검색 | 기술적 비교 제공 |
| 3 | "Attention Mechanism이 뭐야?" | 용어집 | 용어 정의 제공 |
| 4 | "2025년 최신 LLM 논문은?" | 웹 검색 | 최신 논문 검색 |
| 5 | "Attention Is All You Need 논문 요약해줘" | 논문 요약 | 난이도별 요약 제공 |
| 6 | "이 요약 내용 파일로 저장해줘" | 파일 저장 | 파일 생성 확인 |
| 7 | "Self-Attention이 뭐야?" | 용어집 | 용어 정의 제공 |
| 8 | "Transformer의 장점은?" | RAG 검색 | 관련 정보 제공 |
| 9 | "BERT 논문의 핵심 기여는?" | RAG 검색 | 핵심 내용 제공 |
| 10 | "최신 멀티모달 AI 논문 찾아줘" | 웹 검색 | 최신 논문 검색 |

### 6.2 통합 평가 체크리스트

- [ ] 10개 시나리오 모두 정상 동작
- [ ] 도구 선택 정확도 ≥ 85%
- [ ] 평균 응답 시간 ≤ 6초
- [ ] Faithfulness ≥ 0.9
- [ ] UI 정상 렌더링
- [ ] 오류 없음

---

## 7. 평가 실행 스크립트

### 7.1 RAG 평가 스크립트

```python
# scripts/evaluate_rag.py

from src.utils.logger import Logger
import os
from datetime import datetime

def run_rag_evaluation():
    # 로그 폴더 생성
    today = datetime.now().strftime("%Y%m%d")
    time_now = datetime.now().strftime("%H%M%S")
    experiment_name = "eval_rag_system"
    log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
    os.makedirs(log_dir, exist_ok=True)

    logger = Logger(f"{log_dir}/rag_evaluation.log")

    logger.write("=" * 60)
    logger.write("RAG 시스템 평가 시작")
    logger.write("=" * 60)

    # 테스트 데이터 로드
    test_data = load_test_data("data/test/rag_testset.csv")
    logger.write(f"테스트 데이터: {len(test_data)}건")

    # Recall@K 평가
    recall_3 = evaluate_recall_at_k(test_data, k=3)
    recall_5 = evaluate_recall_at_k(test_data, k=5)
    recall_10 = evaluate_recall_at_k(test_data, k=10)

    logger.write(f"Recall@3: {recall_3:.2f}")
    logger.write(f"Recall@5: {recall_5:.2f}")
    logger.write(f"Recall@10: {recall_10:.2f}")

    # Faithfulness 평가
    faithfulness = evaluate_faithfulness(test_data)
    logger.write(f"Faithfulness: {faithfulness:.2f}")

    # 지연 시간 평가
    latencies = evaluate_latency(test_data)
    logger.write(f"p50 지연: {latencies['p50']:.2f}ms")
    logger.write(f"p95 지연: {latencies['p95']:.2f}ms")

    # 결과 저장
    results = {
        "recall_3": recall_3,
        "recall_5": recall_5,
        "recall_10": recall_10,
        "faithfulness": faithfulness,
        "p50_latency_ms": latencies["p50"],
        "p95_latency_ms": latencies["p95"]
    }

    import json
    with open(f"{log_dir}/results.json", "w") as f:
        json.dump(results, f, indent=2)

    logger.write("=" * 60)
    logger.write("RAG 평가 완료")
    logger.write("=" * 60)
    logger.close()

if __name__ == "__main__":
    run_rag_evaluation()
```

---

## 8. 참고 자료

- RAGAS: https://github.com/explodinggradients/ragas
- Langchain Evaluation: https://python.langchain.com/docs/guides/evaluation/
- PostgreSQL Performance: https://www.postgresql.org/docs/current/performance-tips.html
- Information Retrieval Metrics: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)
