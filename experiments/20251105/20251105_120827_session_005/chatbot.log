2025-11-05 12:08:27 | ì„¸ì…˜ ì‹œì‘: session_005
2025-11-05 12:08:27 | í´ë” ê²½ë¡œ: experiments/20251105/20251105_120827_session_005
2025-11-05 12:08:27 | ì„¤ì • íŒŒì¼ ì €ì¥: db_config.yaml
2025-11-05 12:08:27 | ì„¤ì • íŒŒì¼ ì €ì¥: model_config.yaml
2025-11-05 12:08:27 | ì„¤ì • íŒŒì¼ ì €ì¥: multi_request_patterns.yaml
2025-11-05 12:08:27 | Agent ê·¸ë˜í”„ ìƒì„± ì‹œì‘
2025-11-05 12:08:27 | Fallback Chain í™œì„±í™”
2025-11-05 12:08:27 | ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜: 3
2025-11-05 12:08:27 | Router ê²€ì¦ í™œì„±í™”: True
2025-11-05 12:08:27 | Agent ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ
2025-11-05 12:08:27 | ë¼ìš°í„° ë…¸ë“œ ì‹¤í–‰: Attention ë…¼ë¬¸ ì°¾ì•„ì„œ ìš”ì•½í•´ì¤˜
2025-11-05 12:08:27 | ë‹¤ì¤‘ ìš”ì²­ ê°ì§€: ['ì°¾', 'ìš”ì•½'] â†’ ['search_paper', 'summarize']
2025-11-05 12:08:27 | íŒ¨í„´ ì„¤ëª…: ë…¼ë¬¸ ì°¾ì•„ì„œ ìš”ì•½
2025-11-05 12:08:27 | ìˆœì°¨ ì‹¤í–‰ ë„êµ¬: search_paper â†’ summarize
2025-11-05 12:08:29 | pgvector ê²€ìƒ‰ ê¸°ë¡: search_paper
2025-11-05 12:08:29 | ë‚œì´ë„ë³„ LLM ì„ íƒ: difficulty=easy, provider=solar, model=solar-pro2
2025-11-05 12:08:29 | LLM ì´ˆê¸°í™”: provider=solar, model=solar-pro2
2025-11-05 12:08:29 | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:29 | ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:31 | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:31 | ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:38 | ë„êµ¬ ì‹¤í–‰ ì„±ê³µ: search_paper
2025-11-05 12:08:38 | ë¼ìš°í„° ë…¸ë“œ ì‹¤í–‰: Transformer ë…¼ë¬¸ ê²€ìƒ‰í•´ì„œ ìš”ì•½í•˜ê³  ì €ì¥í•´ì¤˜
2025-11-05 12:08:38 | ë‹¤ì¤‘ ìš”ì²­ ê°ì§€: ['ê²€ìƒ‰', 'ìš”ì•½'] â†’ ['search_paper', 'summarize']
2025-11-05 12:08:38 | íŒ¨í„´ ì„¤ëª…: ë…¼ë¬¸ ê²€ìƒ‰ í›„ ìš”ì•½
2025-11-05 12:08:38 | ìˆœì°¨ ì‹¤í–‰ ë„êµ¬: search_paper â†’ summarize
2025-11-05 12:08:38 | pgvector ê²€ìƒ‰ ê¸°ë¡: search_paper
2025-11-05 12:08:39 | ë‚œì´ë„ë³„ LLM ì„ íƒ: difficulty=easy, provider=solar, model=solar-pro2
2025-11-05 12:08:39 | LLM ì´ˆê¸°í™”: provider=solar, model=solar-pro2
2025-11-05 12:08:39 | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:39 | ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:41 | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:41 | ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:46 | ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ ê°ì§€: search_paper
2025-11-05 12:08:46 | ì‹¤íŒ¨ ì‚¬ìœ : ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹˜: .*ì˜¤ë¥˜.*
2025-11-05 12:08:46 | ============================================================
2025-11-05 12:08:46 | Fallback Router ì‹¤í–‰
2025-11-05 12:08:46 | ì‹¤íŒ¨í•œ ë„êµ¬: search_paper
2025-11-05 12:08:46 | ì‹¤íŒ¨ ì‚¬ìœ : ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹˜: .*ì˜¤ë¥˜.*
2025-11-05 12:08:46 | ì¬ì‹œë„ íšŸìˆ˜: 0/3
2025-11-05 12:08:46 | ëª¨ë“  ë„êµ¬ ì‹œë„ ì™„ë£Œ
2025-11-05 12:08:46 | ìµœì¢… Fallback: general ë„êµ¬ ì„ íƒ
2025-11-05 12:08:46 | ë‹¤ìŒ ë„êµ¬ë¡œ ì „í™˜: general
2025-11-05 12:08:46 | ì „í™˜ ì´ìœ : search_paper ë„êµ¬ê°€ ì‹¤íŒ¨í–ˆê¸° ë•Œë¬¸
2025-11-05 12:08:46 | Fallback Chain:
2025-11-05 12:08:46 | ============================================================
2025-11-05 12:08:46 | ì¼ë°˜ ë‹µë³€ ë…¸ë“œ ì‹¤í–‰: Transformer ë…¼ë¬¸ ê²€ìƒ‰í•´ì„œ ìš”ì•½í•˜ê³  ì €ì¥í•´ì¤˜
2025-11-05 12:08:46 | ë‚œì´ë„: easy
2025-11-05 12:08:46 | ë‚œì´ë„ë³„ LLM ì„ íƒ: difficulty=easy, provider=solar, model=solar-pro2
2025-11-05 12:08:46 | LLM ì´ˆê¸°í™”: provider=solar, model=solar-pro2
2025-11-05 12:08:46 | ìˆ˜ì¤€ 'elementary' ë‹µë³€ ìƒì„± ì‹œì‘
2025-11-05 12:08:46 | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:46 | ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:50 | ìˆ˜ì¤€ 'elementary' ë‹µë³€ ìƒì„± ì™„ë£Œ: 734 ê¸€ì
2025-11-05 12:08:50 | ================================================================================
2025-11-05 12:08:50 | [elementary ë‹µë³€ ì „ì²´ ë‚´ìš©]
2025-11-05 12:08:50 | ğŸŒŸ **íŠ¸ëœìŠ¤í¬ë¨¸ ë…¼ë¬¸ ìš”ì•½ (ì´ˆë“±í•™ìƒ ë²„ì „)** ğŸŒŸ  

ì•ˆë…•! ì¢‹ì€ ì§ˆë¬¸ì´ì•¼! ğŸ‘ íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” **ì–¸ì–´ ëª¨ë¸ì˜ ìŠˆí¼íˆì–´ë¡œ** ê°™ì€ ê±°ì•¼.  

1ï¸âƒ£ **"ì£¼ì˜(Attention)"**ë¼ëŠ” íŠ¹ë³„í•œ ëŠ¥ë ¥ì´ ìˆì–´.  
- ë¬¸ì¥ ì† ë‹¨ì–´ë“¤ì˜ ê´€ê³„ë¥¼ **ë ˆê³  ë¸”ë¡ ì—°ê²°**í•˜ë“¯ ë¶„ì„í•´ìš”.  
- ì˜ˆ: "ê³ ì–‘ì´ê°€ ì¥ë¥¼ ì«“ëŠ”ë‹¤" â†’ "ê³ ì–‘ì´-ì¥"ì— ì§‘ì¤‘! ğŸ±ğŸ€  

2ï¸âƒ£ **ìˆœì°¨ì  ì²˜ë¦¬ ëŒ€ì‹  ë³‘ë ¬ ì²˜ë¦¬**!  
- ì˜›ë‚  ëª¨ë¸(RNN)ì€ ë‹¨ì–´ í•˜ë‚˜ì”© ì²˜ë¦¬í–ˆì§€ë§Œ,  
- íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” **ëª¨ë“  ë‹¨ì–´ë¥¼ ë™ì‹œì—** ì²˜ë¦¬í•´ ë¹¨ë¼ìš”! âš¡  

3ï¸âƒ£ **ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°**  
- ì¸ì½”ë”: ë¬¸ì¥ ì˜ë¯¸ í•´ì„ (ì˜ˆ: "ì‚¬ê³¼ ë¨¹ì–´" â†’ ê³¼ì¼ ë¨¹ëŠ” í–‰ë™)  
- ë””ì½”ë”: í•´ì„ëœ ì •ë³´ë¡œ ìƒˆ ë¬¸ì¥ ìƒì„± (ì˜ˆ: "I eat an apple") ğŸ  

4ï¸âƒ£ **ìœ„ì¹˜ ì •ë³´(Positional Encoding)**  
- ë‹¨ì–´ ìˆœì„œë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•´ **ìˆ¨ê²¨ì§„ ì¢Œí‘œ**ë¥¼ ì¶”ê°€í•´.  
- ë§ˆì¹˜ ì±…ì— í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆëŠ” ê²ƒì²˜ëŸ¼! ğŸ“–  

ğŸ‰ **íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì„±ê³¼**:  
- GPT, BERT ê°™ì€ AI ëª¨ë¸ì˜ ê¸°ë°˜ì´ ëì–´.  
- ë²ˆì—­, ìš”ì•½, ì§ˆë¬¸ ë‹µë³€ ë“± ë‹¤ì–‘í•œ ì¼ì„ í•´ë‚¸ë‹¤êµ¬! ğŸ’¡  

ğŸ“ **ì €ì¥ ë°©ë²• ì¶”ì²œ**:  
1. ë…¸ì…˜(Notion)ì´ë‚˜ ì›ë…¸íŠ¸ì— ìš”ì•½í•´ ì ì–´ë³´ì.  
2. í‚¤ì›Œë“œë§Œ ì ì–´ë‘ê³  ë‚˜ì¤‘ì— ì˜ˆì‹œë¡œ ì±„ì›Œë„ ì¢‹ì•„.  
3. ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë¦¬ë©´ ë” ê¸°ì–µí•˜ê¸° ì‰¬ì›Œ! ğŸ¨  

ì˜ ì´í•´í–ˆì–´? ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ ë˜ ë¬¼ì–´ë´! ğŸ¤—
2025-11-05 12:08:50 | ================================================================================
2025-11-05 12:08:50 | ìˆ˜ì¤€ 'beginner' ë‹µë³€ ìƒì„± ì‹œì‘
2025-11-05 12:08:50 | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:08:50 | ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:09:00 | ìˆ˜ì¤€ 'beginner' ë‹µë³€ ìƒì„± ì™„ë£Œ: 2260 ê¸€ì
2025-11-05 12:09:00 | ================================================================================
2025-11-05 12:09:00 | [beginner ë‹µë³€ ì „ì²´ ë‚´ìš©]
2025-11-05 12:09:00 | ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š Transformer ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ìš”ì•½í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì´ˆë³´ìë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ë“œë¦´ê²Œìš”!

---

### 1. **Transformer ë…¼ë¬¸ ê¸°ë³¸ ì •ë³´**  
ğŸ” **"Attention Is All You Need" (2017)**  
- **ì €ì**: Vaswani et al. (êµ¬ê¸€ ë¸Œë ˆì¸ íŒ€)  
- **í•µì‹¬ ì•„ì´ë””ì–´**: RNN/LSTM ì—†ì´ **Attention ë©”ì»¤ë‹ˆì¦˜**ë§Œìœ¼ë¡œ ë²ˆì—­ ëª¨ë¸ êµ¬ì¶•  
- **í˜ì‹ ì„±**: ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥ â†’ í›ˆë ¨ ì†ë„ í–¥ìƒ, ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°  

> ğŸ“Œ **ìš©ì–´ ì„¤ëª…**:  
> - **Attention**: ì…ë ¥ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ (ì˜ˆ: ë¬¸ì¥ì—ì„œ "ê³ ì–‘ì´"ì— ì§‘ì¤‘í•˜ë ¤ë©´ "ì•¼ì˜¹" ê°™ì€ ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬)  
> - **RNN/LSTM**: ìˆœì°¨ì  ë°ì´í„° ì²˜ë¦¬ì— ì‚¬ìš©ë˜ëŠ” ê¸°ì¡´ ëª¨ë¸ (ë¬¸ì¥ ë‹¨ì–´ë¥¼ **í•˜ë‚˜ì”©** ì²˜ë¦¬)  

---

### 2. **ì£¼ìš” êµ¬ì„± ìš”ì†Œ** (ë¹„ìœ ë¡œ ì„¤ëª…)  
#### (1) **Self-Attention (ìê¸° ì£¼ì˜)**  
- **"ë‹¨ì–´ë“¤ì´ ì„œë¡œë¥¼ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œ"**  
  - ì˜ˆì‹œ: "The cat sat on the mat"ì—ì„œ "cat"ì€ "sat"ê³¼ ê°•í•˜ê²Œ ì—°ê²°, "the"ì™€ëŠ” ì•½í•˜ê²Œ ì—°ê²°  
- **ê³„ì‚° ê³¼ì •**: Q(Query), K(Key), V(Value) ë²¡í„°ë¡œ ìœ ì‚¬ë„ ì¸¡ì •  

#### (2) **Multi-Head Attention (ë‹¤ì¤‘ í—¤ë“œ ì£¼ì˜)**  
- **"ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œ"**  
  - 8~16ê°œì˜ Self-Attention í—¤ë“œë¥¼ ë³‘ë ¬ë¡œ ì‚¬ìš©í•´ ë‹¤ì–‘í•œ ê´€ê³„ í¬ì°©  

#### (3) **Positional Encoding (ìœ„ì¹˜ ì¸ì½”ë”©)**  
- **"ë‹¨ì–´ì˜ ìˆœì„œ ì •ë³´ë¥¼ ì¶”ê°€"**  
  - TransformerëŠ” ë‹¨ì–´ ìˆœì„œë¥¼ ëª¨ë¥´ë¯€ë¡œ, ì‚¬ì¸/ì½”ì‚¬ì¸ í•¨ìˆ˜ë¡œ ìœ„ì¹˜ ì •ë³´ ë¶€ì—¬  
  - (ì˜ˆ: "I eat apple" vs "Apple eat I" êµ¬ë¶„)  

#### (4) **Feed-Forward Network & Residual Connection**  
- **FFN**: ê° ë‹¨ì–´ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ (2ì¸µ ì‹ ê²½ë§)  
- **Residual Connection**: ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œ ë°©ì§€ (ê³ ì¸µ ê±´ë¬¼ì—ì„œ ì—˜ë¦¬ë² ì´í„°ì²˜ëŸ¼ ì •ë³´ ì „ë‹¬)  

---

### 3. **ê°„ë‹¨í•œ ì•„í‚¤í…ì²˜ ë„ì‹í™”**  
```python
# ì˜ì‚¬ì½”ë“œ(Pseudo-code)ë¡œ ë³¸ Transformer ë¸”ë¡
class TransformerBlock:
    def __init__(self):
        self.attention = MultiHeadAttention()
        self.ffn = FeedForwardNetwork()
        self.norm1 = LayerNorm()
        self.norm2 = LayerNorm()

    def forward(self, x):
        # 1. Self-Attention + Residual
        attn_output = self.attention(x)
        x = self.norm1(x + attn_output)  # Residual Connection
        
        # 2. Feed-Forward + Residual
        ffn_output = self.ffn(x)
        x = self.norm2(x + ffn_output)
        return x
```

---

### 4. **í•µì‹¬ ì•„ì´ë””ì–´ 3-5ê°œ ìš”ì•½**  
1. **Attentionë§Œìœ¼ë¡œ RNN/LSTM ëŒ€ì²´** â†’ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥  
2. **Self-Attention**: ë‹¨ì–´ ê°„ ê´€ê³„ë¥¼ ì§ì ‘ ê³„ì‚°í•´ ì¥ê¸° ì˜ì¡´ì„± í•´ê²°  
3. **Positional Encoding**: ìˆœì„œ ì •ë³´ ë³´ì¡´ì„ ìœ„í•œ ìˆ˜í•™ì  íŠ¸ë¦­  
4. **Scaled Dot-Product Attention**: ìœ ì‚¬ë„ ê³„ì‚° ì‹œ ì°¨ì› ì¡°ì •ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ  
5. **BERT/GPTì˜ ê¸°ë°˜**: ì´í›„ ëª¨ë“  ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ í† ëŒ€ê°€ ë¨  

---

### 5. **ë…¼ë¬¸ PDF ì €ì¥ ë°©ë²•**  
- **ê³µì‹ ë§í¬**: [arXiv ë…¼ë¬¸ í˜ì´ì§€](https://arxiv.org/abs/1706.03762)  
- **ì €ì¥ íŒ**:  
  1. í˜ì´ì§€ ìš°ì¸¡ ìƒë‹¨ "Other formats" â†’ PDF ë‹¤ìš´ë¡œë“œ  
  2. êµ¬ê¸€ ìŠ¤ì¹¼ë¼ì—ì„œ ê²€ìƒ‰ í›„ "PDF ë³´ê¸°" í´ë¦­  

ë„ì›€ì´ ë˜ì…¨ë‚˜ìš”? ğŸ˜Š ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”!
2025-11-05 12:09:00 | ================================================================================
2025-11-05 12:09:00 | ë„êµ¬ ì‹¤í–‰ ì„±ê³µ: general (fallback ë„êµ¬)
2025-11-05 12:09:00 | ë¼ìš°í„° ë…¸ë“œ ì‹¤í–‰: Attention ê°œë… ì„¤ëª…í•˜ê³  ê´€ë ¨ ë…¼ë¬¸ ì •ë¦¬í•´ì„œ ì €ì¥í•´ì¤˜
2025-11-05 12:09:00 | ë‹¤ì¤‘ ìš”ì²­ ê°ì§€: ['ì„¤ëª…', 'ë…¼ë¬¸'] â†’ ['glossary', 'search_paper']
2025-11-05 12:09:00 | íŒ¨í„´ ì„¤ëª…: ìš©ì–´ ì„¤ëª… í›„ ê´€ë ¨ ë…¼ë¬¸
2025-11-05 12:09:00 | ìˆœì°¨ ì‹¤í–‰ ë„êµ¬: glossary â†’ search_paper
2025-11-05 12:09:00 | pgvector ê²€ìƒ‰ ê¸°ë¡: glossary
2025-11-05 12:09:00 | ë‚œì´ë„ë³„ LLM ì„ íƒ: difficulty=hard, provider=openai, model=gpt-5
2025-11-05 12:09:00 | LLM ì´ˆê¸°í™”: provider=openai, model=gpt-5
2025-11-05 12:09:01 | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:09:01 | ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:09:49 | ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ ê°ì§€: glossary
2025-11-05 12:09:49 | ì‹¤íŒ¨ ì‚¬ìœ : ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹˜: .*ì˜¤ë¥˜.*
2025-11-05 12:09:49 | ============================================================
2025-11-05 12:09:49 | Fallback Router ì‹¤í–‰
2025-11-05 12:09:49 | ì‹¤íŒ¨í•œ ë„êµ¬: glossary
2025-11-05 12:09:49 | ì‹¤íŒ¨ ì‚¬ìœ : ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹˜: .*ì˜¤ë¥˜.*
2025-11-05 12:09:49 | ì¬ì‹œë„ íšŸìˆ˜: 0/3
2025-11-05 12:09:49 | ëª¨ë“  ë„êµ¬ ì‹œë„ ì™„ë£Œ
2025-11-05 12:09:49 | ìµœì¢… Fallback: general ë„êµ¬ ì„ íƒ
2025-11-05 12:09:49 | ë‹¤ìŒ ë„êµ¬ë¡œ ì „í™˜: general
2025-11-05 12:09:49 | ì „í™˜ ì´ìœ : glossary ë„êµ¬ê°€ ì‹¤íŒ¨í–ˆê¸° ë•Œë¬¸
2025-11-05 12:09:49 | Fallback Chain:
2025-11-05 12:09:49 | ============================================================
2025-11-05 12:09:49 | ì¼ë°˜ ë‹µë³€ ë…¸ë“œ ì‹¤í–‰: Attention ê°œë… ì„¤ëª…í•˜ê³  ê´€ë ¨ ë…¼ë¬¸ ì •ë¦¬í•´ì„œ ì €ì¥í•´ì¤˜
2025-11-05 12:09:49 | ë‚œì´ë„: hard
2025-11-05 12:09:49 | ë‚œì´ë„ë³„ LLM ì„ íƒ: difficulty=hard, provider=openai, model=gpt-5
2025-11-05 12:09:49 | LLM ì´ˆê¸°í™”: provider=openai, model=gpt-5
2025-11-05 12:09:49 | ìˆ˜ì¤€ 'intermediate' ë‹µë³€ ìƒì„± ì‹œì‘
2025-11-05 12:09:49 | ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:09:49 | ìµœì¢… í”„ë¡¬í”„íŠ¸ ì €ì¥ ì™„ë£Œ
2025-11-05 12:11:14 | ìˆ˜ì¤€ 'intermediate' ë‹µë³€ ìƒì„± ì™„ë£Œ: 15005 ê¸€ì
2025-11-05 12:11:14 | ================================================================================
2025-11-05 12:11:14 | [intermediate ë‹µë³€ ì „ì²´ ë‚´ìš©]
2025-11-05 12:11:14 | ë‹¤ìŒì€ Attentionì˜ í•µì‹¬ ê°œë… ì„¤ëª…, ì•Œê³ ë¦¬ì¦˜/ìˆ˜ì‹/ì˜ì‚¬ì½”ë“œ, PyTorch ì˜ˆì œ, ê´€ë ¨ ê¸°ìˆ  ë¹„êµ, ì‹¤ë¬´ ê³ ë ¤ì‚¬í•­, ê·¸ë¦¬ê³  ì£¼ìš” ë…¼ë¬¸ ìš”ì•½ì…ë‹ˆë‹¤. ë§ˆì§€ë§‰ì— ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì œê³µí•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì €ì¥í•´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì €ì¥ ìŠ¤í¬ë¦½íŠ¸ë„ í•¨ê»˜ ë“œë¦½ë‹ˆë‹¤.

1) Attention ê°œë… ìš”ì•½
- ëª©ì : ì…ë ¥ì˜ ê° ìœ„ì¹˜ê°€ ë‹¤ë¥¸ ëª¨ë“  ìœ„ì¹˜(í† í°)ì™€ì˜ ì—°ê´€ë„ë¥¼ ê³„ì‚°í•´ â€œì–´ë””ë¥¼ ë³¼ì§€â€ë¥¼ í•™ìŠµ. ê°€ë³€ ê¸¸ì´ ë¬¸ë§¥ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ë™ì ìœ¼ë¡œ ì§‘ì¤‘.
- ê¸°ë³¸ ë©”ì»¤ë‹ˆì¦˜: Query(Q), Key(K), Value(V)ë¥¼ ì •ì˜í•˜ê³  ì •ë ¬ ì ìˆ˜ s_ij = score(q_i, k_j)ë¥¼ ê³„ì‚° â†’ í™•ë¥ ë¡œ ì •ê·œí™” Î±_ij = softmax_j(s_ij) â†’ ê°€ì¤‘í•© v_jì˜ í•© o_i = Î£_j Î±_ij v_j.
- Scaled Dot-Product Attention:
  Attention(Q, K, V) = softmax((Q K^T) / sqrt(d_k) + mask) V
  - sqrt(d_k)ë¡œ ë‚˜ëˆ ì„œ d_kê°€ ì»¤ì§ˆ ë•Œ ë‚´ì  ë¶„ì‚°ì´ ì»¤ì ¸ softmaxê°€ ê³¼ë„í•˜ê²Œ saturated ë˜ëŠ” ë¬¸ì œë¥¼ ì™„í™”.
  - maskëŠ” íŒ¨ë”© ë§ˆìŠ¤í¬ì™€ causal ë§ˆìŠ¤í¬(ë¯¸ë˜ ì°¨ë‹¨)ì— ì‚¬ìš©.
- Multi-Head: ì„œë¡œ ë‹¤ë¥¸ ì„œë¸Œê³µê°„ì—ì„œ ë³‘ë ¬ë¡œ ì£¼ì˜ì§‘ì¤‘(heads)ì„ ìˆ˜í–‰ í›„ concat â†’ ì„ í˜• ê²°í•©. ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ë™ì‹œì— í¬ì°©.
- Self-Attention vs Cross-Attention:
  - Self: Q=K=V(ê°™ì€ ì‹œí€€ìŠ¤). ë¬¸ë§¥ ë‚´ ìƒí˜¸ ì˜ì¡´ í¬ì°©.
  - Cross: QëŠ” ë””ì½”ë”, K/VëŠ” ì¸ì½”ë” ì¶œë ¥(ì˜ˆ: ë²ˆì—­ ì¸ì½”ë”-ë””ì½”ë”).
- Soft vs Hard / Additive vs Multiplicative:
  - Soft: softmaxë¡œ í™•ë¥  ë¶„í¬(ì—°ì†, ë¯¸ë¶„ ê°€ëŠ¥).
  - Hard: argmax ë“± ë¹„ì—°ì†(í›ˆë ¨ ì–´ë ¤ì›€, REINFORCE/straight-through ë“± í•„ìš”).
  - Additive(Bahdanau): MLPë¡œ score(q, k).
  - Multiplicative(Luong)/Dot-product: q^T k. ì—°ì‚° íš¨ìœ¨ì´ ì¢‹ê³  GPU ì¹œí™”ì . Scaled-ë²„ì „ì´ Transformer í‘œì¤€.

2) ì•Œê³ ë¦¬ì¦˜ ì ˆì°¨(Scaled Dot-Product)
ì…ë ¥: QâˆˆR^{BÃ—NqÃ—d}, KâˆˆR^{BÃ—NkÃ—d}, VâˆˆR^{BÃ—NkÃ—dv}, ë§ˆìŠ¤í¬ Mâˆˆ{0, -âˆ}^{BÃ—NqÃ—Nk}
- S = (Q K^T) / sqrt(d)           # ì ìˆ˜ í–‰ë ¬, shape BÃ—NqÃ—Nk
- if M: S = S + M                 # íŒ¨ë”©/causal ë§ˆìŠ¤í‚¹
- A = softmax(S, dim=-1)          # attention í™•ë¥ 
- O = A V                         # ì¶œë ¥, shape BÃ—NqÃ—dv

ì˜ì‚¬ì½”ë“œ:
function attention(Q, K, V, mask=None):
    S = matmul(Q, K.T) / sqrt(d_k)
    if mask: S = S + mask  # mask entries are -inf for disallowed positions
    A = softmax(S, axis=-1)
    O = matmul(A, V)
    return O, A

3) PyTorch êµ¬í˜„ ì˜ˆì‹œ
3-1) ìˆœìˆ˜ PyTorchë¡œ Scaled Dot-Product ë° Multi-Head

import math
import torch
import torch.nn as nn
import torch.nn.functional as F

def scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0):
    # q: (B, H, Nq, Dh), k: (B, H, Nk, Dh), v: (B, H, Nk, Dhv)
    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))
    if attn_mask is not None:
        # attn_mask shape: (B, 1 or H, Nq, Nk) or (1, 1, Nq, Nk)
        scores = scores + attn_mask
    attn = F.softmax(scores, dim=-1)
    attn = F.dropout(attn, p=dropout_p, training=q.requires_grad)
    out = torch.matmul(attn, v)  # (B, H, Nq, Dhv)
    return out, attn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, d_head=None, dropout=0.0, bias=True):
        super().__init__()
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_head = d_head or (d_model // num_heads)
        self.w_q = nn.Linear(d_model, num_heads * self.d_head, bias=bias)
        self.w_k = nn.Linear(d_model, num_heads * self.d_head, bias=bias)
        self.w_v = nn.Linear(d_model, num_heads * self.d_head, bias=bias)
        self.w_o = nn.Linear(num_heads * self.d_head, d_model, bias=bias)
        self.dropout = dropout

    def forward(self, x_q, x_kv=None, attn_mask=None):
        # x_q: (B, Nq, d_model), x_kv: (B, Nk, d_model) or None -> self-attn
        if x_kv is None:
            x_kv = x_q
        B, Nq, _ = x_q.shape
        Nk = x_kv.size(1)

        q = self.w_q(x_q).view(B, Nq, self.num_heads, self.d_head).transpose(1, 2)  # (B,H,Nq,Dh)
        k = self.w_k(x_kv).view(B, Nk, self.num_heads, self.d_head).transpose(1, 2) # (B,H,Nk,Dh)
        v = self.w_v(x_kv).view(B, Nk, self.num_heads, self.d_head).transpose(1, 2) # (B,H,Nk,Dh)

        if attn_mask is not None:
            # Ensure shape: (B, H, Nq, Nk)
            if attn_mask.dim() == 2:
                # (Nq, Nk)
                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)
            elif attn_mask.dim() == 3:
                # (B, Nq, Nk) -> (B,1,Nq,Nk)
                attn_mask = attn_mask.unsqueeze(1)
            # mask values: 0 for allowed, -inf for masked
            attn_mask = attn_mask.to(q.dtype)

        out, attn = scaled_dot_product_attention(q, k, v, attn_mask, self.dropout)
        out = out.transpose(1, 2).contiguous().view(B, Nq, self.num_heads * self.d_head)
        out = self.w_o(out)
        return out, attn

# ì‚¬ìš© ì˜ˆì‹œ
B, N, d_model, H = 2, 16, 256, 8
x = torch.randn(B, N, d_model)
mha = MultiHeadAttention(d_model, H, dropout=0.1)
# causal mask ë§Œë“¤ê¸° (N x N, ìƒì‚¼ê°ì„ -infë¡œ)
causal = torch.triu(torch.ones(N, N), diagonal=1).bool()
attn_mask = torch.zeros(N, N)
attn_mask[causal] = float("-inf")
y, attn = mha(x, attn_mask=attn_mask)

3-2) PyTorch 2.x ê°€ì† ì»¤ë„(sdp/FlashAttention) ì‚¬ìš©
PyTorch 2.0+ì—ì„œëŠ” F.scaled_dot_product_attentionì´ ìƒí™©ì— ë”°ë¼ FlashAttention ì»¤ë„ê¹Œì§€ ìë™ ì„ íƒí•©ë‹ˆë‹¤.

def mha_fast(x_q, x_kv=None, is_causal=False, attn_mask=None, dropout_p=0.0):
    # x_*: (B, N, d_model)
    B, Nq, d_model = x_q.shape
    H = 8
    Dh = d_model // H
    wq, wk, wv, wo = [nn.Linear(d_model, d_model, bias=False) for _ in range(4)]
    q = wq(x_q).view(B, Nq, H, Dh).transpose(1, 2)  # (B,H,Nq,Dh)
    if x_kv is None:
        x_kv = x_q
    Nk = x_kv.size(1)
    k = wk(x_kv).view(B, Nk, H, Dh).transpose(1, 2)
    v = wv(x_kv).view(B, Nk, H, Dh).transpose(1, 2)
    # F.sdp_attention expects (..., Nq, Dh) shapes
    out = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, dropout_p=dropout_p, is_causal=is_causal)
    out = out.transpose(1, 2).contiguous().view(B, Nq, d_model)
    return wo(out)

ì£¼ì˜:
- attn_maskëŠ” additive mask í˜•íƒœ(í—ˆìš©=0, ê¸ˆì§€=-inf) ë˜ëŠ” bool mask(Pytorch 2.1+) ì‚¬ìš©. dtype/shape ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ í™•ì¸.
- is_causal=Trueë©´ ë³„ë„ causal mask ì—†ì´ ë¯¸ë˜ í† í° ì°¨ë‹¨.

4) ë³µì¡ë„ì™€ ë¡±ì‹œí€€ìŠ¤ ë³€í˜•
- í‘œì¤€ Self-Attention: ì‹œê°„/ë©”ëª¨ë¦¬ O(N^2). ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ë³‘ëª©.
- ëŒ€í‘œì  ê²½ëŸ‰í™” ê¸°ë²•
  - Sparse íŒ¨í„´: Longformer(window+global), BigBird(window+random+global, ì´ë¡ ì  í‘œí˜„ë ¥ ë³´ì¥), Sparse Transformer, Routing Transformer â†’ O(NâˆšN)~O(N log N).
  - ì €ë­í¬/í”„ë¡œì ì…˜: Linformer(K,Vë¥¼ ì €ì°¨ì›ìœ¼ë¡œ íˆ¬ì˜) â†’ O(Nr), râ‰ªN.
  - ì»¤ë„ ê·¼ì‚¬: Performer(FAVOR+), Linear Transformers(Ï†(K)^T Ï†(Q)) â†’ O(Nd).
  - í•´ì‹œ/ë¦¬ì»¤ëŸ°ìŠ¤: Reformer(LSH), Transformer-XL(ì„¸ê·¸ë¨¼íŠ¸ ì¬ì‚¬ìš©+ìƒëŒ€ ìœ„ì¹˜) â†’ íš¨ê³¼ì  ë¬¸ë§¥ í™•ì¥.
  - I/O ìµœì í™”: FlashAttention(ì˜¨ì¹© íƒ€ì¼ë§, ì¬ì‚°ì¶œë¡œ ë©”ëª¨ë¦¬ ìµœì†Œí™”) â†’ ê°™ì€ O(N^2)ì´ì§€ë§Œ ëŒ€í­ ê°€ì†/ì €ë©”ëª¨ë¦¬.
- ìœ„ì¹˜ì •ë³´
  - ì ˆëŒ€ ìœ„ì¹˜: Sinusoidal, Learnable.
  - ìƒëŒ€ ìœ„ì¹˜: Shaw et al. 2018, Transformer-XL.
  - RoPE(íšŒì „ ìœ„ì¹˜ ì„ë² ë”©), ALiBi(ì„ í˜• ë°”ì´ì–´ìŠ¤): ê¸¸ì´ ì¼ë°˜í™”ì™€ ì¶”ë¡  ê¸¸ì´ í™•ì¥ì— ê°•í•¨.

5) ë‹¤ë¥¸ ê¸°ìˆ ê³¼ ë¹„êµ
- RNN+Attention vs Transformer(Self-Attention)
  - ì¥ì : ë³‘ë ¬í™”(íŠ¸ë ˆì´ë‹ ì†ë„), ì¥ê±°ë¦¬ ì˜ì¡´ í¬ì°©ì´ ìš©ì´.
  - ë‹¨ì : O(N^2) ë©”ëª¨ë¦¬/ì‹œê°„(í‘œì¤€). RNNì€ O(N)ì´ì§€ë§Œ ë³‘ë ¬í™”ê°€ ì œí•œì .
- Additive vs Scaled Dot-Product
  - Additive: ì‘ì€ ì°¨ì›ì—ì„œ ì„±ëŠ¥ ìš°ìˆ˜ ë³´ê³ ë„ ìˆìœ¼ë‚˜, ë§¤ ìŠ¤ì½”ì–´ì— MLP â†’ ì—°ì‚°/ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨.
  - Scaled Dot-Product: í–‰ë ¬ê³±ìœ¼ë¡œ ê³ íš¨ìœ¨, í˜„ëŒ€ í‘œì¤€.
- Attention vs Convolution
  - CNN: ì§€ì—­ì„±/ì „ì´í•™ìŠµ ê°•ì , O(N) ë˜ëŠ” O(N log N)ë¡œ íš¨ìœ¨ì .
  - Attention: ì „ì—­ ì»¨í…ìŠ¤íŠ¸, í† í° ê°„ ì„ì˜ ìƒí˜¸ì‘ìš© í‘œí˜„ë ¥ì´ í¼.

6) ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­
- ë§ˆìŠ¤í‚¹
  - íŒ¨ë”© ë§ˆìŠ¤í¬: íŒ¨ë”© í† í° ìŠ¤ì½”ì–´ì— -inf ì¶”ê°€.
  - Causal ë§ˆìŠ¤í¬: ë¯¸ë˜ í† í° ì°¨ë‹¨(ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œ).
- ìˆ˜ì¹˜ ì•ˆì •ì„±
  - scale 1/sqrt(d_k) í•„ìˆ˜. fp16/bf16 í˜¼í•©ì •ë°€ ì‹œ -inf ë§ˆìŠ¤í¬ ì²˜ë¦¬ì™€ softmax ì•ˆì •ì„± ì ê²€.
  - F.scaled_dot_product_attention ì‚¬ìš© ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ ì•ˆì •í™”/ì»¤ë„ ìµœì í™”.
- ì„±ëŠ¥/ë©”ëª¨ë¦¬
  - FlashAttention/SDPA ìš°ì„  ì‚¬ìš©. ì‹œí€€ìŠ¤ ê¸¸ì´â†‘ë©´ gradient checkpointing, ì‹œí€€ìŠ¤ ë³‘í•©(packing), ê¸¸ì´ ë²„í‚·íŒ… ê³ ë ¤.
- í¬ì§€ì…”ë„ ì¸ì½”ë”©
  - RoPE/ALiBi ì±„íƒ ì‹œ ê¸¸ì´ ì¼ë°˜í™”ì™€ extrapolation ì´ì . ìƒëŒ€ìœ„ì¹˜(Transformer-XL)ë¡œ ì¥ê±°ë¦¬ ëª¨ë¸ë§ ê°œì„ .
- êµ¬ì¡°/í•™ìŠµ ì•ˆì •ì„±
  - Pre-LN Transformer(ì…ë ¥ì— LayerNorm í›„ ì„œë¸Œì¸µ) â†’ ë”¥í•œ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì•ˆì •ì .
  - Dropout(íŠ¹íˆ attention weights), label smoothing, warmup ìŠ¤ì¼€ì¤„.
- ë””ì½”ë”© ìµœì í™”
  - KV ìºì‹œë¡œ O(T^2)â†’O(T)ë¡œ ì¶”ë¡  ê°€ì†. ë°°ì¹˜ ë‚´ ê¸¸ì´ ê°€ë³€ ì²˜ë¦¬, beam search ì‹œ ìºì‹œ ê´€ë¦¬.
- ìŠ¤íŒŒìŠ¤/ì„ í˜• ì–´í…ì…˜ ì±„íƒ
  - ë„ë©”ì¸(ë¬¸ì„œ/ìƒë¬¼ì„œì—´/ì‹œê³„ì—´)ê³¼ ê¸¸ì´ ë¶„í¬ë¥¼ ë³´ê³  ì„ íƒ. ê·¼ì‚¬ ê¸°ë²•ì€ ì •í™•ë„-íš¨ìœ¨ íŠ¸ë ˆì´ë“œì˜¤í”„ ì¡´ì¬.

7) í•µì‹¬ ë…¼ë¬¸ ì •ë¦¬(JSON)
ì•„ë˜ JSONì„ íŒŒì¼ë¡œ ì €ì¥í•˜ë©´ íƒìƒ‰/ì°¸ì¡°ì— í¸í•©ë‹ˆë‹¤.

[
  {
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "year": 2015,
    "venue": "ICLR",
    "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"],
    "link": "https://arxiv.org/abs/1409.0473",
    "tags": ["Additive Attention", "NMT", "Encoder-Decoder"],
    "summary": "ì–´í…ì…˜ì„ ì •ë ¬ ëª¨ë¸ë¡œ ê³µì‹í™”í•˜ì—¬ ë²ˆì—­ì—ì„œ RNN ë””ì½”ë”ê°€ ì¸ì½”ë” ì€ë‹‰ ìƒíƒœì— ê°€ì¤‘í•©ìœ¼ë¡œ ì ‘ê·¼í•˜ë„ë¡ í•¨.",
    "key_ideas": ["Additive(score)=v^T tanh(W_q q + W_k k)", "ì†Œí”„íŠ¸ ì •ë ¬ë¡œ ì¥ê±°ë¦¬ ì˜ì¡´ í•´ê²°"]
  },
  {
    "title": "Effective Approaches to Attention-based Neural Machine Translation",
    "year": 2015,
    "venue": "EMNLP",
    "authors": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning"],
    "link": "https://arxiv.org/abs/1508.04025",
    "tags": ["Multiplicative Attention", "NMT"],
    "summary": "Dot/general/concat ë“± ë‹¤ì–‘í•œ ì–´í…ì…˜ ë³€í˜•ì„ ë¹„êµ, multiplicativeì˜ íš¨ìœ¨ ê°•ì¡°.",
    "key_ideas": ["q^T k í˜•íƒœì˜ ì ìˆ˜", "ì…ì¶œë ¥ ì—°ê³„ ë°©ì‹ ë¹„êµ"]
  },
  {
    "title": "Attention Is All You Need",
    "year": 2017,
    "venue": "NeurIPS",
    "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "et al."],
    "link": "https://arxiv.org/abs/1706.03762",
    "tags": ["Transformer", "Scaled Dot-Product", "Multi-Head"],
    "summary": "Self-Attention ê¸°ë°˜ ì¸ì½”ë”-ë””ì½”ë”ë¡œ RNN/CNN ì—†ì´ SOTA ë‹¬ì„±.",
    "key_ideas": ["Scaled Dot-Product", "Multi-Head", "Positional Encoding", "Residual+LayerNorm"]
  },
  {
    "title": "Self-Attention with Relative Position Representations",
    "year": 2018,
    "venue": "NAACL",
    "authors": ["Peter Shaw", "Jakob Uszkoreit", "Ashish Vaswani"],
    "link": "https://arxiv.org/abs/1803.02155",
    "tags": ["Relative Position"],
    "summary": "ìƒëŒ€ì  ìœ„ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ìœ„ì¹˜ ë¶ˆë³€ì„±ê³¼ ì¼ë°˜í™” ê°•í™”.",
    "key_ideas": ["content+position ë¶„í•´", "ìƒëŒ€ ìœ„ì¹˜ ë°”ì´ì–´ìŠ¤"]
  },
  {
    "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
    "year": 2019,
    "venue": "ACL",
    "authors": ["Zihang Dai", "et al."],
    "link": "https://arxiv.org/abs/1901.02860",
    "tags": ["Long Context", "Relative Position", "Recurrence"],
    "summary": "ì„¸ê·¸ë¨¼íŠ¸ ê°„ ë©”ëª¨ë¦¬ ì¬ì‚¬ìš©ê³¼ ìƒëŒ€ ìœ„ì¹˜ë¡œ ê¸¸ì´ í•œê³„ë¥¼ ë„˜ìŒ.",
    "key_ideas": ["ë©”ëª¨ë¦¬ ìºì‹œ ì¬ì‚¬ìš©", "ìƒëŒ€ ìœ„ì¹˜ ìŠ¤í‚´"]
  },
  {
    "title": "Generating Long Sequences with Sparse Transformers",
    "year": 2019,
    "venue": "arXiv",
    "authors": ["Rewon Child", "et al."],
    "link": "https://arxiv.org/abs/1904.10509",
    "tags": ["Sparse Attention"],
    "summary": "Factorized/sparse íŒ¨í„´ìœ¼ë¡œ O(N^2) ë¶€ë‹´ ì™„í™”.",
    "key_ideas": ["Strided+Fixed íŒ¨í„´", "ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ì— ì ìš©"]
  },
  {
    "title": "Reformer: The Efficient Transformer",
    "year": 2020,
    "venue": "ICLR",
    "authors": ["Nikita Kitaev", "Åukasz Kaiser", "Anselm Levskaya"],
    "link": "https://arxiv.org/abs/2001.04451",
    "tags": ["LSH Attention", "Reversible Layers"],
    "summary": "LSHë¡œ ê·¼ì ‘ í† í°ë§Œ ì£¼ì˜, ë©”ëª¨ë¦¬ ì ˆê° ìœ„í•´ reversible network ì‚¬ìš©.",
    "key_ideas": ["O(N log N) ê·¼ì‚¬", "ì—­ì „íŒŒ ë©”ëª¨ë¦¬ ì ˆì•½"]
  },
  {
    "title": "Linformer: Self-Attention with Linear Complexity",
    "year": 2020,
    "venue": "NeurIPS",
    "authors": ["Sinong Wang", "et al."],
    "link": "https://arxiv.org/abs/2006.04768",
    "tags": ["Low-Rank", "Linear-Time"],
    "summary": "K,Vë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì„ í˜• íˆ¬ì˜í•´ ì‹œê°„/ë©”ëª¨ë¦¬ O(N).",
    "key_ideas": ["ì €ë­í¬ ê°€ì •", "í”„ë¡œì ì…˜ í–‰ë ¬ í•™ìŠµ"]
  },
  {
    "title": "Longformer: The Long-Document Transformer",
    "year": 2020,
    "venue": "arXiv",
    "authors": ["Iz Beltagy", "Matthew E. Peters", "Arman Cohan"],
    "link": "https://arxiv.org/abs/2004.05150",
    "tags": ["Sparse Window", "Global Tokens"],
    "summary": "ìŠ¬ë¼ì´ë”© ìœˆë„ìš°+ê¸€ë¡œë²Œ í† í°ìœ¼ë¡œ ê¸´ ë¬¸ì„œ ì²˜ë¦¬.",
    "key_ideas": ["O(N) ê·¼ì‚¬", "ë„ë©”ì¸ ë¬¸ì„œ íƒœìŠ¤í¬ ê°œì„ "]
  },
  {
    "title": "Big Bird: Transformers for Longer Sequences",
    "year": 2020,
    "venue": "NeurIPS",
    "authors": ["Manzil Zaheer", "et al."],
    "link": "https://arxiv.org/abs/2007.14062",
    "tags": ["Sparse Attention", "Theoretical Guarantees"],
    "summary": "ëœë¤+ìœˆë„ìš°+ê¸€ë¡œë²Œë¡œ ì—°ê²°ì„± ë³´ì¥, í‘œí˜„ë ¥/ìœ ë‹ˆë²„ì„¤ ê·¼ì‚¬ ì„±ì§ˆ ì¦ëª….",
    "key_ideas": ["O(N)~O(NâˆšN)", "ê·¸ë˜í”„ ì—°ê²°ì„± ë¶„ì„"]
  },
  {
    "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
    "year": 2020,
    "venue": "ICML",
    "authors": ["Angelos Katharopoulos", "et al."],
    "link": "https://arxiv.org/abs/2006.16236",
    "tags": ["Linear Attention", "Kernel Trick"],
    "summary": "ì»¤ë„ íŠ¹ì§•ì‚¬ìƒìœ¼ë¡œ ì–´í…ì…˜ì„ ì„ í˜•í™”, ì˜¤í† ë¦¬ê·¸ë ˆì‹œë¸Œì—ì„œ O(Nd).",
    "key_ideas": ["Î¦(q)Î¦(k)^T V ëˆ„ì ", "ìºì‹œ íš¨ìœ¨â†‘"]
  },
  {
    "title": "Rethinking Attention with Performers",
    "year": 2021,
    "venue": "ICLR",
    "authors": ["Krzysztof Choromanski", "et al."],
    "link": "https://arxiv.org/abs/2009.14794",
    "tags": ["Kernel Approx", "Random Features"],
    "summary": "FAVOR+ ëœë¤ í”¼ì²˜ë¡œ ì •í™•í•œ ë¹„í¸í–¥ ê·¼ì‚¬ ì œê³µ.",
    "key_ideas": ["ì •í™•ë„-íš¨ìœ¨ ê· í˜•", "ëŒ€ê·œëª¨ ì‹œí€€ìŠ¤ ì²˜ë¦¬"]
  },
  {
    "title": "Roformer: Enhanced Transformer with Rotary Position Embedding",
    "year": 2021,
    "venue": "NeurIPS",
    "authors": ["Jianlin Su", "et al."],
    "link": "https://arxiv.org/abs/2104.09864",
    "tags": ["RoPE", "Positional Encoding"],
    "summary": "ë³µì†Œìˆ˜ íšŒì „ í•´ì„ìœ¼ë¡œ ìƒëŒ€ì  ê°ë„ ë³´ì¡´, ê¸¸ì´ ì™¸ì‚½ ì„±ëŠ¥ ê°œì„ .",
    "key_ideas": ["Q/K ê³µê°„ì— íšŒì „ ì ìš©", "ì£¼íŒŒìˆ˜ë³„ íšŒì „"]
  },
  {
    "title": "Train Short, Test Long: Attention with Linear Biases (ALiBi)",
    "year": 2021,
    "venue": "arXiv",
    "authors": ["Ofir Press", "Noah A. Smith", "Mike Lewis"],
    "link": "https://arxiv.org/abs/2108.12409",
    "tags": ["Positional Bias", "Extrapolation"],
    "summary": "ê±°ë¦¬ ê¸°ë°˜ ì„ í˜• ë°”ì´ì–´ìŠ¤ë§Œìœ¼ë¡œ ê¸´ ê¸¸ì´ ì¼ë°˜í™” ê°•í™”.",
    "key_ideas": ["headë³„ ê¸°ìš¸ê¸°", "ê°„ë‹¨/íš¨ê³¼ì "]
  },
  {
    "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
    "year": 2022,
    "venue": "NeurIPS",
    "authors": ["Tri Dao", "et al."],
    "link": "https://arxiv.org/abs/2205.14135",
    "tags": ["Kernel Fusion", "Memory Optimization"],
    "summary": "ì˜¨ì¹© íƒ€ì¼ë§ê³¼ ì¬ê³„ì‚°ìœ¼ë¡œ ì •í™• ì–´í…ì…˜ì„ ê³ ì†/ì €ë©”ëª¨ë¦¬ë¡œ êµ¬í˜„.",
    "key_ideas": ["ë¸”ë¡-ìŠ¤íŠ¸ë¦¬ë° softmax", "ìºì‹œ/DRAM ì™•ë³µ ìµœì†Œí™”"]
  },
  {
    "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
    "year": 2023,
    "venue": "NeurIPS",
    "authors": ["Tri Dao", "et al."],
    "link": "https://arxiv.org/abs/2307.08691",
    "tags": ["Kernel Optimization"],
    "summary": "ì›Œí¬ ë¶„í• /ë³‘ë ¬ì„± ê°œì„ ìœ¼ë¡œ ì¶”ê°€ ê°€ì†. ëŒ€í˜• ë°°ì¹˜/í—¤ë“œì— íš¨ìœ¨ì .",
    "key_ideas": ["warp-level ìµœì í™”", "ë” ë„“ì€ ì§€ì›"]
  },
  {
    "title": "Decomposable Attention Models for Natural Language Inference",
    "year": 2016,
    "venue": "EMNLP",
    "authors": ["Ankur P. Parikh", "et al."],
    "link": "https://arxiv.org/abs/1606.01933",
    "tags": ["Alignment", "NLI"],
    "summary": "ê°„ë‹¨í•œ ì–´í…ì…˜ ë¶„í•´ë¡œ NLIì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥.",
    "key_ideas": ["Attend-Compare-Aggregate", "ë³‘ë ¬í™” ìš©ì´"]
  },
  {
    "title": "Talking-Heads Attention",
    "year": 2021,
    "venue": "ICML",
    "authors": ["Noam Shazeer", "et al."],
    "link": "https://arxiv.org/abs/2003.02436",
    "tags": ["Head Mixing"],
    "summary": "head ê°„ ìƒí˜¸ì‘ìš©ì„ ìœ„í•œ ì„ í˜• ë³€í™˜ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ.",
    "key_ideas": ["before/after softmax mixing", "í‘œí˜„ë ¥ ì¦ê°€"]
  },
  {
    "title": "On Layer Normalization in the Transformer Architecture",
    "year": 2020,
    "venue": "ICML",
    "authors": ["Ruibin Xiong", "et al."],
    "link": "https://arxiv.org/abs/2002.04745",
    "tags": ["Stability", "Pre-LN"],
    "summary": "Pre-LNì´ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ì•ˆì •ì„±ê³¼ ìˆ˜ë ´ì— ìœ ë¦¬í•¨ì„ ë¶„ì„.",
    "key_ideas": ["ê·¸ë¼ë””ì–¸íŠ¸ íë¦„ ê°œì„ ", "ì‹¤ë¬´ í‘œì¤€í™”"]
  }
]

8) ì €ì¥ ë°©ë²•
- íŒŒì¼ë¡œ ì €ì¥(Python):

import json, io
data = [ ... ìœ„ JSON ë¶™ì—¬ë„£ê¸° ... ]
with io.open("attention_papers.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
print("Saved to attention_papers.json")

- ì‰˜ì—ì„œ ì¦‰ì‹œ ì €ì¥(Unix ê³„ì—´):
cat > attention_papers.json << 'EOF'
[ì—¬ê¸°ì— JSON ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ê¸°]
EOF

- ì¶”ê°€ë¡œ ë³¸ ì„¤ëª…ì„ Markdownìœ¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´:
with open("attention_notes.md", "w", encoding="utf-8") as f:
    f.write("""Attention ê°œë… ìš”ì•½ ... (ì´ ë‹µë³€ì˜ ì„¹ì…˜ 1~6ì„ ì •ë¦¬í•´ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”)""")

ì›í•˜ì‹œë©´ íŠ¹ì • í¬ë§·(BibTeX/CSV/Markdown í‘œ)ë¡œë„ ë³€í™˜í•´ ë“œë¦½ë‹ˆë‹¤. ì €ì¥ íŒŒì¼ëª…ì´ë‚˜ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì‹œë©´ ê·¸ì— ë§ì¶° ì¶œë ¥ í˜•ì‹ì„ ì¡°ì •í•´ ë“œë¦´ê²Œìš”.
2025-11-05 12:11:14 | ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: general
2025-11-05 12:11:14 | ì˜¤ë¥˜ ë‚´ìš©: [Errno 32] Broken pipe
2025-11-05 12:11:14 | ==================================================
2025-11-05 12:11:14 | ì‹¤í—˜ ì¢…ë£Œ
2025-11-05 12:11:14 | ==================================================
2025-11-05 12:11:14 | ë¹ˆ í´ë” ì‚­ì œ: experiments/20251105/20251105_120827_session_005/evaluation
2025-11-05 12:11:14 | ë¹ˆ í´ë” ì‚­ì œ: experiments/20251105/20251105_120827_session_005/ui
2025-11-05 12:11:14 | ë¹ˆ í´ë” ì‚­ì œ: experiments/20251105/20251105_120827_session_005/outputs
2025-11-05 12:11:14 | ì´ 3ê°œì˜ ë¹ˆ í´ë” ì‚­ì œ ì™„ë£Œ
