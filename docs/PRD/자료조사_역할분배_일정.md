# 자료조사: 역할 분배 및 일정 계획

## 문서 정보
- **작성일**: 2025-10-29
- **프로젝트**: 논문 리뷰 챗봇 (AI Agent + RAG)
- **팀명**: 연결의 민족
- **팀 구성**: 4명 (10/31 이후 3명)

---

## 1. 팀원 정보

| 이름 | 역할 | 참여 기간 | 비고 |
|------|------|-----------|------|
| 최현화 | 팀장 | 10/28 ~ 11/6 | 전체 프로젝트 총괄 |
| 박재홍 | 팀원 | 10/28 ~ 10/31 | 단기 참여 (4일) |
| 신준엽 | 팀원 | 10/28 ~ 11/6 | 전체 기간 참여 |
| 임예슬 | 팀원 | 10/28 ~ 11/6 | 전체 기간 참여 |

---

## 2. 프로젝트 일정

### 2.1 전체 타임라인

```
10/24 (목) ────┬──── 팀 회의 (17:00) + Langchain 온라인 강의 학습
               │
10/27 (일) ────┬──── 팀 회의 (13:30) + Langchain 온라인 강의 학습
               │
10/28 (월) ────┼──── 대면회의 OT (11:00) + OT 자료 분석 프로젝트 방향성 논의
               │
10/29 (화) ────┼──── 대면회의 (11:00) + 팀 명, 프로젝트 주제, 협업 방법 결정,
               │     자료조사 후 PRD 작성, 개발 환경 설정
10/30 (수) ────┼──── 핵심 기능 개발 시작 + 멘토링
               │
10/31 (금) ────┼──── 핵심 기능 개발 (3명) 박재홍 마지막 날 + 1차 병합
               │
11/01 (토) ────┼──── 기능 개발 계속
               │
11/02 (일) ────┼──── 기능 개발 계속
               │
11/03 (월) ────┼──── 기능 개발 계속
               │
11/04 (화) ────┼──── 단위 테스트 + 통합 테스트
               │
11/05 (수) ────┼──── 오전 10시까지 개인 개발 완료 -> 최종 병합 및 테스트
               │     발표 자료 준비 시작
11/06 (목) ────┴──── 발표 자료 최종 완료 + 발표 (14:00~19:00)
```

### 2.2 단계별 마일스톤

#### Phase 1: 사전 준비 (10/29~10/30, 2일)

**목표:**
- 개발 환경 설정
- 데이터 수집 및 전처리
- 기초 인프라 구축

**주요 작업:**
1. Git 브랜치 전략 수립
2. PostgreSQL + ChromaDB 설정
3. 논문 데이터 50-100편 수집
4. 디렉토리 구조 최종 확정

**완료 기준:**
- ✅ 모든 팀원 개발 환경 동일하게 설정
- ✅ DB 스키마 생성 완료
- ✅ 테스트용 논문 데이터 최소 50편 적재

---

#### Phase 2: 핵심 기능 개발 (10/30~11/03, 5일)

**목표:**
- 필수 기능 구현
- 모듈별 독립 개발

**주요 작업:**
1. RAG 시스템 구현
2. AI Agent 그래프 구현
3. 도구 개발 (5가지)
4. UI 개발
5. 난이도별 프롬프트

**완료 기준:**
- ✅ 각 모듈 독립 실행 가능
- ✅ 단위 테스트 통과
- ✅ 기본적인 질의응답 동작

---

#### Phase 3: 통합 및 테스트 (11/04~11/05 오전, 1.5일)

**목표:**
- 모듈 통합
- 통합 테스트
- 버그 수정

**주요 작업:**
1. main.py에서 모든 모듈 통합
2. 엔드투엔드 테스트
3. UI-Agent 연동 테스트
4. 10개 시나리오 테스트

**완료 기준:**
- ✅ 전체 시스템 정상 동작
- ✅ 10개 시나리오 모두 통과
- ✅ 치명적 버그 없음

---

#### Phase 4: 발표 준비 (11/05 오후~11/06, 1.5일)

**목표:**
- 발표 자료 작성
- 데모 준비
- README.md 작성

**주요 작업:**
1. 발표 PDF 제작
2. 데모 시나리오 준비
3. README.md 완성
4. 발표 리허설

**완료 기준:**
- ✅ 발표 자료 완성
- ✅ 데모 정상 동작
- ✅ README.md 완성

---

## 3. 역할 분배 전략

### 3.1 분배 원칙

1. **모듈 독립성**: 각 담당 파트가 다른 파트에 의존하지 않도록 설계
2. **박재홍 우선 배정**: 10/31까지만 참여하므로 독립적이고 빠르게 완료 가능한 작업 배정
3. **Feature 브랜치 분리**: 겹치지 않게 브랜치 나누기
4. **임시 데이터 사용**: 실제 데이터 없이도 개발 가능하도록 Mock 데이터 활용

### 3.2 디렉토리 구조 기반 분배

현재 프로젝트 구조:
```
src/
├── agent/          # AI Agent 그래프
├── tools/          # 도구들 (5가지)
├── rag/            # RAG 시스템
├── llm/            # LLM 클라이언트
├── prompts/        # 프롬프트 템플릿
├── memory/         # 대화 히스토리
├── text2sql/       # Text-to-SQL (선택)
├── evaluation/     # 성능 평가 (선택)
└── utils/          # 유틸리티

ui/                 # Streamlit UI
data/               # 데이터
scripts/            # 스크립트
```

---

## 4. 팀원별 담당 역할

### 4.1 최현화 (팀장) - 전체 기간 참여

**담당 모듈:**
1. **AI Agent 그래프 (`src/agent/`)**
   - **LangGraph StateGraph 설계 및 구현**
   - **라우터 노드** (LLM function calling으로 도구 선택)
   - **조건부 엣지** (conditional_edges)
   - **Agent State 관리** (TypedDict 기반 상태)
   - **도구 노드 연결** (RAG, 웹 검색, 용어집, 요약, 파일 저장)

2. **LLM 클라이언트 (`src/llm/`)**
   - **Langchain ChatOpenAI 래퍼 구현**
   - **에러 핸들링 및 재시도 로직** (with_retry)
   - **스트리밍 응답 처리** (astream, streaming_callback)
   - **토큰 사용량 추적** (get_openai_callback)
   - Function calling 설정

3. **대화 메모리 시스템 (`src/memory/`)**
   - **Langchain ConversationBufferMemory 구현**
   - **대화 히스토리 관리** (ChatMessageHistory)
   - **컨텍스트 윈도우 최적화** (ConversationSummaryMemory)
   - **세션 관리** (PostgresChatMessageHistory)

4. **도구: 논문 요약 도구 (`src/tools/summarize.py`)**
   - **Langchain @tool 데코레이터 활용**
   - **load_summarize_chain 구현** (stuff, map_reduce, refine)
   - **길이별 요약** (짧게/길게)
   - **섹션별 요약 기능**

5. **프로젝트 총괄**
   - 기능 통합 및 디버깅
   - main.py 작성 (LangGraph 컴파일 및 실행)
   - 코드 리뷰 및 PR 관리
   - 아키텍처 설계 및 검증

6. **발표 자료 총괄**
   - 발표 PDF 제작
   - README.md 작성

**Feature 브랜치:**
- `feature/agent-graph`
- `feature/llm-client`
- `feature/memory`
- `feature/tool-summarize`
- `feature/integration`

**개발 기간:**
- 10/28~10/29: LLM 클라이언트 (ChatOpenAI) 및 공통 인프라 구축
- 10/30~11/01: LangGraph Agent 그래프 및 메모리 시스템 개발
- 11/02~11/03: 논문 요약 도구 (load_summarize_chain) 개발
- 11/04: 통합 작업 및 테스트
- 11/05~11/06: 최종 병합 및 발표 준비

**Langchain 구현 예시:**
```python
# src/agent/graph.py
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    question: str
    difficulty: str
    messages: Annotated[list, operator.add]
    final_answer: str

def create_agent_graph():
    workflow = StateGraph(AgentState)

    # 노드 정의
    workflow.add_node("router", router_node)
    workflow.add_node("search_paper", search_paper_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("generate_answer", generate_answer_node)

    # 엣지 설정
    workflow.add_conditional_edges(
        "router",
        route_to_tool,
        {
            "search_paper": "search_paper",
            "web_search": "web_search",
            "answer": "generate_answer"
        }
    )

    workflow.set_entry_point("router")
    return workflow.compile()

# src/llm/client.py
from langchain_openai import ChatOpenAI
from langchain.callbacks import get_openai_callback

class LLMClient:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0.7,
            streaming=True
        )

    def invoke_with_tracking(self, messages):
        with get_openai_callback() as cb:
            response = self.llm.invoke(messages)
            print(f"Tokens: {cb.total_tokens}, Cost: ${cb.total_cost}")
        return response
```

---

### 4.2 박재홍 - 10/31까지만 참여 (4일)

**담당 모듈:**
1. **데이터 수집 및 Langchain 문서 처리 (`scripts/` + `src/data/`)**
   - arXiv API로 논문 수집 스크립트
   - **Langchain Document Loader 구현** (PDF → Langchain Document 변환)
   - **Langchain Text Splitter 구현** (RecursiveCharacterTextSplitter)
   - PostgreSQL 메타데이터 저장

2. **임베딩 및 Vector DB 적재 (`src/data/embeddings.py`)**
   - **OpenAI Embeddings를 사용한 임베딩 생성**
   - **Langchain ChromaDB 연동 및 문서 적재**
   - 용어집 데이터 임베딩 및 저장
   - 배치 처리 최적화

3. **데이터베이스 초기 설정**
   - PostgreSQL 스키마 생성
   - ChromaDB 초기화 및 컬렉션 생성
   - 테스트 데이터 로드

**Feature 브랜치:**
- `feature/data-collection`
- `feature/document-processing`
- `feature/database-setup`

**개발 기간:**
- 10/28: 환경 설정 + arXiv API 조사
- 10/29: 데이터 수집 + Langchain Document Loader/Splitter 구현
- 10/30: 임베딩 생성 + ChromaDB 적재 + DB 스키마 생성
- 10/31: 용어집 임베딩 + 최종 확인 및 인수인계

**핵심 작업:**
- 10/31까지 **최소 50-100편 논문 데이터** Langchain 파이프라인으로 처리하여 Vector DB 저장 완료
- Langchain Document Loader, Text Splitter, Embeddings 활용하여 RAG 데이터 준비
- 이후 팀원들이 실제 데이터로 개발 가능하도록 인프라 준비

**Langchain 구현 예시:**
```python
# src/data/document_loader.py
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

class PaperDocumentProcessor:
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.vectorstore = Chroma(
            collection_name="papers",
            embedding_function=self.embeddings
        )

    def process_paper(self, pdf_path: str, metadata: dict):
        # PDF 로드
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # 청크 분할
        chunks = self.text_splitter.split_documents(documents)

        # 메타데이터 추가
        for chunk in chunks:
            chunk.metadata.update(metadata)

        # Vector DB 저장
        self.vectorstore.add_documents(chunks)

        return len(chunks)
```

**인수인계 문서 작성 (10/31):**
- 데이터 수집 방법 문서화
- Langchain Document 처리 파이프라인 설명
- DB 스키마 및 ChromaDB 컬렉션 구조
- 추가 데이터 수집 및 임베딩 방법

---

### 4.3 신준엽 - 전체 기간 참여

**담당 모듈:**
1. **RAG 시스템 (`src/rag/`)**
   - **Langchain Chroma VectorStore 연동**
   - **VectorStoreRetriever 구현** (similarity, mmr)
   - **MultiQueryRetriever 구현** (쿼리 확장)
   - **ContextualCompressionRetriever** (문맥 압축)
   - **임베딩 관리** (OpenAIEmbeddings)
   - **용어집 RAG 통합** (별도 Chroma 컬렉션)

2. **도구: RAG 검색 도구 (`src/tools/rag_search.py`)**
   - **Langchain @tool 데코레이터로 search_paper_database 구현**
   - **Retriever.invoke() 호출**
   - **메타데이터 필터링** (년도, 저자 등)
   - **유사도 점수 반환**

3. **도구: 용어집 도구 (`src/tools/glossary.py`)**
   - **Langchain @tool 데코레이터로 search_glossary 구현**
   - **용어집 전용 VectorStore 검색**
   - **난이도별 설명 반환** (Easy/Hard 메타데이터 필터)

**Feature 브랜치:**
- `feature/rag-system`
- `feature/tool-rag-search`
- `feature/tool-glossary`

**개발 기간:**
- 10/28~10/30: RAG 시스템 기초 구현 (Chroma + Retriever)
- 10/31~11/02: 용어집 통합 및 MultiQueryRetriever 구현
- 11/03~11/04: 성능 최적화 (ContextualCompression) 및 테스트

**Langchain 구현 예시:**
```python
# src/rag/retriever.py
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers import MultiQueryRetriever, ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

class RAGRetriever:
    def __init__(self, llm):
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        self.vectorstore = Chroma(
            collection_name="papers",
            embedding_function=self.embeddings
        )

        # 기본 Retriever
        base_retriever = self.vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 5, "fetch_k": 20}
        )

        # MultiQuery Retriever (쿼리 확장)
        self.retriever = MultiQueryRetriever.from_llm(
            retriever=base_retriever,
            llm=llm
        )

# src/tools/rag_search.py
from langchain.tools import tool
from langchain.schema import Document

@tool
def search_paper_database(query: str, year_filter: int = None) -> list[Document]:
    """논문 데이터베이스에서 관련 논문을 검색합니다."""
    # Retriever 호출
    if year_filter:
        results = retriever.invoke(
            query,
            filter={"year": {"$gte": year_filter}}
        )
    else:
        results = retriever.invoke(query)

    return results
```

---

### 4.4 임예슬 - 전체 기간 참여

**담당 모듈:**
1. **Streamlit UI (`ui/app.py`)**
   - 채팅 인터페이스
   - **LangGraph Agent 스트리밍 연동** (astream_events)
   - **StreamlitCallbackHandler 구현**
   - 난이도 선택 UI
   - 대화 히스토리 표시 (ChatMessageHistory 연동)
   - 파일 다운로드 기능

2. **프롬프트 템플릿 (`src/prompts/`)**
   - **Langchain PromptTemplate 구현**
   - **ChatPromptTemplate으로 Easy/Hard 모드 프롬프트 구성**
   - **FewShotPromptTemplate** (예시 기반 프롬프트)
   - **도구별 프롬프트** (SystemMessage, HumanMessage)

3. **도구: 웹 검색 도구 (`src/tools/web_search.py`)**
   - **Langchain TavilySearchResults 도구 연동**
   - **@tool 데코레이터로 커스텀 웹 검색 래퍼 구현**

4. **도구: 파일 저장 도구 (`src/tools/file_save.py`)**
   - **Langchain @tool 데코레이터로 file_save 구현**
   - 대화 내용 저장
   - 요약 내용 저장

**Feature 브랜치:**
- `feature/streamlit-ui`
- `feature/prompts`
- `feature/tool-web-search`
- `feature/tool-file-save`

**개발 기간:**
- 10/28~10/30: Streamlit UI 기초 개발 + StreamlitCallbackHandler
- 10/31~11/02: Langchain PromptTemplate 개발 (Easy/Hard 모드)
- 11/03~11/04: TavilySearchResults 및 파일 저장 도구 개발

**Langchain 구현 예시:**
```python
# src/prompts/templates.py
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

EASY_MODE_PROMPT = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(
        "당신은 AI/ML 논문을 쉽게 설명하는 전문가입니다. "
        "초심자도 이해할 수 있도록 쉬운 용어로 설명해주세요."
    ),
    HumanMessagePromptTemplate.from_template("{question}")
])

HARD_MODE_PROMPT = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(
        "당신은 AI/ML 논문 전문가입니다. "
        "기술적인 세부사항과 수식을 포함하여 깊이 있게 설명해주세요."
    ),
    HumanMessagePromptTemplate.from_template("{question}")
])

# src/tools/web_search.py
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.tools import tool

# Langchain 기본 도구 사용
tavily_search = TavilySearchResults(max_results=3)

@tool
def search_latest_papers(query: str) -> str:
    """최신 AI/ML 논문을 웹에서 검색합니다."""
    results = tavily_search.invoke({"query": f"{query} AI ML paper 2024 2025"})
    return results

# ui/app.py
from langchain.callbacks.streamlit import StreamlitCallbackHandler

# Agent 실행 with Streamlit 콜백
st_callback = StreamlitCallbackHandler(st.container())
async for event in agent.astream_events({"question": user_input}, callbacks=[st_callback]):
    if event["event"] == "on_chat_model_stream":
        st.write(event["data"]["chunk"].content)
```

---

### 4.5 공통 작업

**모든 팀원:**
1. **유틸리티 (`src/utils/`)**
   - 로거, DB 유틸리티 등
   - 필요 시 각자 추가

2. **테스트 코드**
   - 각자 담당 모듈에 대한 단위 테스트 작성
   - 통합 테스트는 최현화 주도로 진행

---

## 5. Git 브랜치 전략

### 5.1 브랜치 구조

```
main (배포용, 항상 안정 상태)
  │
  ├── develop (통합 브랜치)
  │     │
  │     ├── feature/agent-graph           (최현화) - LangGraph StateGraph
  │     ├── feature/llm-client            (최현화) - ChatOpenAI 래퍼
  │     ├── feature/memory                (최현화) - ConversationBufferMemory
  │     ├── feature/tool-summarize        (최현화) - load_summarize_chain
  │     ├── feature/integration           (최현화) - main.py 통합
  │     ├── feature/data-collection       (박재홍) - arXiv 데이터 수집
  │     ├── feature/document-processing   (박재홍) - Document Loader/Splitter
  │     ├── feature/database-setup        (박재홍) - PostgreSQL + ChromaDB
  │     ├── feature/rag-system            (신준엽) - Chroma + Retriever
  │     ├── feature/tool-rag-search       (신준엽) - @tool RAG 검색
  │     ├── feature/tool-glossary         (신준엽) - @tool 용어집
  │     ├── feature/streamlit-ui          (임예슬) - Streamlit + Callback
  │     ├── feature/prompts               (임예슬) - PromptTemplate
  │     ├── feature/tool-web-search       (임예슬) - TavilySearchResults
  │     └── feature/tool-file-save        (임예슬) - @tool 파일 저장
```

### 5.2 작업 흐름

1. **개발 시작:**
   ```bash
   git checkout develop
   git pull origin develop
   git checkout -b feature/기능명
   ```

2. **개발 중:**
   ```bash
   # 주기적으로 커밋
   git add .
   git commit -m "feat: 기능 설명"
   ```

3. **PR 생성:**
   ```bash
   git push origin feature/기능명
   # GitHub에서 develop으로 PR 생성
   ```

4. **코드 리뷰 후 병합:**
   - 팀장(최현화)이 리뷰 후 승인
   - 충돌 해결 후 병합

5. **최종 배포 (11/05):**
   ```bash
   git checkout main
   git merge develop
   git push origin main
   ```

---

## 6. 일일 스탠드업 미팅

### 6.1 매일 오전 10시 (15분)

**공유 내용:**
1. 어제 한 일
2. 오늘 할 일
3. 블로커 (막힌 부분)

**형식 (Slack/Discord):**
```
[이름]
- 어제: Agent 라우터 노드 구현 완료
- 오늘: 조건부 엣지 구현 예정
- 블로커: PostgreSQL 연결 오류 해결 필요
```

### 6.2 주요 동기화 시점

- **10/30 (수) 멘토링 전**: 현재 진행 상황 공유
- **11/01 (금)**: 박재홍 인수인계 확인, 역할 재분배
- **11/04 (화) 멘토링 전**: 통합 테스트 준비 상황 공유

---

## 7. 리스크 관리

### 7.1 주요 리스크

| 리스크 | 확률 | 영향도 | 대응 방안 |
|--------|------|--------|-----------|
| OpenAI API 비용 초과 | 중 | 중 | - 개발 시 GPT-3.5-turbo 사용<br/>- 최종 테스트만 GPT-4 |
| 모듈 통합 시 충돌 | 높음 | 중 | - 주기적인 동기화<br/>- 인터페이스 사전 합의 |
| 시간 부족 | 높음 | 높음 | - 선택 기능 포기 (Text-to-SQL 등)<br/>- 핵심 기능 우선 완성<br/>- 팀장이 통합 작업 집중 지원 |

### 7.2 백업 플랜

**시간이 부족할 경우 생략 가능한 기능:**
1. ~~Text-to-SQL~~ (선택 기능, 가산점이지만 필수 아님)
2. ~~성능 평가 기능~~ (선택 기능)
3. ~~Reranking~~ (RAG 성능 개선 옵션)

**반드시 완성해야 하는 핵심 기능:**
1. ✅ RAG 논문 검색 (필수)
2. ✅ 웹 검색 (필수)
3. ✅ 파일 저장 (필수)
4. ✅ 용어집 검색
5. ✅ 논문 요약 도구 (최현화 담당으로 우선순위 상향)
6. ✅ 난이도별 답변 (Easy/Hard)
7. ✅ Streamlit UI
8. ✅ AI Agent 라우팅

---

## 8. 커뮤니케이션 도구

### 8.1 도구 선택

- **Slack** 또는 **Discord**: 실시간 소통
- **GitHub Issues**: 버그 및 작업 추적
- **GitHub Projects**: 칸반 보드로 진행 상황 관리
- **Google Docs**: 발표 자료 공동 작성

### 8.2 GitHub Issues 라벨

- `bug`: 버그
- `feature`: 새 기능
- `docs`: 문서
- `test`: 테스트
- `priority-high`: 높은 우선순위
- `priority-low`: 낮은 우선순위
- `blocked`: 블로커 있음

---

## 9. 성공 기준

### 9.1 최소 성공 기준 (Must Have)

- ✅ 사용자가 질문 입력 시 AI Agent가 적절한 도구 선택
- ✅ RAG로 논문 검색 및 답변 생성
- ✅ 웹 검색으로 최신 정보 검색
- ✅ 파일 저장 기능 동작
- ✅ 용어집 자동 설명 기능
- ✅ 논문 요약 도구
- ✅ Easy/Hard 모드 답변 차이 명확
- ✅ 10개 시나리오 모두 통과
- ✅ Streamlit UI 정상 동작
- ✅ 발표 자료 완성

### 9.2 목표 성공 기준 (Should Have)

- ⭐ 50개 이상 논문 데이터 확보
- ⭐ UI 디자인 완성도
- ⭐ 대화 히스토리 관리 기능
- ⭐ 스트리밍 응답

### 9.3 최상 성공 기준 (Nice to Have)

- 🌟 Text-to-SQL 기능
- 🌟 성능 평가 기능
- 🌟 100개 이상 논문 데이터
- 🌟 Reranking 구현

---

## 10. 발표 준비

### 10.1 발표 자료 구성 (PDF)

1. **프로젝트 소개** (2분)
   - 배경: 왜 논문 리뷰 챗봇을 만들었는가?
   - 목표 사용자: 초심자 + 전문가

2. **시스템 아키텍처** (3분)
   - AI Agent + RAG 구조 설명
   - Mermaid 다이어그램 활용

3. **핵심 기능 시연** (5분)
   - 시나리오 1: Easy 모드로 Transformer 논문 질문
   - 시나리오 2: Hard 모드로 기술적 질문
   - 시나리오 3: 최신 논문 웹 검색
   - 시나리오 4: 용어 질문
   - 시나리오 5: 파일 저장

4. **기술 스택 및 도구** (2분)
   - OpenAI GPT-4
   - Langchain + LangGraph
   - PostgreSQL + ChromaDB
   - Streamlit

5. **어려웠던 점 및 해결 방법** (2분)
   - 짧은 기간
   - 데이터 수집
   - 모듈 통합

6. **향후 개선 방향** (1분)
   - 더 많은 논문 데이터
   - 성능 최적화
   - 추가 기능

### 10.2 시나리오 10개 (발표 자료에 포함)

1. "Transformer 논문 설명해줘" (Easy 모드)
2. "BERT와 GPT의 차이는?" (Hard 모드)
3. "Attention Mechanism이 뭐야?" (용어집)
4. "2025년 최신 LLM 논문은?" (웹 검색)
5. "Attention Is All You Need 논문 요약해줘" (논문 요약)
6. "이 요약 내용 파일로 저장해줘" (파일 저장)
7. "Self-Attention이 뭐야?" (용어집)
8. "Transformer의 장점은?" (RAG 검색)
9. "BERT 논문의 핵심 기여는?" (RAG 검색)
10. "최신 멀티모달 AI 논문 찾아줘" (웹 검색)

---

## 11. README.md 작성 가이드

### 11.1 필수 포함 내용

```markdown
# 논문 리뷰 챗봇 (AI Agent + RAG)

## 프로젝트 소개
AI/ML 논문을 쉽게 이해할 수 있도록 돕는 챗봇입니다.

## 주요 기능
- RAG 기반 논문 검색
- 웹 검색 (최신 논문)
- 용어집 자동 설명
- 난이도별 답변 (Easy/Hard)
- 파일 저장

## 기술 스택
- OpenAI GPT-4
- Langchain + LangGraph
- PostgreSQL + ChromaDB
- Streamlit

## 설치 및 실행
\`\`\`bash
pip install -r requirements.txt
streamlit run ui/app.py
\`\`\`

## 데이터베이스 스키마
(테이블 설명)

## 팀원
- 최현화 (팀장)
- 박재홍
- 신준엽
- 임예슬

## 라이선스
MIT License
```

---

## 13. 참고 자료

- LangGraph 공식 문서: https://langchain-ai.github.io/langgraph/
- Langchain 공식 문서: https://python.langchain.com/docs/
- Streamlit 공식 문서: https://docs.streamlit.io/
- arXiv API: https://info.arxiv.org/help/api/index.html
