# 11. 데이터베이스 설계

## 문서 정보

- **작성일**: 2025-10-30
- **프로젝트명**: 논문 리뷰 챗봇 (AI Agent + RAG)
- **팀명**: 연결의 민족

---

## 1. 데이터베이스 아키텍처

### 1.1 DB 스키마 구조도

```mermaid
graph TB
    subgraph Papers["🔸 논문 관리"]
        direction TB
        P[papers 테이블<br/>paper_id, arxiv_id, title<br/>authors, publish_date, abstract]
        PC[paper_chunks<br/>Langchain PGVector 컬렉션<br/>chunk_text, embedding, paper_id]
    end

    subgraph Glossary["🔹 용어집"]
        direction TB
        G[glossary 테이블<br/>term_id, term<br/>easy_explanation<br/>hard_explanation]
    end

    Papers -.->|메타데이터| P
    Glossary -.->|정의 조회| G
    Papers -.->|벡터 검색| PC

    %% Subgraph 스타일
    style Papers fill:#e1f5ff,stroke:#01579b,stroke-width:3px,color:#000
    style Glossary fill:#f3e5f5,stroke:#4a148c,stroke-width:3px,color:#000

    %% 노드 스타일
    style P fill:#90caf9,stroke:#1976d2,color:#000
    style PC fill:#64b5f6,stroke:#1976d2,color:#000
    style G fill:#ce93d8,stroke:#7b1fa2,color:#000
```

**DB 스키마 구조도 설명:**
- 데이터베이스를 2개의 주요 영역(논문 관리, 용어집)으로 구분하여 각 테이블의 역할과 관계를 표현
- 논문 관리 영역에서는 papers 테이블이 논문 메타데이터를 저장하고, paper_chunks 컬렉션이 청크 분할된 텍스트와 임베딩을 pgvector로 저장하여 벡터 검색을 지원
- 용어집 영역에서는 glossary 테이블이 용어 정의와 난이도별 설명을 저장

### 1.2 쿼리 흐름

```mermaid
sequenceDiagram
    autonumber
    participant App as 애플리케이션
    participant PG as PostgreSQL
    participant VDB as pgvector

    App->>App: 사용자 질문 입력
    App->>VDB: 임베딩 벡터로 변환
    VDB->>VDB: 유사도 검색 (Cosine)<br/>Top-K 문서 추출
    VDB-->>App: Document 리스트 반환<br/>(paper_id 메타데이터 포함)

    App->>PG: 메타데이터 조회<br/>FROM papers WHERE paper_id IN (...)
    PG-->>App: title, authors, publish_date, category

    App->>App: 컨텍스트 구성<br/>LLM에 전달
```

**쿼리 흐름 설명:**
- 사용자 질문이 입력되면 애플리케이션, PostgreSQL, pgvector 간의 상호작용을 통해 관련 논문을 검색하는 전체 프로세스를 순차적으로 표현
- 질문을 임베딩 벡터로 변환한 후 pgvector에서 Cosine 유사도 기반으로 Top-K 문서를 추출
- 검색된 Document의 메타데이터에서 paper_id를 추출하고, 해당 paper_id로 papers 테이블에서 제목, 저자, 년도 등의 메타데이터를 가져옴
- 최종적으로 검색된 문서 내용과 메타데이터를 결합하여 컨텍스트를 구성하고 LLM에 전달하여 답변 생성

### 1.3 전체 구조

**선택:** PostgreSQL + pgvector 통합 솔루션

**이유:**
- 관계형 데이터와 벡터 검색을 하나의 DB에서 처리
- 운영 및 유지보수 간소화
- Langchain과 완벽한 통합

---

## 2. PostgreSQL 스키마

### 2.1 papers 테이블 (논문 메타데이터)

```sql
CREATE TABLE IF NOT EXISTS papers (
    paper_id SERIAL PRIMARY KEY,
    arxiv_id VARCHAR(64),                 -- arXiv ID (예: 1706.03762)
    title TEXT NOT NULL,
    authors TEXT,                          -- 쉼표로 구분된 저자 목록
    publish_date DATE,
    source VARCHAR(32),                    -- 'arxiv', 'IEEE', 'ACL' 등
    url TEXT UNIQUE,                       -- 논문 URL (중복 방지)
    category TEXT,                         -- 'cs.AI', 'cs.CL', 'cs.CV' 등
    citation_count INT,                    -- 인용 횟수
    abstract TEXT,                         -- 논문 초록
    created_at TIMESTAMP DEFAULT NOW()
);

-- 인덱스 생성
CREATE INDEX IF NOT EXISTS idx_papers_title ON papers USING GIN (to_tsvector('simple', title));
CREATE INDEX IF NOT EXISTS idx_papers_category ON papers(category);
CREATE INDEX IF NOT EXISTS idx_papers_date ON papers(publish_date);
```

**주요 컬럼 설명:**
- `paper_id`: 논문 고유 ID (자동 증가)
- `arxiv_id`: arXiv 논문 식별자
- `title`: 논문 제목
- `authors`: 저자 목록 (쉼표로 구분)
- `publish_date`: 출판일
- `source`: 출처 (기본값: 'arxiv')
- `url`: 논문 URL (UNIQUE 제약)
- `category`: 논문 카테고리
- `abstract`: 논문 초록

### 2.2 glossary 테이블 (용어집)

```sql
CREATE TABLE IF NOT EXISTS glossary (
    term_id SERIAL PRIMARY KEY,
    term VARCHAR(200) NOT NULL UNIQUE,     -- 용어
    definition TEXT NOT NULL,              -- 기본 정의
    easy_explanation TEXT,                 -- Easy 모드 설명
    hard_explanation TEXT,                 -- Hard 모드 설명
    category TEXT,                         -- 'ML', 'NLP', 'CV', 'RL' 등
    difficulty_level INT,                  -- 난이도 레벨 (1: 초급, 2: 중급, 3: 고급)
    related_terms TEXT,                    -- 관련 용어
    examples TEXT,                         -- 사용 예시
    created_at TIMESTAMP DEFAULT NOW()
);

-- 인덱스
CREATE INDEX IF NOT EXISTS idx_glossary_term ON glossary(term);
CREATE INDEX IF NOT EXISTS idx_glossary_category ON glossary(category);
```

**주요 컬럼 설명:**
- `term_id`: 용어 고유 ID
- `term`: 용어명 (UNIQUE)
- `definition`: 기본 정의
- `easy_explanation`: 초심자용 쉬운 설명
- `hard_explanation`: 전문가용 상세 설명
- `difficulty_level`: 난이도 (1=초급, 2=중급, 3=고급)
- `related_terms`: 관련 용어 (텍스트 형식)
- `examples`: 사용 예시

---

## 3. pgvector 컬렉션

### 3.1 컬렉션 구조

**1개의 pgvector 컬렉션:**

1. **paper_chunks**: 논문 본문 청크 (Langchain PGVector)

### 3.2 Langchain PGVector 설정

**파일 위치:** `src/data/embeddings.py`

```python
from langchain_postgres.vectorstores import PGVector
from langchain_openai import OpenAIEmbeddings
import os
from pathlib import Path
from dotenv import load_dotenv

# 환경 변수 로드
load_dotenv(Path(__file__).resolve().parents[2] / ".env")

# 임베딩 모델
embeddings = OpenAIEmbeddings(
    model=os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small"),
    api_key=os.getenv("OPENAI_API_KEY"),
)

# 연결 문자열 (DATABASE_URL 환경 변수 사용)
CONNECTION_STRING = os.getenv("DATABASE_URL")
if not CONNECTION_STRING:
    raise RuntimeError("DATABASE_URL이 설정되지 않았습니다.")

# 논문 본문 청크 컬렉션
paper_chunks_store = PGVector(
    collection_name="paper_chunks",
    connection=CONNECTION_STRING,
    embeddings=embeddings,
)
```

**메타데이터 포함:**
- Document 메타데이터에 `paper_id` 포함 (papers 테이블과 조인용)
- `source`: 논문 출처
- `arxiv_id`: arXiv ID
- `chunk_id`: 청크 식별자

---

## 4. 데이터 흐름

### 4.1 논문 데이터 저장 흐름

```
1. arXiv API로 논문 메타데이터 수집
   → scripts/collect_arxiv_papers.py

2. PostgreSQL papers 테이블에 메타데이터 저장
   → scripts/setup_database.py의 insert_paper_metadata()
   → paper_id 반환 및 arxiv_id → paper_id 매핑 생성

3. PDF 다운로드 및 텍스트 추출
   → src/data/document_loader.py의 load_pdf()

4. RecursiveCharacterTextSplitter로 청크 분할
   → src/data/document_loader.py의 load_and_split()
   → chunk_size=1000, chunk_overlap=200

5. OpenAI Embeddings로 임베딩 생성
   → src/data/embeddings.py의 PaperEmbeddingManager

6. pgvector paper_chunks 컬렉션에 저장
   → add_documents() 또는 add_documents_with_paper_id()
   → 배치 처리 (batch_size=50)
   → paper_id 메타데이터 포함
```

### 4.2 검색 흐름

```
1. 사용자 질문 입력

2. OpenAI Embeddings로 질문 임베딩 생성

3. pgvector similarity_search로 관련 청크 검색 (Top-K)
   → Document 리스트 반환 (paper_id 메타데이터 포함)

4. paper_id 리스트 추출

5. PostgreSQL papers 테이블에서 메타데이터 조회
   SELECT paper_id, title, authors, publish_date, category
   FROM papers
   WHERE paper_id IN (...)

6. 검색된 청크 + 메타데이터 결합하여 컨텍스트 구성

7. LLM에 컨텍스트 전달하여 답변 생성
```

---

## 5. 샘플 데이터

### 5.1 papers 테이블

```sql
INSERT INTO papers (arxiv_id, title, authors, publish_date, source, url, category, abstract)
VALUES (
    '1706.03762',
    'Attention Is All You Need',
    'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin',
    '2017-06-12',
    'arxiv',
    'https://arxiv.org/abs/1706.03762',
    'cs.CL',
    'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks...'
);
```

### 5.2 glossary 테이블

```sql
INSERT INTO glossary (term, definition, easy_explanation, hard_explanation, category, difficulty_level)
VALUES (
    'Attention Mechanism',
    'A technique that allows models to focus on specific parts of the input when generating output.',
    '책을 읽을 때 중요한 부분에 집중하는 것처럼, AI가 입력 데이터에서 중요한 부분에 집중하는 기술입니다.',
    'A weighted sum mechanism that computes attention scores between query and key vectors using dot-product similarity, allowing the model to dynamically focus on relevant input positions during sequence processing.',
    'Deep Learning',
    2
);
```

---

## 6. 초기화 스크립트

### 6.1 scripts/setup_database.py

**실행 방법:**

```bash
# 환경 변수 설정
export DATABASE_URL="postgresql://user:password@localhost:5432/papers"
export OPENAI_API_KEY="your-openai-api-key"

# 스크립트 실행
python scripts/setup_database.py
```

**주요 기능:**
1. pgvector 확장 활성화
2. papers, glossary 테이블 생성
3. 인덱스 생성
4. 초기 용어집 데이터 삽입 (`insert_glossary_data()`)
5. JSON 메타데이터를 papers 테이블에 삽입 (`insert_paper_metadata()`)
6. arxiv_id → paper_id 매핑 파일 생성 (`save_paper_id_mapping()`)

**스크립트 구조:**
- `ensure_pgvector()`: pgvector 확장 설치
- `insert_glossary_data()`: 초기 용어집 데이터 삽입
- `insert_paper_metadata()`: `data/raw/arxiv_papers_metadata.json` 파일에서 논문 메타데이터 삽입
- `save_paper_id_mapping()`: `data/processed/paper_id_mapping.json` 파일 생성

---

## 7. 백업 및 복구

### 7.1 백업

```bash
# 전체 데이터베이스 백업
pg_dump -U your_username -d papers -F c -f backup_$(date +%Y%m%d).dump

# 특정 테이블만 백업
pg_dump -U your_username -d papers -t papers -F c -f papers_backup.dump
pg_dump -U your_username -d papers -t glossary -F c -f glossary_backup.dump

# pgvector 컬렉션 포함 백업 (langchain_postgres의 내부 테이블)
pg_dump -U your_username -d papers -F c -f full_backup_$(date +%Y%m%d).dump
```

### 7.2 복구

```bash
# 데이터베이스 복원
createdb papers_restored
pg_restore -U your_username -d papers_restored backup_20251030.dump
```

---

## 8. 데이터 파일 위치

### 8.1 로컬 파일 구조

```
data/
├── raw/
│   ├── pdfs/                           # 다운로드한 PDF 파일
│   └── arxiv_papers_metadata.json     # 논문 메타데이터 JSON
└── processed/
    └── paper_id_mapping.json           # arxiv_id → paper_id 매핑
```

### 8.2 데이터베이스 연결 정보

**환경 변수 (.env 파일):**
```bash
DATABASE_URL=postgresql://user:password@localhost:5432/papers
OPENAI_API_KEY=sk-...
OPENAI_EMBEDDING_MODEL=text-embedding-3-small  # 선택사항
```

**연결 문자열 형식:**
- `postgresql://user:password@host:port/database`
- 또는 `postgresql+psycopg2://user:password@host:port/database` (SQLAlchemy용)

---

## 9. 참고 자료

- PostgreSQL 공식 문서: https://www.postgresql.org/docs/
- pgvector GitHub: https://github.com/pgvector/pgvector
- Langchain PGVector: https://python.langchain.com/docs/integrations/vectorstores/pgvector
- 프로젝트 구현 코드:
  - `scripts/setup_database.py` - 데이터베이스 초기화 스크립트
  - `src/data/embeddings.py` - PGVector 임베딩 관리
  - `src/data/document_loader.py` - PDF 문서 로더

---

## 10. 주의사항

1. **pgvector 확장 설치 필요**
   - PostgreSQL에 pgvector 확장이 설치되어 있어야 함
   - `CREATE EXTENSION vector;` 실행 필요 (관리자 권한)

2. **DATABASE_URL 환경 변수 필수**
   - 모든 스크립트는 `.env` 파일 또는 환경 변수에서 `DATABASE_URL` 읽음
   - 연결 실패 시 명확한 오류 메시지 제공

3. **중복 데이터 방지**
   - `papers` 테이블의 `url` 컬럼에 UNIQUE 제약 조건
   - `ON CONFLICT (url) DO NOTHING` 사용하여 중복 삽입 방지

4. **배치 처리**
   - OpenAI Embeddings API Rate Limit 대응
   - `batch_size=50`으로 배치 처리
   - 배치 간 대기 시간 적용

5. **메타데이터 매핑**
   - `paper_id_mapping.json` 파일은 `data/processed/` 디렉토리에 저장
   - arxiv_id → paper_id 매핑으로 문서와 논문 메타데이터 연결
