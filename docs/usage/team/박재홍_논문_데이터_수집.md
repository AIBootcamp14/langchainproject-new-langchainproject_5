# 박재홍 - 논문 데이터 수집 파이프라인 실행 가이드

**담당자**: 박재홍
**작업 범위**: arXiv 논문 수집, Document 처리, 임베딩 생성 및 PostgreSQL + pgvector 적재
**문서 작성일**: 2025-11-03

---

## 실행 환경 준비

### 1. 가상환경 활성화

```bash
# pyenv 가상환경 활성화
source ~/.pyenv/versions/langchain_py3_11_9/bin/activate
```

### 2. 환경 변수 확인

`.env` 파일에 다음 환경 변수가 설정되어 있는지 확인:

```bash
# PostgreSQL 설정
POSTGRES_USER=langchain
POSTGRES_PASSWORD=dusrufdmlalswhr
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=papers

# OpenAI API 키
OPENAI_API_KEY=sk-...
```

### 3. PostgreSQL 서버 실행 확인

```bash
# PostgreSQL 서비스 상태 확인
sudo systemctl status postgresql

# 실행 중이 아니면 시작
sudo systemctl start postgresql
```

---

## 실행 우선순위 및 명령어

### 우선순위 1: arXiv 논문 수집

#### 실행 명령어
```bash
python scripts/data/collect_arxiv_papers.py
```

#### 설명
- arXiv API를 사용하여 AI/ML 관련 논문 수집
- PDF 파일 다운로드 및 메타데이터 추출
- 7개 키워드로 각 15편씩 수집 (키워드당 중복 제거)

#### 수집 키워드
1. transformer attention
2. BERT GPT
3. large language model
4. retrieval augmented generation
5. neural machine translation
6. question answering
7. AI agent

#### 필요 조건
- 인터넷 연결
- `.env` 파일 설정

#### 생성 결과물
- `data/raw/pdfs/`: PDF 파일들 (arXiv ID 형식: `2412.09925v2.pdf`)
- `data/raw/arxiv_papers_metadata.json`: 논문 메타데이터 JSON
- `experiments/[날짜]/[시간]_session_[번호]/chatbot.log`: 수집 로그

#### 예상 소요 시간
- 약 20-30분 (논문 수에 따라 다름)
- 이미 다운로드된 PDF는 건너뜀

#### 실행 결과 예시
```
======================================================================
arXiv 논문 수집 시작
======================================================================
2025-11-03 00:54:25 | 세션 시작: session_004
2025-11-03 00:54:25 | 키워드 수집 시작: transformer attention
2025-11-03 00:54:25 | 검색 시작: query=transformer attention, max_results=15
2025-11-03 00:54:26 | 다운로드 완료: Simulating Hard Attention Using Soft Attention (2412.09925v2)
...
2025-11-03 00:54:27 | 총 105편의 논문 수집 완료
✅ arXiv 논문 수집 완료
```

---

### 우선순위 2: PostgreSQL 데이터베이스 초기화

#### 실행 명령어
```bash
python scripts/data/setup_database.py
```

#### 설명
- PostgreSQL에 `papers`, `glossary` 테이블 생성
- pgvector 확장 활성화
- 논문 메타데이터를 `papers` 테이블에 INSERT
- arXiv ID → paper_id 매핑 파일 생성

#### 필요 조건
- **우선순위 1 완료 필수** (논문 수집 완료)
- PostgreSQL 서버 실행 중
- `.env` 파일에 DB 접속 정보 설정

#### 생성 결과물
- PostgreSQL `papers` 테이블: 논문 메타데이터
- PostgreSQL `glossary` 테이블: 용어집 (초기 데이터 포함)
- `data/processed/paper_id_mapping.json`: arXiv ID → paper_id 매핑

#### 예상 소요 시간
- 약 1-2분

#### 실행 결과 예시
```
Schema created and committed.
Glossary seed data inserted (3 terms).
Inserted 105 papers into database.
Data insertion completed.
Saved arxiv_id→paper_id mapping: /home/ieyeppo/AI_Lab/langchain-project/data/processed/paper_id_mapping.json (105개)
Database setup completed (tables, indexes, glossary seed, id mapping).
```

#### 생성 테이블 스키마

**papers 테이블:**
- `paper_id`: SERIAL PRIMARY KEY
- `arxiv_id`: VARCHAR(64)
- `title`: TEXT NOT NULL
- `authors`: TEXT
- `publish_date`: DATE
- `source`: VARCHAR(32)
- `url`: TEXT UNIQUE
- `category`: TEXT
- `citation_count`: INT
- `abstract`: TEXT
- `created_at`: TIMESTAMP
- `updated_at`: TIMESTAMP

**glossary 테이블:**
- `term_id`: SERIAL PRIMARY KEY
- `term`: TEXT UNIQUE
- `definition`: TEXT
- `easy_explanation`: TEXT
- `hard_explanation`: TEXT
- `category`: TEXT
- `difficulty_level`: INT
- `related_terms`: TEXT
- `examples`: TEXT
- `created_at`: TIMESTAMP
- `updated_at`: TIMESTAMP

---

### 우선순위 3: PDF 문서 로드 및 청크 분할

#### 실행 명령어
```bash
python scripts/data/process_documents.py
```

#### 설명
- PDF 파일을 읽고 텍스트 추출
- RecursiveCharacterTextSplitter로 청크 분할
- 각 청크에 메타데이터 추가 (arxiv_id, chunk_id, 제목 등)

#### 필요 조건
- **우선순위 1 완료 필수** (PDF 파일 존재)
- **우선순위 2 완료 권장** (메타데이터 매핑)

#### 청크 분할 설정
- `chunk_size`: 1000자
- `chunk_overlap`: 200자
- 구분자: `\n\n`, `\n`, `. `, ` `, ``

#### 생성 결과물
- Document 객체 리스트 (메모리에 저장, 다음 단계로 전달)
- 콘솔 출력: 생성된 청크 개수

#### 예상 소요 시간
- 약 10-20분 (논문 수와 PDF 크기에 따라 다름)

#### 실행 결과 예시
```
============================================================
PDF 문서 로드 및 청크 분할
============================================================
PDF 디렉토리: /home/ieyeppo/AI_Lab/langchain-project/data/raw/pdfs
메타데이터: /home/ieyeppo/AI_Lab/langchain-project/data/raw/arxiv_papers_metadata.json

PDF 파일 로드 중...
✅ 총 8901개 청크 생성 완료

첫 번째 청크 예시:
------------------------------------------------------------
Simulating Hard Attention Using Soft Attention...
메타데이터: {'source': 'data/raw/pdfs/2412.09925v2.pdf', 'chunk_id': 0, 'title': '...'}
```

---

### 우선순위 4: 임베딩 생성 및 Vector DB 저장

#### 실행 명령어
```bash
python scripts/data/load_embeddings.py
```

#### 설명
- OpenAI Embeddings API로 문서 임베딩 생성
- PostgreSQL pgvector에 임베딩 벡터 저장
- 배치 처리 (50개씩) + Rate Limit 대응

#### 필요 조건
- **우선순위 1, 2 완료 필수**
- `.env` 파일에 `OPENAI_API_KEY` 설정
- OpenAI API 사용 가능 (유료 계정 권장)

#### 임베딩 모델
- `text-embedding-3-small` (1536 차원)
- 설정 위치: `configs/model_config.yaml`

#### 배치 처리 설정
- 배치 크기: 50개
- 배치 간 대기: 100ms
- Rate Limit 오류 시: 5초 대기 후 재시도

#### 생성 결과물

**PostgreSQL + pgvector에 저장되는 데이터:**

1. **langchain_pg_collection 테이블**: 컬렉션 메타데이터
   - `uuid`: 컬렉션 고유 ID
   - `name`: 컬렉션 이름 (`paper_chunks`)

2. **langchain_pg_embedding 테이블**: 임베딩 벡터 및 문서
   - `id`: 문서 고유 ID
   - `collection_id`: 컬렉션 UUID (FK)
   - `embedding`: 벡터 (vector 타입, 1536차원)
   - `document`: 원본 텍스트 (논문 청크)
   - `cmetadata`: JSON 메타데이터 (arxiv_id, paper_id, chunk_id 등)

3. **콘솔 출력**: 저장된 문서 개수 및 진행률

**저장 개수 예시:**
- 약 8,500개의 임베딩 벡터 (논문 수에 따라 다름)

#### 예상 소요 시간
- 약 30분-1시간 (논문 수에 따라 다름)
- OpenAI API Rate Limit에 따라 소요 시간 증가 가능

#### 실행 결과 예시
```
============================================================
임베딩 생성 및 Vector DB 저장
============================================================
PDF 디렉토리: /home/ieyeppo/AI_Lab/langchain-project/data/raw/pdfs
메타데이터: /home/ieyeppo/AI_Lab/langchain-project/data/raw/arxiv_papers_metadata.json
매핑 파일: /home/ieyeppo/AI_Lab/langchain-project/data/processed/paper_id_mapping.json

1단계: PDF 문서 로드 및 청크 분할 중...
   ✅ 8901개 청크 생성 완료

2단계: paper_id 매핑 파일 로드 중...
   ✅ 105개 매핑 로드 완료

3단계: 임베딩 생성 및 Vector DB 저장 중...
   (시간이 소요될 수 있습니다. 배치 처리 중...)

✅ 8501개 문서가 Vector DB에 저장되었습니다.
```

---

### 우선순위 5: 전체 파이프라인 일괄 실행 (선택사항)

#### 실행 명령어
```bash
python scripts/data/run_full_pipeline.py
```

#### 설명
- 우선순위 1~4를 순차적으로 자동 실행
- 각 단계 완료 여부 확인 후 다음 단계 진행
- 중간에 오류 발생 시 해당 단계에서 중단

#### 필요 조건
- 모든 환경 설정 완료
- PostgreSQL 서버 실행 중
- OpenAI API 키 설정
- 인터넷 연결

#### 실행 단계
1. **Phase 1**: arXiv 논문 수집
2. **Phase 2**: PostgreSQL 데이터베이스 초기화
3. **Phase 3**: PDF 문서 로드 및 청크 분할
4. **Phase 4**: 임베딩 생성 및 Vector DB 저장

#### 예상 소요 시간
- 약 1-2시간 (전체 파이프라인)

#### 실행 결과 예시
```
============================================================
전체 데이터 파이프라인 실행
============================================================

============================================================
Phase 1: arXiv 논문 수집
============================================================
✅ Phase 1: arXiv 논문 수집 완료

============================================================
Phase 2: PostgreSQL 데이터베이스 초기화
============================================================
✅ Phase 2: PostgreSQL 데이터베이스 초기화 완료

============================================================
Phase 3: PDF 문서 로드 및 청크 분할
============================================================
✅ Phase 3: PDF 문서 로드 및 청크 분할 완료

============================================================
Phase 4: 임베딩 생성 및 Vector DB 저장
============================================================
✅ Phase 4: 임베딩 생성 및 Vector DB 저장 완료

============================================================
✅ 전체 파이프라인 실행 완료!
============================================================
```

---

## 데이터 확인 방법

### 1. 수집된 PDF 파일 확인

```bash
# PDF 개수 확인
ls data/raw/pdfs/*.pdf | wc -l

# PDF 파일 목록 확인
ls -lh data/raw/pdfs/
```

### 2. 메타데이터 JSON 확인

```bash
# 논문 개수 확인
python -c "import json; data=json.load(open('data/raw/arxiv_papers_metadata.json', encoding='utf-8')); print(f'{len(data)}편')"

# 첫 번째 논문 정보 출력
python -c "import json; data=json.load(open('data/raw/arxiv_papers_metadata.json', encoding='utf-8')); print(data[0])"
```

### 3. PostgreSQL 데이터 확인

```bash
# papers 테이블 레코드 수
PGPASSWORD=dusrufdmlalswhr psql -h localhost -U langchain -d papers -c "SELECT COUNT(*) FROM papers;"

# 최근 논문 5개 조회
PGPASSWORD=dusrufdmlalswhr psql -h localhost -U langchain -d papers -c "SELECT paper_id, title, category FROM papers ORDER BY created_at DESC LIMIT 5;"

# glossary 테이블 확인 (초기 seed 데이터 3개)
PGPASSWORD=dusrufdmlalswhr psql -h localhost -U langchain -d papers -c "SELECT term, definition FROM glossary;"

# glossary 개수 확인
PGPASSWORD=dusrufdmlalswhr psql -h localhost -U langchain -d papers -c "SELECT COUNT(*) FROM glossary;"
```

**참고:** glossary 테이블은 초기에 3개의 seed 데이터만 저장됩니다:
- Attention Mechanism
- Fine-tuning
- BLEU Score

실제 용어집은 RAG 시스템에서 LLM이 논문을 분석하여 자동으로 추출하고 추가하는 구조입니다.

### 4. pgvector 임베딩 확인

```bash
# ⚠️ 잘못된 방법 (결과: 0개로 잘못 나옴)
PGPASSWORD=dusrufdmlalswhr psql -h localhost -U langchain -d papers -c "SELECT COUNT(*) FROM langchain_pg_embedding WHERE cmetadata->>'collection_name' = 'paper_chunks';"

# ✅ 올바른 방법: collection과 JOIN하여 조회
PGPASSWORD=dusrufdmlalswhr psql -h localhost -U langchain -d papers -c "
SELECT COUNT(*)
FROM langchain_pg_embedding e
JOIN langchain_pg_collection c ON e.collection_id = c.uuid
WHERE c.name = 'paper_chunks';"

# paper_chunks 컬렉션 정보 확인
PGPASSWORD=dusrufdmlalswhr psql -h localhost -U langchain -d papers -c "SELECT * FROM langchain_pg_collection;"

# 임베딩 샘플 확인 (첫 3개)
PGPASSWORD=dusrufdmlalswhr psql -h localhost -U langchain -d papers -c "
SELECT
    e.id,
    LEFT(e.document, 100) as document_preview,
    e.cmetadata->>'arxiv_id' as arxiv_id,
    e.cmetadata->>'paper_id' as paper_id,
    e.cmetadata->>'chunk_id' as chunk_id
FROM langchain_pg_embedding e
JOIN langchain_pg_collection c ON e.collection_id = c.uuid
WHERE c.name = 'paper_chunks'
LIMIT 3;"
```

**예상 결과:**
- **8,500개 이상의 임베딩 벡터** (논문 수에 따라 다름)
- 각 임베딩은 1536차원 벡터 (OpenAI text-embedding-3-small)
```

### 5. 매핑 파일 확인

```bash
# paper_id 매핑 개수 확인
python -c "import json; m=json.load(open('data/processed/paper_id_mapping.json', encoding='utf-8')); print(f'{len(m)}개 매핑')"

# 매핑 파일 내용 확인
cat data/processed/paper_id_mapping.json | head -20
```

### 6. 실험 로그 확인

```bash
# 가장 최근 실험 폴더 찾기
ls -t experiments/$(date +%Y%m%d)/ | head -1

# 최근 로그 확인
tail -50 experiments/$(date +%Y%m%d)/$(ls -t experiments/$(date +%Y%m%d)/ | head -1)/chatbot.log
```

---

## 주의사항

### 1. 실행 순서 준수
- **반드시 우선순위 1 → 2 → 3 → 4 순서대로 실행**
- 우선순위 5는 전체를 한 번에 실행하는 옵션 (처음 실행 시 권장)

### 2. 환경 변수 설정
- `.env` 파일에 모든 환경 변수가 올바르게 설정되어 있는지 확인
- 특히 `OPENAI_API_KEY`와 PostgreSQL 접속 정보는 필수

### 3. OpenAI API Rate Limit
- 무료 계정은 Rate Limit이 낮아 시간이 오래 걸릴 수 있음
- 유료 계정 권장 (Tier 1 이상)
- Rate Limit 오류 발생 시 자동으로 5초 대기 후 재시도

### 4. PostgreSQL 연결
- PostgreSQL 서버가 실행 중인지 확인
- 데이터베이스 `papers`가 생성되어 있어야 함 (없으면 자동 생성)

### 5. 디스크 공간
- PDF 파일: 약 200-500MB (100편 기준)
- PostgreSQL 데이터: 약 100-200MB
- 여유 공간 1GB 이상 권장

### 6. 중복 실행
- `collect_arxiv_papers.py`: 이미 다운로드된 PDF는 건너뜀
- `setup_database.py`: `ON CONFLICT (url) DO NOTHING`으로 중복 방지
- `load_embeddings.py`: 중복 실행 시 동일 데이터가 재삽입될 수 있음 (주의)

### 7. 빈 폴더 정리
- `ExperimentManager`는 세션 종료 시 빈 폴더를 자동으로 정리
- `experiments/` 폴더에 사용하지 않는 빈 폴더가 자동 삭제됨

---

## 트러블슈팅

### 오류 1: `ModuleNotFoundError: No module named 'src'`

**원인**: 프로젝트 루트 경로가 올바르게 설정되지 않음

**해결 방법**:
```bash
# 프로젝트 루트에서 실행하는지 확인
pwd
# 출력: /home/ieyeppo/AI_Lab/langchain-project

# 프로젝트 루트가 아니면 이동
cd /home/ieyeppo/AI_Lab/langchain-project
```

### 오류 2: `psycopg2.OperationalError: FATAL: password authentication failed`

**원인**: PostgreSQL 접속 정보가 올바르지 않음

**해결 방법**:
```bash
# .env 파일 확인
cat .env | grep POSTGRES

# PostgreSQL 접속 테스트
PGPASSWORD=dusrufdmlalswhr psql -h localhost -U langchain -d papers -c "SELECT 1;"
```

### 오류 3: `openai.error.RateLimitError: Rate limit reached`

**원인**: OpenAI API Rate Limit 초과

**해결 방법**:
- 배치 크기를 줄임: `batch_size=50` → `batch_size=20`
- 대기 시간 증가: `embeddings.py`의 `time.sleep(0.1)` → `time.sleep(0.5)`
- 유료 계정 Tier 업그레이드

### 오류 4: `arxiv.arxiv.HTTPError: 429 Too Many Requests`

**원인**: arXiv API Rate Limit 초과

**해결 방법**:
- 잠시 대기 후 재실행
- 키워드당 수집 개수 줄이기: `per_keyword=15` → `per_keyword=10`

### 오류 5: PDF 로딩 실패 (`PyPDFLoader` 오류)

**원인**: 손상된 PDF 파일 또는 암호화된 PDF

**해결 방법**:
- 해당 PDF 파일 삭제 후 재수집
- `document_loader.py`는 오류 발생 시 해당 파일을 건너뜀 (자동 처리)

---

## 관련 문서

- [프로젝트 개요](/docs/PRD/01_프로젝트_개요.md)
- [데이터베이스 설계](/docs/PRD/11_데이터베이스_설계.md)
- [RAG 시스템 설계](/docs/PRD/13_RAG_시스템_설계.md)
- [박재홍 역할 문서](/docs/roles/담당역할_03_박재홍_논문데이터수집.md)
- [이슈 문서 03-2](/docs/issues/03-2_data_pipeline_completion_report.md)
- [실험 결과 보고서](/docs/experiments/20251031/paper_review_app_ml_brief.md)

---

## 문의

구현 관련 문의사항이 있으시면 담당자(박재홍)에게 연락해주세요.

**문서 작성 완료일**: 2025-11-03
**작성자**: AI Assistant (박재홍 코드 분석 기반)
