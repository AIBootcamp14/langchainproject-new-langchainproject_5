{
  "golden_dataset": [
    {
      "question": "BERT가 뭐야?",
      "difficulty": "elementary",
      "expected_tool": "glossary",
      "expected_answer_keywords": [
        "언어 모델",
        "컴퓨터",
        "단어 이해",
        "문장"
      ],
      "category": "용어 정의",
      "complexity": "low",
      "answer_style": "매우 쉬운 비유와 일상 언어 사용 (초등학생 수준)",
      "example_answer_intro": "BERT는 컴퓨터가 우리말을 이해하도록 도와주는 특별한 프로그램이에요. 마치 책을 읽을 때...",
      "expected_tools": []
    },
    {
      "question": "BERT가 뭐야?",
      "difficulty": "elementary",
      "expected_tool": "glossary",
      "expected_answer_keywords": [
        "언어 모델",
        "양방향",
        "사전 학습",
        "Transformer"
      ],
      "category": "용어 정의",
      "complexity": "low",
      "answer_style": "쉬운 용어로 설명, 간단한 비유 포함 (초급자 수준)",
      "example_answer_intro": "BERT는 Google이 만든 언어 모델로, 문장의 앞뒤 문맥을 모두 고려하여...",
      "expected_tools": []
    },
    {
      "question": "BERT의 핵심 구조는?",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "Transformer",
        "인코더",
        "어텐션",
        "임베딩",
        "레이어"
      ],
      "category": "기술적 분석",
      "complexity": "medium",
      "answer_style": "기술 용어 사용, 구조적 설명 (중급자 수준)",
      "example_answer_intro": "BERT는 Transformer 인코더를 기반으로 하며, 다층 어텐션 메커니즘을...",
      "expected_tools": []
    },
    {
      "question": "BERT의 양방향 학습 메커니즘과 시간 복잡도는?",
      "difficulty": "advanced",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "Masked LM",
        "NSP",
        "O(n²)",
        "복잡도",
        "Self-Attention"
      ],
      "category": "기술적 분석",
      "complexity": "high",
      "answer_style": "전문 용어, 수식, 알고리즘 설명 (고급자/전문가 수준)",
      "example_answer_intro": "BERT는 Masked Language Modeling(MLM)과 Next Sentence Prediction(NSP)를 통해 양방향 문맥을 학습하며, Self-Attention의 시간 복잡도는 O(n²d)로...",
      "expected_tools": []
    },
    {
      "question": "RAG가 뭐야?",
      "difficulty": "elementary",
      "expected_tool": "glossary",
      "expected_answer_keywords": [
        "검색",
        "답변",
        "자료 찾기"
      ],
      "category": "용어 정의",
      "complexity": "low",
      "answer_style": "초등학생도 이해할 수 있는 비유",
      "example_answer_intro": "RAG는 도서관 사서처럼 먼저 필요한 책을 찾고, 그 책의 내용을 읽어서 답을 해주는 거예요...",
      "expected_tools": []
    },
    {
      "question": "RAG 논문 찾아줘",
      "difficulty": "beginner",
      "expected_tool": "search_paper",
      "expected_answer_keywords": [
        "Retrieval",
        "Generation",
        "논문",
        "검색"
      ],
      "category": "논문 검색",
      "complexity": "low",
      "answer_style": "논문 제목, 저자, 핵심 내용 간단 요약",
      "example_answer_intro": "RAG(Retrieval-Augmented Generation) 논문을 찾았습니다. Lewis et al. (2020)의 연구로...",
      "expected_tools": []
    },
    {
      "question": "RAG의 Retriever와 Generator 구조는?",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "DPR",
        "BART",
        "파이프라인",
        "문서 검색",
        "생성"
      ],
      "category": "기술적 분석",
      "complexity": "medium",
      "answer_style": "구조적 설명, 컴포넌트별 역할",
      "example_answer_intro": "RAG는 Retriever(DPR)와 Generator(BART)로 구성되며, 두 단계 파이프라인으로...",
      "expected_tools": []
    },
    {
      "question": "RAG의 학습 방법과 Dense Passage Retrieval의 손실 함수는?",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "End-to-End",
        "NLL",
        "Contrastive Loss",
        "In-batch Negatives"
      ],
      "category": "기술적 분석",
      "complexity": "high",
      "answer_style": "수식, 알고리즘, 학습 전략 상세 설명",
      "example_answer_intro": "RAG는 End-to-End 학습이 가능하며, DPR의 손실 함수는 Contrastive Loss로...",
      "expected_tools": []
    },
    {
      "question": "2024년 최신 AI 뉴스",
      "difficulty": "beginner",
      "expected_tool": "web_search",
      "expected_answer_keywords": [
        "최신",
        "AI",
        "뉴스",
        "간단 설명"
      ],
      "category": "최신 정보",
      "complexity": "low",
      "answer_style": "쉬운 단어로 뉴스 요약",
      "example_answer_intro": "요즘 AI는 그림도 그리고, 숙제도 도와주고, 노래도 만들어요...",
      "expected_tools": []
    },
    {
      "question": "2024년 최신 Transformer 연구",
      "difficulty": "beginner",
      "expected_tool": "web_search",
      "expected_answer_keywords": [
        "2024",
        "Transformer",
        "최신",
        "연구"
      ],
      "category": "최신 정보",
      "complexity": "low",
      "answer_style": "주요 연구 동향 간단 요약",
      "example_answer_intro": "2024년 Transformer 연구는 효율성 개선과 긴 문맥 처리에 집중하고 있으며...",
      "expected_tools": []
    },
    {
      "question": "2024년 Transformer 변형 모델들의 효율성 비교",
      "difficulty": "intermediate",
      "expected_tool": "web_search",
      "expected_answer_keywords": [
        "Linformer",
        "Performer",
        "효율성",
        "비교",
        "복잡도"
      ],
      "category": "최신 정보",
      "complexity": "medium",
      "answer_style": "모델별 성능 지표, 비교 분석",
      "example_answer_intro": "2024년 주요 Transformer 변형들은 선형 복잡도를 목표로 하며, Linformer는...",
      "expected_tools": []
    },
    {
      "question": "2024년 State Space Model과 Transformer의 이론적 등가성",
      "difficulty": "advanced",
      "expected_tool": "web_search",
      "expected_answer_keywords": [
        "Mamba",
        "S4",
        "등가성",
        "수학적 증명",
        "복잡도 분석"
      ],
      "category": "최신 정보",
      "complexity": "high",
      "answer_style": "수학적 증명, 이론적 분석, 논문 인용",
      "example_answer_intro": "Gu et al. (2024)는 Mamba의 선형 복잡도가 Transformer의 어텐션과 이론적으로 등가함을 증명...",
      "expected_tools": []
    },
    {
      "question": "Attention Is All You Need 읽어줘",
      "difficulty": "beginner",
      "expected_tool": "summarize",
      "expected_answer_keywords": [
        "Transformer",
        "집중",
        "단어",
        "번역"
      ],
      "category": "논문 요약",
      "complexity": "low",
      "answer_style": "동화책처럼 쉽게 설명",
      "example_answer_intro": "이 논문은 컴퓨터가 중요한 단어에 집중하는 새로운 방법을 소개해요...",
      "expected_tools": []
    },
    {
      "question": "Attention Is All You Need 요약해줘",
      "difficulty": "beginner",
      "expected_tool": "summarize",
      "expected_answer_keywords": [
        "Transformer",
        "Attention",
        "인코더",
        "디코더"
      ],
      "category": "논문 요약",
      "complexity": "low",
      "answer_style": "주요 내용 3-5개 항목으로 정리",
      "example_answer_intro": "Vaswani et al. (2017)의 Transformer 논문 요약: 1. RNN 없이 어텐션만 사용...",
      "expected_tools": []
    },
    {
      "question": "Transformer 논문의 핵심 기여와 구조",
      "difficulty": "intermediate",
      "expected_tool": "summarize",
      "expected_answer_keywords": [
        "Multi-Head Attention",
        "Positional Encoding",
        "병렬화",
        "성능"
      ],
      "category": "논문 요약",
      "complexity": "medium",
      "answer_style": "구조도 설명, 핵심 기여 분석",
      "example_answer_intro": "Transformer의 핵심은 (1) Multi-Head Attention으로 여러 관점에서 문맥 파악...",
      "expected_tools": []
    },
    {
      "question": "Transformer 논문의 Self-Attention 수식 유도와 복잡도 증명",
      "difficulty": "advanced",
      "expected_tool": "summarize",
      "expected_answer_keywords": [
        "수식",
        "Scaled Dot-Product",
        "O(n²)",
        "증명",
        "연산량"
      ],
      "category": "논문 요약",
      "complexity": "high",
      "answer_style": "수식 전개, 복잡도 증명, 알고리즘 분석",
      "example_answer_intro": "Self-Attention은 Attention(Q,K,V) = softmax(QK^T/√d_k)V로 정의되며, 시간 복잡도 증명...",
      "expected_tools": []
    },
    {
      "question": "인공지능이 뭐야?",
      "difficulty": "elementary",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "컴퓨터",
        "똑똑하게",
        "생각",
        "학습"
      ],
      "category": "개념 설명",
      "complexity": "low",
      "answer_style": "초등학생 눈높이 설명",
      "example_answer_intro": "인공지능은 컴퓨터가 사람처럼 생각하고 배우는 거예요...",
      "expected_tools": []
    },
    {
      "question": "BERT와 GPT의 차이는?",
      "difficulty": "beginner",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "양방향",
        "단방향",
        "마스킹",
        "생성"
      ],
      "category": "개념 비교",
      "complexity": "low",
      "answer_style": "표로 비교, 주요 차이점 3가지",
      "example_answer_intro": "BERT와 GPT의 주요 차이: (1) BERT는 양방향, GPT는 단방향...",
      "expected_tools": []
    },
    {
      "question": "BERT와 GPT의 아키텍처 및 학습 목표 비교",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "Encoder",
        "Decoder",
        "MLM",
        "CLM",
        "구조"
      ],
      "category": "개념 비교",
      "complexity": "medium",
      "answer_style": "구조도 비교, 학습 방식 상세 설명",
      "example_answer_intro": "BERT는 Transformer Encoder 기반 MLM, GPT는 Decoder 기반 CLM으로...",
      "expected_tools": []
    },
    {
      "question": "BERT와 GPT의 표현력 차이 이론적 분석",
      "difficulty": "advanced",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "Perplexity",
        "GLUE",
        "벤치마크",
        "수학적 증명",
        "정보 이론"
      ],
      "category": "개념 비교",
      "complexity": "high",
      "answer_style": "수학적 증명, 정보 이론 기반 분석, 벤치마크 비교",
      "example_answer_intro": "정보 이론 관점에서 BERT의 양방향 문맥은 조건부 엔트로피 H(X|Context)를 최소화하는 반면...",
      "expected_tools": []
    },
    {
      "question": "Self-Attention이 뭐야?",
      "difficulty": "elementary",
      "expected_tool": "glossary",
      "expected_answer_keywords": [
        "집중",
        "중요한 단어",
        "관계"
      ],
      "category": "용어 정의",
      "complexity": "low",
      "answer_style": "일상생활 비유",
      "example_answer_intro": "Self-Attention은 여러 단어 중에서 중요한 단어에 집중하는 거예요. 마치 선생님 말씀 중에서...",
      "expected_tools": []
    },
    {
      "question": "Attention 메커니즘 설명해줘",
      "difficulty": "beginner",
      "expected_tool": "glossary",
      "expected_answer_keywords": [
        "Attention",
        "가중치",
        "중요도",
        "집중"
      ],
      "category": "용어 정의",
      "complexity": "low",
      "answer_style": "그림으로 설명 가능한 수준",
      "example_answer_intro": "Attention은 입력 시퀀스에서 중요한 부분에 가중치를 두는 메커니즘으로...",
      "expected_tools": []
    },
    {
      "question": "Multi-Head Attention의 동작 원리는?",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "Query",
        "Key",
        "Value",
        "Head",
        "병렬"
      ],
      "category": "기술적 분석",
      "complexity": "medium",
      "answer_style": "단계별 프로세스 설명",
      "example_answer_intro": "Multi-Head Attention은 여러 개의 어텐션 헤드를 병렬로 실행하여...",
      "expected_tools": []
    },
    {
      "question": "Self-Attention의 시간 복잡도와 공간 복잡도 증명",
      "difficulty": "advanced",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "O(n²)",
        "O(n²d)",
        "메모리",
        "증명",
        "최적화"
      ],
      "category": "기술적 분석",
      "complexity": "high",
      "answer_style": "수식 전개, 복잡도 증명",
      "example_answer_intro": "Self-Attention의 시간 복잡도 T(n) = O(n²d) 증명: QK^T 연산이 O(n²d), softmax가 O(n²)이므로...",
      "expected_tools": []
    },
    {
      "question": "Transformer가 RNN보다 좋은 이유는?",
      "difficulty": "beginner",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "빠름",
        "병렬",
        "장거리 의존성"
      ],
      "category": "개념 비교",
      "complexity": "low",
      "answer_style": "장단점 비교",
      "example_answer_intro": "Transformer는 RNN보다 (1) 병렬 처리가 가능해 빠르고...",
      "expected_tools": []
    },
    {
      "question": "Transformer와 RNN의 병렬화 가능성 비교",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "순차",
        "병렬",
        "GPU",
        "연산 효율"
      ],
      "category": "개념 비교",
      "complexity": "medium",
      "answer_style": "GPU 아키텍처 관점 설명",
      "example_answer_intro": "RNN은 순차적 의존성으로 병렬화 불가능하나, Transformer는 Self-Attention으로...",
      "expected_tools": []
    },
    {
      "question": "Transformer가 RNN보다 나은 이유는?",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "병렬화",
        "long-range",
        "의존성",
        "속도",
        "Gradient",
        "Vanishing"
      ],
      "category": "개념 비교",
      "complexity": "high",
      "answer_style": "이론적 증명, 실험 결과",
      "example_answer_intro": "Transformer가 RNN을 능가하는 이론적 근거: (1) Gradient Flow 측면에서 RNN은 BPTT 시 Vanishing Gradient...",
      "expected_tools": []
    },
    {
      "question": "LoRA가 뭐야?",
      "difficulty": "elementary",
      "expected_tool": "glossary",
      "expected_answer_keywords": [
        "효율적",
        "학습",
        "적은 파라미터"
      ],
      "category": "용어 정의",
      "complexity": "low",
      "answer_style": "핵심만 간단히",
      "example_answer_intro": "LoRA는 큰 모델을 적은 파라미터로 효율적으로 학습하는 방법이에요...",
      "expected_tools": []
    },
    {
      "question": "LoRA와 Full Fine-tuning의 차이는?",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "Low-Rank",
        "파라미터",
        "효율",
        "메모리"
      ],
      "category": "개념 비교",
      "complexity": "medium",
      "answer_style": "파라미터 수, 메모리, 성능 비교표",
      "example_answer_intro": "LoRA는 Low-Rank 행렬로 파라미터를 0.1%로 줄이며, Full Fine-tuning 대비...",
      "expected_tools": []
    },
    {
      "question": "LoRA의 Low-Rank Adaptation 수학적 원리",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "SVD",
        "행렬 분해",
        "Rank",
        "수학적 증명"
      ],
      "category": "기술적 분석",
      "complexity": "high",
      "answer_style": "수식 유도, SVD 설명",
      "example_answer_intro": "LoRA는 가중치 업데이트 ΔW를 ΔW=BA (B∈R^(d×r), A∈R^(r×k))로 분해하여...",
      "expected_tools": []
    },
    {
      "question": "BLEU Score가 뭐야?",
      "difficulty": "elementary",
      "expected_tool": "glossary",
      "expected_answer_keywords": [
        "번역",
        "점수",
        "비교"
      ],
      "category": "용어 정의",
      "complexity": "low",
      "answer_style": "쉬운 비유",
      "example_answer_intro": "BLEU Score는 번역이 얼마나 잘됐는지 점수를 주는 거예요...",
      "expected_tools": []
    },
    {
      "question": "BLEU Score가 뭐야?",
      "difficulty": "elementary",
      "expected_tool": "glossary",
      "expected_answer_keywords": [
        "평가 지표",
        "번역",
        "유사도",
        "n-gram"
      ],
      "category": "용어 정의",
      "complexity": "low",
      "answer_style": "계산 방법 간단 설명",
      "example_answer_intro": "BLEU는 기계 번역 품질을 평가하는 지표로, n-gram 일치도를 측정하여...",
      "expected_tools": []
    },
    {
      "question": "BLEU Score 계산 방법과 한계점은?",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "Precision",
        "Brevity Penalty",
        "n-gram",
        "한계"
      ],
      "category": "기술적 분석",
      "complexity": "medium",
      "answer_style": "수식, 예시, 한계점 분석",
      "example_answer_intro": "BLEU = BP × exp(Σw_n log p_n)으로 계산되며, Brevity Penalty는...",
      "expected_tools": []
    },
    {
      "question": "BLEU Score의 수학적 정의와 대안 지표 비교",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "Modified Precision",
        "Geometric Mean",
        "METEOR",
        "BERTScore"
      ],
      "category": "기술적 분석",
      "complexity": "high",
      "answer_style": "수식 증명, 지표 간 상관관계 분석",
      "example_answer_intro": "BLEU의 Modified Precision p_n = Σ_C Σ_ngram min(Count, Count_ref) / Σ_C Σ_ngram Count로 정의되며...",
      "expected_tools": []
    },
    {
      "question": "GPT-4 논문 검색",
      "difficulty": "beginner",
      "expected_tool": "search_paper",
      "expected_answer_keywords": [
        "GPT-4",
        "논문",
        "OpenAI"
      ],
      "category": "논문 검색",
      "complexity": "low",
      "answer_style": "논문 제목, 저자, 간단 요약",
      "example_answer_intro": "GPT-4 Technical Report (OpenAI, 2023)을 찾았습니다...",
      "expected_tools": []
    },
    {
      "question": "GPT-4의 주요 개선사항은?",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "멀티모달",
        "성능",
        "안전성",
        "벤치마크"
      ],
      "category": "기술적 분석",
      "complexity": "medium",
      "answer_style": "GPT-3.5 대비 비교, 벤치마크 결과",
      "example_answer_intro": "GPT-4는 GPT-3.5 대비 (1) 멀티모달 지원 (2) 추론 능력 향상...",
      "expected_tools": []
    },
    {
      "question": "GPT-4의 아키텍처와 학습 방법론",
      "difficulty": "intermediate",
      "expected_tool": "general",
      "expected_answer_keywords": [
        "RLHF",
        "Alignment",
        "Constitutional AI",
        "스케일링"
      ],
      "category": "기술적 분석",
      "complexity": "high",
      "answer_style": "학습 알고리즘, RLHF 수식, 스케일링 법칙",
      "example_answer_intro": "GPT-4는 RLHF와 Constitutional AI를 결합하여, 보상 모델 r_θ(x,y) = log P(y_w > y_l | x)를...",
      "expected_tools": []
    },
    {
      "question": "2025년 LLM 트렌드",
      "difficulty": "beginner",
      "expected_tool": "web_search",
      "expected_answer_keywords": [
        "2025",
        "LLM",
        "트렌드",
        "최신"
      ],
      "category": "최신 정보",
      "complexity": "low",
      "answer_style": "주요 트렌드 3-5개 나열",
      "example_answer_intro": "2025년 LLM 주요 트렌드: (1) 멀티모달 확장 (2) 효율성 개선...",
      "expected_tools": []
    },
    {
      "question": "2025년 LLM의 주요 기술적 혁신은?",
      "difficulty": "intermediate",
      "expected_tool": "web_search",
      "expected_answer_keywords": [
        "MoE",
        "긴 문맥",
        "효율성",
        "추론"
      ],
      "category": "최신 정보",
      "complexity": "medium",
      "answer_style": "기술별 상세 설명, 논문 인용",
      "example_answer_intro": "2025년 LLM 혁신: (1) Mixture-of-Experts로 1T 파라미터 달성...",
      "expected_tools": []
    },
    {
      "question": "2025년 LLM 트렌드",
      "difficulty": "intermediate",
      "expected_tool": "web_search",
      "expected_answer_keywords": [
        "2025",
        "LLM",
        "트렌드",
        "최신",
        "이론",
        "스케일링"
      ],
      "category": "최신 정보",
      "complexity": "high",
      "answer_style": "이론적 배경, 스케일링 법칙, 논문 심층 분석",
      "example_answer_intro": "2025년 LLM은 Chinchilla 스케일링 법칙을 넘어 N_optimal ∝ C^a (a≈0.5)를 수정한...",
      "expected_tools": []
    },
    {
      "question": "대화 내용 저장해줘",
      "difficulty": "beginner",
      "expected_tool": "save_file",
      "expected_answer_keywords": [
        "저장",
        "파일"
      ],
      "category": "파일 관리",
      "complexity": "low",
      "answer_style": "간단 확인 메시지",
      "example_answer_intro": "네, 대화 내용을 파일로 저장했어요!",
      "expected_tools": []
    },
    {
      "question": "대화 내용 저장해줘",
      "difficulty": "beginner",
      "expected_tool": "save_file",
      "expected_answer_keywords": [
        "저장",
        "파일",
        "다운로드"
      ],
      "category": "파일 관리",
      "complexity": "low",
      "answer_style": "파일명, 경로 포함",
      "example_answer_intro": "대화 내용을 response_20250104_143025.txt로 저장했습니다...",
      "expected_tools": []
    },
    {
      "question": "Attention Mechanism 논문 찾아서 요약해줘",
      "difficulty": "beginner",
      "expected_tool": "search_paper",
      "expected_answer_keywords": [
        "논문",
        "요약",
        "Attention",
        "핵심"
      ],
      "category": "다중요청: 논문검색+요약",
      "complexity": "medium",
      "answer_style": "논문 검색 결과 + 요약 내용",
      "example_answer_intro": "Attention Mechanism 관련 논문을 찾았습니다. 주요 내용을 요약하면...",
      "expected_tools": ["search_paper", "summarize"]
    },
    {
      "question": "최근 Transformer 관련 논문을 찾고, 핵심 내용을 요약해줘",
      "difficulty": "beginner",
      "expected_tool": "search_paper",
      "expected_answer_keywords": [
        "Transformer",
        "논문",
        "요약",
        "최근"
      ],
      "category": "다중요청: 논문검색+요약",
      "complexity": "medium",
      "answer_style": "최신 논문 목록 + 핵심 요약",
      "example_answer_intro": "최근 Transformer 논문들을 찾았습니다. 핵심 내용은...",
      "expected_tools": ["search_paper", "summarize"]
    },
    {
      "question": "2024년 이후 Reinforcement Learning 관련 논문 중 상위 인용 5개만 골라서 보여줘",
      "difficulty": "beginner",
      "expected_tool": "text2sql",
      "expected_answer_keywords": [
        "2024",
        "Reinforcement Learning",
        "인용",
        "상위 5개"
      ],
      "category": "다중요청: 통계+논문검색",
      "complexity": "medium",
      "answer_style": "통계 조회 결과 + 논문 상세 정보",
      "example_answer_intro": "2024년 이후 RL 논문 중 상위 인용 5개를 찾았습니다...",
      "expected_tools": ["text2sql", "search_paper"]
    },
    {
      "question": "GPT 계열 모델의 발전 과정을 논문 기준으로 정리해줘",
      "difficulty": "intermediate",
      "expected_tool": "search_paper",
      "expected_answer_keywords": [
        "GPT",
        "발전",
        "논문",
        "정리"
      ],
      "category": "다중요청: 논문검색+요약",
      "complexity": "high",
      "answer_style": "GPT 시리즈 논문 + 발전 과정 요약",
      "example_answer_intro": "GPT 계열 모델의 발전 과정을 논문 기준으로 정리하면: GPT-1부터...",
      "expected_tools": ["search_paper", "summarize"]
    },
    {
      "question": "BERT와 RoBERTa의 차이점을 관련 논문들을 바탕으로 비교해줘",
      "difficulty": "intermediate",
      "expected_tool": "search_paper",
      "expected_answer_keywords": [
        "BERT",
        "RoBERTa",
        "차이점",
        "비교"
      ],
      "category": "다중요청: 논문검색+요약",
      "complexity": "high",
      "answer_style": "두 논문 비교 분석 + 주요 차이점",
      "example_answer_intro": "BERT와 RoBERTa 논문을 비교하면, 주요 차이점은...",
      "expected_tools": ["search_paper", "summarize"]
    },
    {
      "question": "최신 AI 컨퍼런스 소식을 찾아서 정리해줘",
      "difficulty": "beginner",
      "expected_tool": "web_search",
      "expected_answer_keywords": [
        "최신",
        "컨퍼런스",
        "AI",
        "정리"
      ],
      "category": "다중요청: 웹검색+요약",
      "complexity": "medium",
      "answer_style": "웹 검색 결과 + 주요 내용 요약",
      "example_answer_intro": "최신 AI 컨퍼런스 소식을 찾았습니다. 주요 내용은...",
      "expected_tools": ["web_search", "summarize"]
    },
    {
      "question": "Transformer 설명하고 관련 논문도 보여줘",
      "difficulty": "beginner",
      "expected_tool": "glossary",
      "expected_answer_keywords": [
        "Transformer",
        "설명",
        "논문"
      ],
      "category": "다중요청: 용어집+논문검색",
      "complexity": "medium",
      "answer_style": "용어 정의 + 관련 논문 목록",
      "example_answer_intro": "Transformer는 2017년 제안된 모델로... 관련 논문은...",
      "expected_tools": ["glossary", "search_paper"]
    },
    {
      "question": "Multimodal AI 관련 논문을 찾아서 주요 응용 사례를 정리해줘",
      "difficulty": "intermediate",
      "expected_tool": "search_paper",
      "expected_answer_keywords": [
        "Multimodal",
        "AI",
        "응용",
        "사례"
      ],
      "category": "다중요청: 논문검색+요약",
      "complexity": "high",
      "answer_style": "논문 검색 + 응용 사례 중심 요약",
      "example_answer_intro": "Multimodal AI 논문들을 찾았습니다. 주요 응용 사례는...",
      "expected_tools": ["search_paper", "summarize"]
    },
    {
      "question": "2024년 BERT 계열 논문 통계 보여주고 대표 논문 하나 요약해줘",
      "difficulty": "intermediate",
      "expected_tool": "text2sql",
      "expected_answer_keywords": [
        "2024",
        "BERT",
        "통계",
        "요약"
      ],
      "category": "다중요청: 통계+요약",
      "complexity": "high",
      "answer_style": "통계 정보 + 대표 논문 요약",
      "example_answer_intro": "2024년 BERT 계열 논문은 총 X편입니다. 대표 논문을 요약하면...",
      "expected_tools": ["text2sql", "summarize"]
    },
    {
      "question": "AI 윤리 관련 논문이 있다면 찾아서 주요 논점을 요약해줘",
      "difficulty": "beginner",
      "expected_tool": "search_paper",
      "expected_answer_keywords": [
        "AI",
        "윤리",
        "논문",
        "논점"
      ],
      "category": "다중요청: 논문검색+요약",
      "complexity": "medium",
      "answer_style": "논문 검색 + 주요 논점 요약",
      "example_answer_intro": "AI 윤리 관련 논문을 찾았습니다. 주요 논점은...",
      "expected_tools": ["search_paper", "summarize"]
    }
  ],
  "dataset_metadata": {
    "total_questions": 52,
    "version": "3.0",
    "created_date": "2025-11-04",
    "updated_date": "2025-11-05",
    "description": "다중 요청 질문 10개 추가 (42→52개), 실제 시나리오 기반",
    "difficulty_distribution": {
      "elementary": 8,
      "beginner": 18,
      "intermediate": 21,
      "advanced": 5
    },
    "user_level_distribution": {
      "elementary": 8,
      "beginner": 18,
      "intermediate": 16,
      "advanced": 10
    },
    "tool_distribution": {
      "glossary": 9,
      "search_paper": 12,
      "web_search": 9,
      "summarize": 13,
      "general": 14,
      "save_file": 2,
      "text2sql": 3
    },
    "category_distribution": {
      "용어 정의": 10,
      "논문 검색": 4,
      "최신 정보": 8,
      "논문 요약": 4,
      "개념 비교": 8,
      "개념 설명": 2,
      "기술적 분석": 10,
      "파일 관리": 2,
      "다중요청: 논문검색+요약": 7,
      "다중요청: 통계+논문검색": 1,
      "다중요청: 웹검색+요약": 1,
      "다중요청: 용어집+논문검색": 1,
      "다중요청: 통계+요약": 1
    },
    "complexity_distribution": {
      "low": 18,
      "medium": 18,
      "high": 16
    },
    "user_level_mapping": {
      "elementary": {
        "description": "초등학생 수준 - 일상 언어, 쉬운 비유",
        "difficulty_range": [
          "easy"
        ],
        "answer_characteristics": [
          "전문 용어 최소화",
          "실생활 비유 사용",
          "짧고 간단한 문장",
          "동화책 스타일"
        ]
      },
      "beginner": {
        "description": "초급자 수준 - 기본 개념 이해",
        "difficulty_range": [
          "easy"
        ],
        "answer_characteristics": [
          "기본 용어 설명 포함",
          "간단한 비유",
          "3-5개 핵심 포인트",
          "그림으로 설명 가능한 수준"
        ]
      },
      "intermediate": {
        "description": "중급자 수준 - 기술적 이해",
        "difficulty_range": [
          "hard"
        ],
        "answer_characteristics": [
          "기술 용어 사용",
          "구조적 설명",
          "알고리즘 프로세스",
          "비교 분석 포함"
        ]
      },
      "advanced": {
        "description": "고급자/전문가 수준 - 깊이 있는 분석",
        "difficulty_range": [
          "hard"
        ],
        "answer_characteristics": [
          "수식과 증명",
          "이론적 배경",
          "논문 인용",
          "복잡도 분석"
        ]
      }
    },
    "quality_criteria": {
      "answer_completeness": "각 user_level에 맞는 답변 완성도",
      "consistency": "동일 질문에 대한 user_level별 답변 일관성",
      "progression": "elementary → beginner → intermediate → advanced 점진적 난이도 증가",
      "tool_accuracy": "질문에 적합한 도구 선택 정확도"
    },
    "usage_guide": {
      "router_test": "expected_tool로 라우팅 정확도 테스트",
      "difficulty_test": "user_level별 답변 스타일 적합성 테스트",
      "prompt_engineering": "user_level별 프롬프트 최적화",
      "evaluation": "expected_answer_keywords + answer_style로 답변 품질 평가"
    },
    "last_updated": "2025-11-05",
    "changelog": {
      "v3.0": "다중 요청 질문 10개 추가 (scenarios 분석 기반)",
      "v2.0": "4단계 난이도, user_level 필드 제거",
      "v1.0": "초기 버전"
    }
  }
}