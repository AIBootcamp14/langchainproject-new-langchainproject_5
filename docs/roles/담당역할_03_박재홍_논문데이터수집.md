# 담당역할: 박재홍 - 논문 데이터 수집 및 DB 구축

## 담당자 정보
- **이름**: 박재홍
- **역할**: 데이터 인프라 구축 담당
- **참여 기간**: 10/28 ~ 10/31 (4일, 단기 참여)
- **핵심 역할**: 논문 데이터 수집, Langchain Document 처리, DB 적재

---

## 담당 모듈 및 작업

### 1. 데이터 수집 (`scripts/`)
- arXiv API로 논문 수집 스크립트
- Semantic Scholar API 연동 (선택)
- 논문 메타데이터 수집

### 2. Langchain 문서 처리 (`src/data/`)
- Langchain Document Loader 구현 (PyPDFLoader)
- Langchain Text Splitter 구현 (RecursiveCharacterTextSplitter)
- PDF → Langchain Document 변환

### 3. 임베딩 및 Vector DB 적재 (`src/data/embeddings.py`)
- OpenAI Embeddings를 사용한 임베딩 생성
- Langchain PGVector (PostgreSQL + pgvector) 연동 및 문서 적재
- 용어집 데이터 임베딩 및 저장
- 배치 처리 최적화

### 4. 데이터베이스 초기 설정
- PostgreSQL 스키마 생성 (papers, glossary 테이블)
- pgvector extension 설치 및 초기화
- 벡터 컬렉션 생성
- 테스트 데이터 로드

---

## 핵심 작업: 논문 데이터 수집

### 목표
**10/31까지 최소 50-100편 논문 데이터**를 Langchain 파이프라인으로 처리하여 Vector DB 저장 완료

### 1. arXiv API로 논문 수집
```python
# scripts/collect_arxiv_papers.py

import arxiv
import os
from datetime import datetime

class ArxivPaperCollector:
    """arXiv에서 논문을 자동으로 수집"""

    def __init__(self, save_dir: str = "data/raw/pdfs"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def collect_papers(self, query: str, max_results: int = 100):
        """
        arXiv에서 논문 수집

        Args:
            query: 검색 쿼리
            max_results: 최대 수집 논문 수

        Returns:
            수집한 논문 메타데이터 리스트
        """
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )

        papers_metadata = []

        for result in search.results():
            try:
                # 메타데이터 수집
                paper_info = {
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "published_date": result.published,
                    "summary": result.summary,
                    "pdf_url": result.pdf_url,
                    "entry_id": result.entry_id,
                    "categories": result.categories,
                    "primary_category": result.primary_category
                }

                papers_metadata.append(paper_info)

                # PDF 다운로드
                arxiv_id = result.entry_id.split('/')[-1]
                pdf_filename = f"{self.save_dir}/{arxiv_id}.pdf"

                result.download_pdf(filename=pdf_filename)
                print(f"✓ Downloaded: {result.title}")

            except Exception as e:
                print(f"✗ Error downloading {result.title}: {e}")
                continue

        return papers_metadata

    def collect_by_keywords(self, keywords: list, papers_per_keyword: int = 20):
        """
        여러 키워드로 논문 수집

        Args:
            keywords: 키워드 리스트
            papers_per_keyword: 키워드당 수집할 논문 수

        Returns:
            전체 수집한 논문 메타데이터
        """
        all_papers = []

        for keyword in keywords:
            print(f"\n[수집 중] 키워드: {keyword}")
            papers = self.collect_papers(keyword, max_results=papers_per_keyword)
            all_papers.extend(papers)
            print(f"✓ {len(papers)}편 수집 완료")

        # 중복 제거 (제목 기준)
        unique_papers = self.remove_duplicates(all_papers)
        print(f"\n[총 수집] {len(unique_papers)}편 (중복 제거 후)")

        return unique_papers

    def remove_duplicates(self, papers: list) -> list:
        """제목 기준 중복 제거"""
        seen_titles = set()
        unique_papers = []

        for paper in papers:
            title_normalized = paper['title'].lower().strip()

            if title_normalized not in seen_titles:
                unique_papers.append(paper)
                seen_titles.add(title_normalized)

        return unique_papers


# 실행
if __name__ == "__main__":
    collector = ArxivPaperCollector()

    # AI/ML 논문 키워드
    keywords = [
        "transformer OR attention mechanism",
        "BERT OR GPT",
        "large language model",
        "retrieval augmented generation",
        "neural machine translation",
        "question answering",
        "few-shot learning"
    ]

    # 논문 수집 (키워드당 15편 = 총 ~100편)
    papers = collector.collect_by_keywords(keywords, papers_per_keyword=15)

    # 메타데이터 JSON 저장
    import json
    with open("data/raw/arxiv_papers_metadata.json", "w") as f:
        json.dump(papers, f, indent=2, default=str)

    print(f"\n메타데이터 저장 완료: data/raw/arxiv_papers_metadata.json")
```

---

## Langchain Document Loader 구현

### 2. PDF → Langchain Document 변환
```python
# src/data/document_loader.py

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import os

class PaperDocumentLoader:
    """논문 PDF를 Langchain Document로 변환"""

    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # 청크 크기
            chunk_overlap=200,  # 청크 간 중복 (맥락 유지)
            separators=["\n\n", "\n", ". ", " ", ""],  # 분할 우선순위
            length_function=len
        )

    def load_pdf(self, pdf_path: str, metadata: dict = None) -> list[Document]:
        """
        PDF 파일을 Langchain Document로 로드

        Args:
            pdf_path: PDF 파일 경로
            metadata: 추가할 메타데이터

        Returns:
            Langchain Document 리스트
        """
        # PyPDFLoader로 PDF 로드
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # 메타데이터 추가
        if metadata:
            for doc in documents:
                doc.metadata.update(metadata)

        return documents

    def load_and_split(self, pdf_path: str, metadata: dict = None) -> list[Document]:
        """
        PDF 파일을 로드하고 청크로 분할

        Args:
            pdf_path: PDF 파일 경로
            metadata: 추가할 메타데이터

        Returns:
            분할된 Langchain Document 리스트
        """
        # 1. PDF 로드
        documents = self.load_pdf(pdf_path, metadata)

        # 2. 텍스트 청크 분할
        chunks = self.text_splitter.split_documents(documents)

        # 3. 청크별 메타데이터 추가 (페이지 번호 등)
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = i

        return chunks

    def load_all_pdfs(self, pdf_dir: str, metadata_file: str = None) -> list[Document]:
        """
        디렉토리의 모든 PDF 파일을 로드하고 분할

        Args:
            pdf_dir: PDF 디렉토리 경로
            metadata_file: 메타데이터 JSON 파일 경로

        Returns:
            모든 PDF의 분할된 Document 리스트
        """
        # 메타데이터 로드 (JSON 파일)
        import json
        metadata_dict = {}

        if metadata_file and os.path.exists(metadata_file):
            with open(metadata_file, 'r') as f:
                papers_metadata = json.load(f)

                # arXiv ID로 메타데이터 매핑
                for paper in papers_metadata:
                    arxiv_id = paper['entry_id'].split('/')[-1]
                    metadata_dict[arxiv_id] = {
                        'title': paper['title'],
                        'authors': ', '.join(paper['authors']),
                        'publish_date': str(paper['published_date']),
                        'source': 'arXiv',
                        'url': paper['pdf_url'],
                        'category': paper['primary_category']
                    }

        # 모든 PDF 로드 및 분할
        all_chunks = []
        pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]

        for pdf_file in pdf_files:
            pdf_path = os.path.join(pdf_dir, pdf_file)
            arxiv_id = pdf_file.replace('.pdf', '')

            # 메타데이터 조회
            metadata = metadata_dict.get(arxiv_id, {})

            # PDF 로드 및 분할
            try:
                chunks = self.load_and_split(pdf_path, metadata)
                all_chunks.extend(chunks)
                print(f"✓ Loaded: {pdf_file} ({len(chunks)} chunks)")
            except Exception as e:
                print(f"✗ Error loading {pdf_file}: {e}")
                continue

        print(f"\n총 {len(all_chunks)} 청크 로드 완료")
        return all_chunks
```

---

## 임베딩 및 Vector DB 적재

### 3. OpenAI Embeddings 생성 및 pgvector 저장
```python
# src/data/embeddings.py

from langchain_openai import OpenAIEmbeddings
from langchain_postgres.vectorstores import PGVector
from langchain.schema import Document
import os

class PaperEmbeddingManager:
    """논문 임베딩 및 Vector DB 적재 관리"""

    def __init__(self):
        # OpenAI Embeddings 초기화
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # PostgreSQL + pgvector VectorStore 초기화
        self.vectorstore = PGVector(
            collection_name="paper_chunks",
            embedding_function=self.embeddings,
            connection_string="postgresql://user:password@localhost:5432/papers"
        )

    def add_documents(self, documents: list[Document], batch_size: int = 100):
        """
        Document 리스트를 Vector DB에 추가

        Args:
            documents: Langchain Document 리스트
            batch_size: 배치 크기 (OpenAI API 속도 제한 대응)
        """
        total = len(documents)
        print(f"[임베딩 시작] 총 {total} 청크")

        # 배치 처리
        for i in range(0, total, batch_size):
            batch = documents[i:i+batch_size]

            try:
                self.vectorstore.add_documents(batch)
                print(f"✓ {i+len(batch)}/{total} 청크 처리 완료")

            except Exception as e:
                print(f"✗ Error at batch {i}: {e}")
                continue

        print(f"\n[완료] {total} 청크 Vector DB 저장 완료")

    def add_documents_with_paper_id(self, documents: list[Document], paper_id_mapping: dict):
        """
        PostgreSQL paper_id를 메타데이터에 추가하여 저장

        Args:
            documents: Langchain Document 리스트
            paper_id_mapping: {arxiv_id: paper_id} 매핑
        """
        for doc in documents:
            # arXiv ID 추출
            url = doc.metadata.get('url', '')
            if 'arxiv.org' in url:
                arxiv_id = url.split('/')[-1]
                paper_id = paper_id_mapping.get(arxiv_id)

                if paper_id:
                    doc.metadata['paper_id'] = paper_id

        # Vector DB에 저장
        self.add_documents(documents)


# 실행
if __name__ == "__main__":
    from src.data.document_loader import PaperDocumentLoader

    # 1. PDF 로드 및 분할
    loader = PaperDocumentLoader()
    chunks = loader.load_all_pdfs(
        pdf_dir="data/raw/pdfs",
        metadata_file="data/raw/arxiv_papers_metadata.json"
    )

    # 2. 임베딩 및 Vector DB 저장
    embedding_manager = PaperEmbeddingManager()
    embedding_manager.add_documents(chunks, batch_size=50)
```

---

## PostgreSQL 데이터베이스 설정

### 4. 스키마 생성 및 메타데이터 저장
```python
# scripts/setup_database.py

import psycopg2
import json

def create_tables(conn):
    """PostgreSQL 테이블 생성"""
    cursor = conn.cursor()

    # 논문 메타데이터 테이블
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS papers (
            paper_id SERIAL PRIMARY KEY,
            title VARCHAR(500) NOT NULL,
            authors TEXT,
            publish_date DATE,
            source VARCHAR(100),
            url TEXT UNIQUE,
            category VARCHAR(100),
            citation_count INT DEFAULT 0,
            abstract TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)

    # 용어집 테이블
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS glossary (
            term_id SERIAL PRIMARY KEY,
            term VARCHAR(200) NOT NULL UNIQUE,
            definition TEXT NOT NULL,
            easy_explanation TEXT,
            hard_explanation TEXT,
            category VARCHAR(100),
            difficulty_level VARCHAR(20),
            related_terms TEXT[],
            examples TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)

    # 인덱스 생성
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_papers_title ON papers USING GIN (to_tsvector('english', title));")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_papers_category ON papers(category);")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_papers_date ON papers(publish_date);")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_glossary_term ON glossary(term);")

    conn.commit()
    cursor.close()
    print("✓ 테이블 및 인덱스 생성 완료")


def insert_paper_metadata(conn, papers_metadata: list):
    """논문 메타데이터를 PostgreSQL에 삽입"""
    cursor = conn.cursor()

    paper_id_mapping = {}

    for paper in papers_metadata:
        try:
            cursor.execute("""
                INSERT INTO papers (title, authors, publish_date, source, url, abstract, category)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
                ON CONFLICT (url) DO NOTHING
                RETURNING paper_id
            """, (
                paper['title'],
                ', '.join(paper['authors']),
                paper['published_date'],
                'arXiv',
                paper['pdf_url'],
                paper['summary'],
                paper['primary_category']
            ))

            result = cursor.fetchone()
            if result:
                paper_id = result[0]
                arxiv_id = paper['entry_id'].split('/')[-1]
                paper_id_mapping[arxiv_id] = paper_id

                print(f"✓ Inserted: {paper['title']}")

        except Exception as e:
            print(f"✗ Error inserting {paper['title']}: {e}")
            continue

    conn.commit()
    cursor.close()

    print(f"\n✓ {len(paper_id_mapping)} 논문 메타데이터 삽입 완료")
    return paper_id_mapping


def insert_glossary_data(conn):
    """용어집 초기 데이터 삽입"""
    cursor = conn.cursor()

    glossary_data = [
        {
            "term": "Attention Mechanism",
            "definition": "A technique that allows models to focus on specific parts of the input when generating output.",
            "easy_explanation": "책을 읽을 때 중요한 부분에 집중하는 것처럼, AI가 입력 데이터에서 중요한 부분에 집중하는 기술입니다.",
            "hard_explanation": "A weighted sum mechanism that computes attention scores between query and key vectors, allowing the model to dynamically focus on relevant input positions during sequence processing.",
            "category": "Deep Learning",
            "difficulty_level": "intermediate"
        },
        {
            "term": "Fine-tuning",
            "definition": "The process of adapting a pre-trained model to a specific task by training it on task-specific data.",
            "easy_explanation": "이미 학습된 AI 모델을 특정 작업에 맞게 추가로 학습시키는 과정입니다. 예를 들어, 일반 번역 모델을 의학 번역에 특화시키는 것입니다.",
            "hard_explanation": "Transfer learning technique where a pre-trained model's weights are updated on downstream task data, typically with a lower learning rate to preserve general knowledge while adapting to task-specific patterns.",
            "category": "Machine Learning",
            "difficulty_level": "beginner"
        },
        {
            "term": "BLEU Score",
            "definition": "A metric for evaluating machine translation quality by comparing generated text to reference translations.",
            "easy_explanation": "기계 번역의 품질을 평가하는 점수입니다. 컴퓨터가 번역한 문장이 사람이 번역한 문장과 얼마나 비슷한지를 측정합니다.",
            "hard_explanation": "Bilingual Evaluation Understudy score computed using modified n-gram precision with brevity penalty, ranging from 0 to 1, where higher scores indicate better translation quality.",
            "category": "NLP",
            "difficulty_level": "intermediate"
        }
    ]

    for term_data in glossary_data:
        try:
            cursor.execute("""
                INSERT INTO glossary (term, definition, easy_explanation, hard_explanation, category, difficulty_level)
                VALUES (%s, %s, %s, %s, %s, %s)
                ON CONFLICT (term) DO NOTHING
            """, (
                term_data['term'],
                term_data['definition'],
                term_data['easy_explanation'],
                term_data['hard_explanation'],
                term_data['category'],
                term_data['difficulty_level']
            ))

            print(f"✓ Inserted term: {term_data['term']}")

        except Exception as e:
            print(f"✗ Error inserting term: {e}")
            continue

    conn.commit()
    cursor.close()

    print("\n✓ 용어집 데이터 삽입 완료")


# 실행
if __name__ == "__main__":
    # PostgreSQL 연결
    conn = psycopg2.connect("postgresql://user:password@localhost/papers")

    # 1. 테이블 생성
    create_tables(conn)

    # 2. 논문 메타데이터 삽입
    with open("data/raw/arxiv_papers_metadata.json", 'r') as f:
        papers_metadata = json.load(f)

    paper_id_mapping = insert_paper_metadata(conn, papers_metadata)

    # 3. 용어집 데이터 삽입
    insert_glossary_data(conn)

    # 4. paper_id_mapping 저장
    with open("data/processed/paper_id_mapping.json", 'w') as f:
        json.dump(paper_id_mapping, f, indent=2)

    conn.close()
    print("\n✓ 데이터베이스 설정 완료")
```

---

## 10/31 인수인계 문서

### 완료 항목 체크리스트
- [ ] arXiv에서 최소 50편 논문 수집
- [ ] PDF → Langchain Document 변환 완료
- [ ] PostgreSQL 스키마 생성 (papers, glossary)
- [ ] PostgreSQL + pgvector 컬렉션 생성 (paper_chunks, glossary_embeddings)
- [ ] 논문 메타데이터 PostgreSQL 저장
- [ ] 논문 임베딩 pgvector 저장
- [ ] 용어집 초기 데이터 삽입
- [ ] paper_id_mapping.json 생성

### 인수인계 내용

#### 1. 데이터 위치
- **PDF 파일**: `data/raw/pdfs/`
- **메타데이터**: `data/raw/arxiv_papers_metadata.json`
- **paper_id 매핑**: `data/processed/paper_id_mapping.json`
- **Vector DB**: PostgreSQL + pgvector (connection: `postgresql://user:password@localhost:5432/papers`)

#### 2. 추가 데이터 수집 방법
```bash
# 추가 논문 수집
python scripts/collect_arxiv_papers.py --query "새로운 키워드" --max-results 50

# 임베딩 및 DB 저장
python src/data/embeddings.py
```

#### 3. DB 연결 정보
- PostgreSQL + pgvector: `postgresql://user:password@localhost:5432/papers`
  - 논문 메타데이터: `papers` 테이블
  - 용어집: `glossary` 테이블
  - 벡터 임베딩: pgvector extension 사용

#### 4. 주의사항
- OpenAI API 키 환경변수 설정 필수: `OPENAI_API_KEY`
- 임베딩 생성 시 배치 크기 조절 (API 속도 제한)
- PDF 다운로드 실패 시 재시도 로직 필요

---

## 개발 일정

### 10/28 (화)
- 환경 설정
- arXiv API 조사
- 수집 스크립트 초안 작성

### 10/29 (수)
- 논문 수집 (50-100편)
- Langchain Document Loader/Splitter 구현
- PDF → Document 변환 테스트

### 10/30 (목)
- OpenAI Embeddings 생성
- pgvector 적재
- PostgreSQL 스키마 생성
- 메타데이터 저장

### 10/31 (금)
- 용어집 데이터 삽입
- 최종 확인 및 테스트
- 인수인계 문서 작성
- 팀원들에게 인수인계

---

## Feature 브랜치

- `feature/data-collection` - arXiv 데이터 수집
- `feature/document-processing` - Document Loader/Splitter
- `feature/database-setup` - PostgreSQL + pgvector 설정

---

## 참고 자료

- arXiv API: https://info.arxiv.org/help/api/index.html
- Langchain Document Loaders: https://python.langchain.com/docs/integrations/document_loaders/
- Langchain Text Splitters: https://python.langchain.com/docs/modules/data_connection/document_transformers/
- Langchain Embeddings: https://python.langchain.com/docs/integrations/text_embedding/
- pgvector 문서: https://github.com/pgvector/pgvector
