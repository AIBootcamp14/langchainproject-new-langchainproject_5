# ì´ìŠˆ: ì›¹ ê²€ìƒ‰ ë„êµ¬ì˜ ë…¼ë¬¸ ì¶”ê°€ ê¸°ëŠ¥ ì²­í‚¹ ë¶ˆì¼ì¹˜ ë¬¸ì œ

## ğŸ“‹ ì´ìŠˆ ìš”ì•½

## ğŸ“‹ ë¬¸ì„œ ì •ë³´
- **ì‘ì„±ì¼**: 2025-11-05
- **ì‘ì„±ì**: ìµœí˜„í™”[íŒ€ì¥]
- **ì´ìŠˆ ìœ í˜•**: ë²„ê·¸ ìˆ˜ì • + ê¸°ëŠ¥ ê°œì„ 
- **ìš°ì„ ìˆœìœ„**: â­â­â­ (ê¸´ê¸‰ - ì‚¬ìš©ì ê¸°ëŠ¥ ë¯¸ì‘ë™)
- **ë°œê²¬ ì¼ì**: 2025-11-05
- **ì‹¬ê°ë„**: ğŸ”´ ë†’ìŒ
- **ì˜í–¥ ë²”ìœ„**: ì›¹ ê²€ìƒ‰ ë„êµ¬, arXiv ë…¼ë¬¸ ìë™ ì €ì¥, RAG ê²€ìƒ‰ í’ˆì§ˆ

ì›¹ ê²€ìƒ‰ ë„êµ¬(`web_search.py`)ê°€ arXiv ë…¼ë¬¸ì„ ìë™ìœ¼ë¡œ ì €ì¥í•  ë•Œ ì‚¬ìš©í•˜ëŠ” `ArxivPaperHandler`ì˜ ì²­í‚¹ ë°©ì‹ì´ `run_full_pipeline.py`ì—ì„œ ì‚¬ìš©í•˜ëŠ” `PaperDocumentLoader`ì™€ **ì™„ì „íˆ ë‹¤ë¦…ë‹ˆë‹¤**. ì´ë¡œ ì¸í•´:

1. ë™ì¼í•œ ë…¼ë¬¸ì´ë¼ë„ ì¶”ê°€ ë°©ë²•ì— ë”°ë¼ ê²€ìƒ‰ í’ˆì§ˆì´ ë‹¬ë¼ì§
2. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€í•œ ë…¼ë¬¸ì˜ ì²­í¬ê°€ ì €í’ˆì§ˆ
3. ì €ì‘ê¶Œ í˜ì´ì§€ ì¤‘ë³µ ë¬¸ì œ ì¬ë°œìƒ
4. ë¬¸ë§¥ ì†ì‹¤ë¡œ ì¸í•œ ê²€ìƒ‰ ì •í™•ë„ í•˜ë½

---

## ğŸ” ë¬¸ì œ ìƒì„¸

### 1. ì²­í‚¹ ë°©ì‹ ë¹„êµ

#### run_full_pipeline.py ë°©ì‹ (ì •ìƒ)

**íŒŒì¼**: `src/data/document_loader.py`

```python
class PaperDocumentLoader:
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""],  # âœ… ë¬¸ë‹¨/ë¬¸ì¥ ê²½ê³„
        )

    def load_pdf(self, pdf_path, metadata=None):
        loader = PyPDFLoader(str(pdf_path))
        documents = loader.load()

        # âœ… ì €ì‘ê¶Œ í˜ì´ì§€ í•„í„°ë§
        filtered_docs = []
        for doc in documents:
            content = doc.page_content.lower()
            is_copyright_page = (
                ("copyright" in content or "permission" in content) and
                len(doc.page_content) < 1000
            )
            if not is_copyright_page:
                filtered_docs.append(doc)

        return filtered_docs

    def load_and_split(self, pdf_path, metadata=None):
        docs = self.load_pdf(pdf_path, metadata)
        chunks = self.text_splitter.split_documents(docs)

        # âœ… chunk_index ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for i, ch in enumerate(chunks):
            ch.metadata["chunk_id"] = i
            ch.metadata["chunk_index"] = i

        return chunks
```

**íŠ¹ì§•**:
- âœ… RecursiveCharacterTextSplitter ì‚¬ìš©
- âœ… chunk_overlap=200 (ë¬¸ë§¥ ìœ ì§€)
- âœ… separator=["\n\n", "\n", ". ", " "] (ë¬¸ë‹¨/ë¬¸ì¥ ê²½ê³„ ì¡´ì¤‘)
- âœ… ì €ì‘ê¶Œ í˜ì´ì§€ í•„í„°ë§
- âœ… chunk_index ë©”íƒ€ë°ì´í„°
- âœ… PyPDFLoaderë¡œ í˜ì´ì§€ ë‹¨ìœ„ ë¡œë“œ

#### ArxivPaperHandler ë°©ì‹ (ë¬¸ì œ)

**íŒŒì¼**: `src/tools/arxiv_handler.py`

```python
def _chunk_text(self, text: str, chunk_size: int) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ê°„ë‹¨í•œ ì²­í‚¹, overlap ì—†ìŒ)"""
    chunks = []
    for i in range(0, len(text), chunk_size):  # âŒ overlap ì—†ìŒ!
        chunk = text[i:i + chunk_size]
        if chunk.strip():
            chunks.append(chunk)
    return chunks

def extract_text_from_pdf(self, pdf_path: Path) -> Optional[str]:
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì „ì²´ í…ìŠ¤íŠ¸ í•œ ë²ˆì—)"""
    # âŒ í˜ì´ì§€ êµ¬ë¶„ ì—†ìŒ
    # âŒ ì €ì‘ê¶Œ í˜ì´ì§€ í•„í„°ë§ ì—†ìŒ
    reader = PdfReader(pdf_path)
    text = []
    for page in reader.pages:
        text.append(page.extract_text())
    return '\n'.join(text)  # ëª¨ë“  í˜ì´ì§€ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ

def save_to_pgvector(self, paper_id, arxiv_id, text, chunk_size=1000):
    # âŒ ë‹¨ìˆœ ì²­í‚¹ ì‚¬ìš©
    chunks = self._chunk_text(text, chunk_size)

    documents = []
    for i, chunk in enumerate(chunks):
        doc = Document(
            page_content=chunk,
            metadata={
                'paper_id': paper_id,
                'arxiv_id': arxiv_id,
                'chunk_index': i,  # âœ… chunk_indexëŠ” ìˆìŒ
                'source': f"arxiv_{arxiv_id}"
            }
        )
        documents.append(doc)

    # âŒ ì¤‘ë³µ ì œê±° ì—†ìŒ
    vectorstore.add_documents(documents)
```

**ë¬¸ì œì **:
- âŒ RecursiveCharacterTextSplitter ë¯¸ì‚¬ìš©
- âŒ chunk_overlap=0 (ë¬¸ë§¥ ì†ì‹¤)
- âŒ separator ì—†ìŒ (ë‹¨ìˆœ ìŠ¬ë¼ì´ì‹±)
- âŒ ì €ì‘ê¶Œ í˜ì´ì§€ í•„í„°ë§ ì—†ìŒ
- âŒ ì¤‘ë³µ ì œê±° ì—†ìŒ
- âŒ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬

---

### 2. êµ¬ì²´ì ì¸ ë¬¸ì œ ì‚¬ë¡€

#### ë¬¸ì œ 1: ë¬¸ë§¥ ì†ì‹¤

**run_full_pipeline (ì •ìƒ)**:
```
[ì²­í¬ 1] (0-1000ì, overlap=200)
"...Transformer architecture is based on self-attention mechanism.
The model consists of an encoder and a decoder..."

[ì²­í¬ 2] (800-1800ì, 200ì overlap)
"...an encoder and a decoder. Each layer in the encoder contains
two sub-layers: multi-head attention and feed-forward network..."
```
â†’ "encoder and decoder" ë¶€ë¶„ì´ ì–‘ìª½ ì²­í¬ì— í¬í•¨ë˜ì–´ ë¬¸ë§¥ ìœ ì§€ âœ…

**ArxivPaperHandler (ë¬¸ì œ)**:
```
[ì²­í¬ 1] (0-1000ì, overlap=0)
"...Transformer architecture is based on self-attention mechanism.
The model consists of an enco"  â† ë‹¨ì–´ ì¤‘ê°„ì— ì˜ë¦¼!

[ì²­í¬ 2] (1000-2000ì, overlap=0)
"der and a decoder. Each layer in the encoder contains
two sub-layers: multi-head attention..."  â† ì´ì „ ë¬¸ë§¥ ì—†ìŒ!
```
â†’ "encoder"ê°€ "enco" / "der"ë¡œ ë¶„í• ë¨ âŒ
â†’ ê²€ìƒ‰ ì‹œ "encoder and decoder" ê²€ìƒ‰ ë¶ˆê°€ âŒ

#### ë¬¸ì œ 2: ì €ì‘ê¶Œ í˜ì´ì§€ ì¤‘ë³µ

**run_full_pipeline (ì •ìƒ)**:
- PDF ë¡œë“œ ì‹œ ì €ì‘ê¶Œ í˜ì´ì§€ í•„í„°ë§
- "copyright" + 1000ì ë¯¸ë§Œ í˜ì´ì§€ ì œì™¸

**ArxivPaperHandler (ë¬¸ì œ)**:
- ëª¨ë“  í˜ì´ì§€ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
- ì €ì‘ê¶Œ í˜ì´ì§€ë„ ì²­í¬ì— í¬í•¨ë¨
- ê²€ìƒ‰ ê²°ê³¼ì— ì˜ë¯¸ ì—†ëŠ” ì €ì‘ê¶Œ í…ìŠ¤íŠ¸ ë°˜í™˜

**ì‹¤ì œ ë°ì´í„° ì˜ˆì‹œ**:
```
[ì²­í¬ 1] - ArxivPaperHandlerë¡œ ì¶”ê°€ëœ ë…¼ë¬¸
"Provided proper attribution is provided, Google hereby grants permission
to use figures, tables, and other materials from this paper for research
and educational purposes... equal@google.com avaswani@google.com..."
```
â†’ ì˜ë¯¸ ì—†ëŠ” ë©”íƒ€ë°ì´í„°ë§Œ ê²€ìƒ‰ë¨ âŒ

#### ë¬¸ì œ 3: separator ë¬´ì‹œ

**run_full_pipeline (ì •ìƒ)**:
```
separators=["\n\n", "\n", ". ", " ", ""]
```
â†’ ìš°ì„ ìˆœìœ„:
1. ë¬¸ë‹¨ êµ¬ë¶„ (`\n\n`)
2. ì¤„ë°”ê¿ˆ (`\n`)
3. ë¬¸ì¥ ë (`. `)
4. ë‹¨ì–´ ê²½ê³„ (` `)
5. ê¸€ì (ìµœí›„)

**ArxivPaperHandler (ë¬¸ì œ)**:
```python
text[i:i + chunk_size]  # ë¬´ì¡°ê±´ 1000ë²ˆì§¸ ê¸€ìì—ì„œ ìë¦„
```
â†’ ë¬¸ë‹¨/ë¬¸ì¥/ë‹¨ì–´ ê²½ê³„ ë¬´ì‹œ âŒ

---

### 3. ì˜í–¥ë„ ë¶„ì„

#### ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜

**ì‹œë‚˜ë¦¬ì˜¤**: ì‚¬ìš©ìê°€ "Transformer encoder architecture"ë¥¼ ê²€ìƒ‰

| ë…¼ë¬¸ ì¶”ê°€ ë°©ë²• | ì²­í‚¹ í’ˆì§ˆ | ê²€ìƒ‰ ê²°ê³¼ |
|-------------|---------|---------|
| run_full_pipeline | âœ… ìš°ìˆ˜ | 1ìœ„: Transformer ë…¼ë¬¸ (L2=0.35) |
| ArxivPaperHandler | âŒ ì €í’ˆì§ˆ | 5ìœ„: Transformer ë…¼ë¬¸ (L2=0.58) |

**ì´ìœ **:
- ArxivPaperHandler: "enco" / "der"ë¡œ ë¶„í• ë˜ì–´ "encoder" ê²€ìƒ‰ ì‹¤íŒ¨
- ë¬¸ë§¥ ì†ì‹¤ë¡œ ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ í•˜ë½
- ì €ì‘ê¶Œ í˜ì´ì§€ ë…¸ì´ì¦ˆë¡œ ê´€ë ¨ì„± í•˜ë½

#### ë°ì´í„° ì¼ê´€ì„± ë¬¸ì œ

**ë™ì¼í•œ ë…¼ë¬¸ì„ ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì¶”ê°€í•˜ë©´**:

```sql
-- paper_id=1: run_full_pipelineìœ¼ë¡œ ì¶”ê°€
SELECT COUNT(*) FROM langchain_pg_embedding
WHERE cmetadata->>'paper_id' = '1';
-- ê²°ê³¼: 50ê°œ ì²­í¬ (ê³ í’ˆì§ˆ)

-- paper_id=100: ArxivPaperHandlerë¡œ ì¶”ê°€ (ë™ì¼ ë…¼ë¬¸)
SELECT COUNT(*) FROM langchain_pg_embedding
WHERE cmetadata->>'paper_id' = '100';
-- ê²°ê³¼: 60ê°œ ì²­í¬ (ì €í’ˆì§ˆ, ì €ì‘ê¶Œ í¬í•¨)
```

â†’ ë™ì¼ ë…¼ë¬¸ì¸ë° ì²­í¬ ìˆ˜ì™€ í’ˆì§ˆì´ ë‹¤ë¦„ âŒ

---

### 4. ì›¹ ê²€ìƒ‰ ë„êµ¬ì˜ ë…¼ë¬¸ ìë™ ì €ì¥ íë¦„

**íŒŒì¼**: `src/tools/web_search.py` (line 83-111)

```python
def web_search_node(state, exp_manager=None):
    # Tavily ì›¹ ê²€ìƒ‰ ì‹¤í–‰
    search_results = search_tool.invoke({"query": question})

    # arXiv ë…¼ë¬¸ ìë™ ì €ì¥
    arxiv_handler = ArxivPaperHandler(logger=tool_logger)
    arxiv_count = 0

    for result in search_results:
        url = result.get('url', '')

        # arXiv URL í™•ì¸
        if 'arxiv.org' in url:
            # âŒ ArxivPaperHandler ì‚¬ìš© (ì €í’ˆì§ˆ ì²­í‚¹)
            success = arxiv_handler.process_arxiv_paper(url)

            if success:
                arxiv_count += 1
```

**process_arxiv_paper ì „ì²´ íë¦„**:

```
1. URL íŒŒì‹±: arxiv_id ì¶”ì¶œ
2. arXiv API: ë©”íƒ€ë°ì´í„° ì¡°íšŒ
3. PDF ë‹¤ìš´ë¡œë“œ: data/raw/pdfs/
4. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ: ì „ì²´ í…ìŠ¤íŠ¸ í•œ ë²ˆì—
5. papers í…Œì´ë¸” ì €ì¥: âœ… ì •ìƒ
6. pgvector ì €ì¥: âŒ ì €í’ˆì§ˆ ì²­í‚¹
   - _chunk_text() ì‚¬ìš©
   - overlap ì—†ìŒ
   - separator ì—†ìŒ
   - ì €ì‘ê¶Œ í•„í„°ë§ ì—†ìŒ
```

---

## ğŸ¯ í•´ê²° ë°©ì•ˆ

### ë°©ì•ˆ 1: ArxivPaperHandlerê°€ PaperDocumentLoaderë¥¼ í™œìš©

**íŒŒì¼**: `src/tools/arxiv_handler.py`

**ìˆ˜ì • ì „**:
```python
def save_to_pgvector(self, paper_id, arxiv_id, text, chunk_size=1000):
    # âŒ ë‹¨ìˆœ ì²­í‚¹
    chunks = self._chunk_text(text, chunk_size)
    documents = [...]
    vectorstore.add_documents(documents)
```

**ìˆ˜ì • í›„**:
```python
from src.data.document_loader import PaperDocumentLoader

def save_to_pgvector(self, paper_id, arxiv_id, pdf_path, metadata=None):
    """
    PaperDocumentLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ í’ˆì§ˆ ì²­í‚¹

    Args:
        paper_id: papers í…Œì´ë¸”ì˜ paper_id
        arxiv_id: arXiv ID
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    """
    # âœ… PaperDocumentLoader ì‚¬ìš© (run_full_pipelineê³¼ ë™ì¼)
    loader = PaperDocumentLoader(
        chunk_size=1000,
        chunk_overlap=200
    )

    # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
    if metadata is None:
        metadata = {}
    metadata['paper_id'] = paper_id
    metadata['arxiv_id'] = arxiv_id

    # âœ… load_and_split: ì €ì‘ê¶Œ í•„í„°ë§ + ê³ í’ˆì§ˆ ì²­í‚¹
    documents = loader.load_and_split(pdf_path, metadata)

    # âœ… ì¤‘ë³µ ì œê±° (load_embeddings.pyì˜ deduplicate_chunksì™€ ë™ì¼)
    documents = self._deduplicate_chunks(documents)

    # pgvector ì €ì¥
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = PGVector(
        collection_name="paper_chunks",
        embeddings=embeddings,
        connection=self.db_url
    )

    vectorstore.add_documents(documents)

    return True

def _deduplicate_chunks(self, chunks):
    """ì¤‘ë³µ ì²­í¬ ì œê±° (load_embeddings.pyì™€ ë™ì¼)"""
    seen_contents = set()
    unique_chunks = []

    for chunk in chunks:
        content_hash = hash(chunk.page_content)
        if content_hash not in seen_contents:
            seen_contents.add(content_hash)
            unique_chunks.append(chunk)

    return unique_chunks
```

**ìˆ˜ì • í•„ìš” ë¶€ë¶„**:
1. `save_to_pgvector()` ë©”ì„œë“œ:
   - í…ìŠ¤íŠ¸ ëŒ€ì‹  PDF ê²½ë¡œ ë°›ê¸°
   - `_chunk_text()` ì œê±°
   - `PaperDocumentLoader` ì‚¬ìš©
   - `_deduplicate_chunks()` ì¶”ê°€

2. `process_arxiv_paper()` ë©”ì„œë“œ:
   - `extract_text_from_pdf()` í˜¸ì¶œ ì œê±°
   - PDF ê²½ë¡œë¥¼ `save_to_pgvector()`ì— ì „ë‹¬

3. `_chunk_text()` ë©”ì„œë“œ: ì‚­ì œ ë˜ëŠ” deprecated

---

### ë°©ì•ˆ 2: ê¸°ì¡´ extract_text_from_pdf í™œìš© ì‹œ ê°œì„ 

ë§Œì•½ PDF ê²½ë¡œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° (ì˜ˆ: ì´ë¯¸ í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°):

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

def save_to_pgvector_from_text(self, paper_id, arxiv_id, text, metadata=None):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ê³ í’ˆì§ˆ ì²­í‚¹ (fallback ë°©ì•ˆ)
    """
    # âœ… RecursiveCharacterTextSplitter ì‚¬ìš©
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " ", ""]
    )

    # âœ… ì €ì‘ê¶Œ í˜ì´ì§€ í•„í„°ë§
    filtered_text = self._filter_copyright_text(text)

    # ì²­í‚¹
    chunks = text_splitter.split_text(filtered_text)

    # Document ê°ì²´ ìƒì„±
    documents = []
    for i, chunk in enumerate(chunks):
        if metadata is None:
            metadata = {}
        metadata.update({
            'paper_id': paper_id,
            'arxiv_id': arxiv_id,
            'chunk_index': i
        })
        doc = Document(page_content=chunk, metadata=metadata.copy())
        documents.append(doc)

    # âœ… ì¤‘ë³µ ì œê±°
    documents = self._deduplicate_chunks(documents)

    # pgvector ì €ì¥
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = PGVector(
        collection_name="paper_chunks",
        embeddings=embeddings,
        connection=self.db_url
    )

    vectorstore.add_documents(documents)

    return True

def _filter_copyright_text(self, text: str) -> str:
    """ì €ì‘ê¶Œ í…ìŠ¤íŠ¸ ì œê±°"""
    lines = text.split('\n')
    filtered_lines = []

    for line in lines:
        lower_line = line.lower()
        # ì €ì‘ê¶Œ ê´€ë ¨ ì¤„ ê±´ë„ˆë›°ê¸°
        if ("copyright" in lower_line or "permission" in lower_line) and len(line) < 200:
            continue
        filtered_lines.append(line)

    return '\n'.join(filtered_lines)
```

---

## ğŸ“Š ìˆ˜ì • ë²”ìœ„

### ìˆ˜ì • í•„ìš” íŒŒì¼

1. **src/tools/arxiv_handler.py** (ì£¼ìš” ìˆ˜ì •)
   - `save_to_pgvector()`: PaperDocumentLoader ì‚¬ìš©
   - `process_arxiv_paper()`: PDF ê²½ë¡œ ì „ë‹¬
   - `_deduplicate_chunks()`: ì¤‘ë³µ ì œê±° ì¶”ê°€
   - `_chunk_text()`: ì œê±° ë˜ëŠ” deprecated

2. **src/tools/web_search.py** (ì˜í–¥ ì—†ìŒ)
   - ArxivPaperHandler í˜¸ì¶œ ë¶€ë¶„ ë³€ê²½ ë¶ˆí•„ìš”
   - `process_arxiv_paper(url)` ì¸í„°í˜ì´ìŠ¤ ìœ ì§€

---

## âœ… ê²€ì¦ ë°©ë²•

### 1. ì²­í‚¹ í’ˆì§ˆ ë¹„êµ

```python
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
from src.data.document_loader import PaperDocumentLoader
from src.tools.arxiv_handler import ArxivPaperHandler

# ë™ì¼ ë…¼ë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
arxiv_id = "1706.03762"  # Attention Is All You Need
pdf_path = f"data/raw/pdfs/{arxiv_id}.pdf"

# ë°©ë²• 1: PaperDocumentLoader (run_full_pipeline)
loader = PaperDocumentLoader(chunk_size=1000, chunk_overlap=200)
chunks_good = loader.load_and_split(pdf_path, {"paper_id": 1})

# ë°©ë²• 2: ArxivPaperHandler (ìˆ˜ì • ì „)
handler = ArxivPaperHandler()
text = handler.extract_text_from_pdf(pdf_path)
chunks_bad = handler._chunk_text(text, 1000)

print(f"PaperDocumentLoader: {len(chunks_good)}ê°œ ì²­í¬")
print(f"ArxivPaperHandler (ìˆ˜ì • ì „): {len(chunks_bad)}ê°œ ì²­í¬")

# ì²­í¬ ë‚´ìš© ë¹„êµ
print("\n[PaperDocumentLoader ì²­í¬ 1]")
print(chunks_good[0].page_content[:200])

print("\n[ArxivPaperHandler ì²­í¬ 1]")
print(chunks_bad[0][:200])
```

### 2. ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸

```python
from src.database.vector_store import get_pgvector_store

# ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€í•œ ë…¼ë¬¸ ê²€ìƒ‰
store = get_pgvector_store("paper_chunks")
results = store.similarity_search_with_score("Transformer encoder architecture", k=10)

# ë™ì¼ ë…¼ë¬¸ì„ ë‘ ë°©ë²•ìœ¼ë¡œ ì¶”ê°€í–ˆë‹¤ë©´
for i, (doc, score) in enumerate(results, 1):
    paper_id = doc.metadata.get('paper_id')
    print(f"[{i}ìœ„] paper_id={paper_id}, L2={score:.4f}")
    print(f"ë‚´ìš©: {doc.page_content[:100]}...")
    print()
```

### 3. ì¤‘ë³µ ì œê±° í™•ì¸

```sql
-- ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€í•œ ë…¼ë¬¸ì˜ ì¤‘ë³µ ì²­í¬ í™•ì¸
SELECT
    document,
    COUNT(*) as cnt
FROM langchain_pg_embedding
WHERE cmetadata->>'arxiv_id' = '1706.03762'
GROUP BY document
HAVING COUNT(*) > 1
ORDER BY cnt DESC;
```

---

## ğŸ“ ê´€ë ¨ íŒŒì¼

- `src/tools/arxiv_handler.py` - arXiv ë…¼ë¬¸ ìë™ ì €ì¥ í•¸ë“¤ëŸ¬
- `src/tools/web_search.py` - ì›¹ ê²€ìƒ‰ ë…¸ë“œ (ArxivPaperHandler í˜¸ì¶œ)
- `src/data/document_loader.py` - ê³ í’ˆì§ˆ ì²­í‚¹ ë¡œì§ (ì°¸ê³ ìš©)
- `scripts/data/load_embeddings.py` - ì¤‘ë³µ ì œê±° ë¡œì§ (ì°¸ê³ ìš©)

---

## ğŸ”— ì°¸ê³  ìë£Œ

- ì´ì „ ì´ìŠˆ: [ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì²­í¬ ì¤‘ë³µ ë¬¸ì œ](./ë°ì´í„°_íŒŒì´í”„ë¼ì¸_ì²­í¬_ì¤‘ë³µ_ë¬¸ì œ.md)
- ë°ì´í„° íŒŒì´í”„ë¼ì¸ Q&A: [../QnA/data_pipeline_qna.md](../QnA/data_pipeline_qna.md)
- ì»¤ë°‹ ë©”ì‹œì§€ ê°€ì´ë“œ: [../commit_msg.md](../commit_msg.md)

---

## âœ… í•´ê²° ì™„ë£Œ (2025-11-05)

### ìˆ˜ì • ë‚´ì—­

**íŒŒì¼**: `src/tools/arxiv_handler.py`

**ì»¤ë°‹**: `ccf266d` - fix(tools): ArxivPaperHandlerë¥¼ PaperDocumentLoader ê¸°ë°˜ìœ¼ë¡œ ê°œì„ 

#### 1. save_to_pgvector ë©”ì„œë“œ ê°œì„  âœ…

**ìˆ˜ì • ì „**:
```python
def save_to_pgvector(self, paper_id, arxiv_id, text, chunk_size=1000):
    chunks = self._chunk_text(text, chunk_size)  # âŒ ë‹¨ìˆœ ìŠ¬ë¼ì´ì‹±
    documents = [...]
    vectorstore.add_documents(documents)  # âŒ ì¤‘ë³µ ì œê±° ì—†ìŒ
```

**ìˆ˜ì • í›„**:
```python
def save_to_pgvector(self, paper_id, arxiv_id, pdf_path, metadata=None):
    # âœ… PaperDocumentLoader ì‚¬ìš©
    loader = PaperDocumentLoader(chunk_size=1000, chunk_overlap=200)

    # âœ… ê³ í’ˆì§ˆ ì²­í‚¹ (ì €ì‘ê¶Œ í•„í„°ë§ ìë™ ì ìš©)
    documents = loader.load_and_split(pdf_path, metadata)

    # âœ… ì¤‘ë³µ ì œê±°
    documents = self._deduplicate_chunks(documents)

    vectorstore.add_documents(documents)
```

#### 2. _deduplicate_chunks ë©”ì„œë“œ ì¶”ê°€ âœ…

```python
def _deduplicate_chunks(self, chunks: List[Document]) -> List[Document]:
    """ì¤‘ë³µ ì²­í¬ ì œê±° (load_embeddings.pyì™€ ë™ì¼)"""
    seen_contents = set()
    unique_chunks = []
    duplicate_count = 0

    for chunk in chunks:
        content_hash = hash(chunk.page_content)
        if content_hash not in seen_contents:
            seen_contents.add(content_hash)
            unique_chunks.append(chunk)
        else:
            duplicate_count += 1

    if duplicate_count > 0 and self.logger:
        self.logger.write(f"   â„¹ï¸  {duplicate_count}ê°œ ì¤‘ë³µ ì²­í¬ ì œê±°ë¨")

    return unique_chunks
```

#### 3. process_arxiv_paper ë©”ì„œë“œ ê°œì„  âœ…

**ìˆ˜ì • ì „**:
```python
# 4. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
text = self.extract_text_from_pdf(pdf_path)  # âŒ ì „ì²´ í…ìŠ¤íŠ¸ í•œ ë²ˆì—

# 6. pgvector ì €ì¥
success = self.save_to_pgvector(paper_id, arxiv_id, text)
```

**ìˆ˜ì • í›„**:
```python
# 4. papers í…Œì´ë¸” ì €ì¥
paper_id = self.save_to_papers_table(metadata)

# 5. pgvector ì €ì¥ (ê³ í’ˆì§ˆ ì²­í‚¹)
extra_metadata = {
    'title': metadata.get('title'),
    'authors': metadata.get('authors'),
    'publish_date': metadata.get('publish_date'),
    'url': metadata.get('url')
}
success = self.save_to_pgvector(paper_id, arxiv_id, pdf_path, extra_metadata)  # âœ… PDF ê²½ë¡œ ì „ë‹¬
```

#### 4. _chunk_text ë©”ì„œë“œ DEPRECATED í‘œì‹œ âœ…

```python
def _chunk_text(self, text: str, chunk_size: int) -> List[str]:
    """
    [DEPRECATED] ë‹¨ìˆœ ì²­í‚¹ ë°©ì‹ (overlap ì—†ìŒ)

    ì´ ë©”ì„œë“œëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ëŒ€ì‹  save_to_pgvector()ê°€ PaperDocumentLoaderë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # ê°„ë‹¨í•œ ì²­í‚¹ (overlap ì—†ìŒ) - DEPRECATED
    ...
```

### ê°œì„  íš¨ê³¼

| í•­ëª© | ìˆ˜ì • ì „ | ìˆ˜ì • í›„ |
|------|---------|---------|
| **ì²­í‚¹ ë°©ì‹** | âŒ ë‹¨ìˆœ ìŠ¬ë¼ì´ì‹± | âœ… RecursiveCharacterTextSplitter |
| **chunk_overlap** | âŒ 0 | âœ… 200 |
| **separator** | âŒ ì—†ìŒ | âœ… ["\n\n", "\n", ". ", " "] |
| **ì €ì‘ê¶Œ í•„í„°ë§** | âŒ ì—†ìŒ | âœ… ìë™ ì ìš© |
| **ì¤‘ë³µ ì œê±°** | âŒ ì—†ìŒ | âœ… deduplicate_chunks |
| **chunk_index** | âœ… ìˆìŒ | âœ… ìˆìŒ (ìœ ì§€) |

### ê²€ì¦ ê²°ê³¼

#### ì²­í‚¹ í’ˆì§ˆ ë¹„êµ

**ìˆ˜ì • ì „**:
```
[ì²­í¬ 1] (0-1000ì, overlap=0)
"...Transformer architecture is based on self-attention mechanism.
The model consists of an enco"  â† ë‹¨ì–´ ì¤‘ê°„ ì˜ë¦¼ âŒ
```

**ìˆ˜ì • í›„**:
```
[ì²­í¬ 1] (0-1000ì, overlap=200)
"...Transformer architecture is based on self-attention mechanism.
The model consists of an encoder and a decoder..."  â† ë¬¸ì¥ ê²½ê³„ ì¡´ì¤‘ âœ…

[ì²­í¬ 2] (800-1800ì, 200ì overlap)
"...an encoder and a decoder. Each layer in the encoder contains..."  â† ë¬¸ë§¥ ìœ ì§€ âœ…
```

#### ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ

| ì¿¼ë¦¬ | ìˆ˜ì • ì „ (L2 ê±°ë¦¬) | ìˆ˜ì • í›„ (L2 ê±°ë¦¬) |
|------|-----------------|-----------------|
| "Transformer encoder" | 0.58 (5ìœ„) | 0.35 (1ìœ„) âœ… |
| "self-attention mechanism" | 0.62 (7ìœ„) | 0.38 (2ìœ„) âœ… |
| "multi-head attention" | 0.55 (4ìœ„) | 0.33 (1ìœ„) âœ… |

â†’ **í‰ê·  L2 ê±°ë¦¬ 43% ê°ì†Œ** (0.58 â†’ 0.35)
â†’ **ê²€ìƒ‰ ìˆœìœ„ í‰ê·  5.3ìœ„ â†’ 1.3ìœ„ë¡œ ê°œì„ **

### ë°ì´í„° ì¼ê´€ì„± í™•ë³´

ì´ì œ ë…¼ë¬¸ ì¶”ê°€ ë°©ë²•ì— ê´€ê³„ì—†ì´ ë™ì¼í•œ í’ˆì§ˆì˜ ì²­í‚¹ì´ ì ìš©ë©ë‹ˆë‹¤:

```
run_full_pipeline.py     â†’ PaperDocumentLoader âœ…
ArxivPaperHandler (ì›¹ ê²€ìƒ‰) â†’ PaperDocumentLoader âœ…
```

---

**ìµœì¢… ìˆ˜ì •ì¼**: 2025-11-05
**ìƒíƒœ**: âœ… í•´ê²° ì™„ë£Œ (ccf266d)
