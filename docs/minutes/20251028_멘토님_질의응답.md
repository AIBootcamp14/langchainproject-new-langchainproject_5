### 질문 1:

**질문 내용:** RAG와 AI Agent 중 양자 택일 해야하는지?

### 답변:

둘 중 하나 선택해서 진행하면 됩니다.

---

### 질문 2: AI Agent와 RAG 기능 조합에 대한 질문

**질문 내용:**
AI Agent를 선택했을 때 선택 기능에 보면 '3. 지식베이스(Vector DB, RDB 등)에 데이터를 수집해 적재하고, 이를 기반으로 답하는 RAG 기능' 부분에 대해 궁금합니다. AI Agent를 선택하며 RAG를 같이 사용한 프로젝트의 주제를 정하려고 하는데, 예시를 알려주시면 좋겠습니다. RAG를 해서 RDBMS 관계형 데이터베이스를 사용할 때 어떤 예시가 있는지 궁금합니다.

### **답변:**

도구 호출 기반으로 동작하는 AI Agent가 필수인데, 거기에 RAG를 추가하면 가산점을 받을 수 있습니다.

그래서:

- **필수 구현**: 웹 검색 사용하는 에이전트, 파일 검색 사용하는 에이전트
- **RAG 추가 시**: 에이전트가 웹 검색, 파일 저장하는 것 뿐만 아니라, 특정 도메인 주제로 답변하는 챗봇을 만들 수 있습니다.

**예시 - 여행 챗봇:**

- 여행 주제로 답변하는 챗봇이라고 가정
- 여행 패키지 설명 PDF를 챗봇이 가지고 있음
- 사용자가 "어떠어떠한 상품에 대해 설명해줘"라고 질문
- RAG를 기반으로 답변 가능

**Vector DB vs RDB 사용:**

- **Vector DB**: 문서 기반 검색
- **RDB 사용 시**:
    - 테이블 형태로 데이터 저장 (예: 여행지, 관광명소, 비용 등)
    - RDB를 기준으로 답변 가능
    - Text-to-SQL 방식으로 구현

### **정리:**

- 도메인이 너무 다양할 것 같음
- AI Agent에서 말하는 Vector DB와 RDB 사용해서 RAG 구현하라고 하는 것은 **추가 맥락, 추가 정보, 문서를 기반으로 답변할 수 있는 에이전트**를 만들기 위해서 사용할 수 있습니다.

---

### 질문 3: 질의어를 통한 DB 선택 로직

**질문 내용:**
질의어가 request고, 질의어를 통해서 RDB를 사용할지 Vector DB를 사용할지 LLM을 사용할지 판단을 하는게 구현해야 하는 기능중에 하나인지? 하이브리드로 하는건지?

### **답변:**

에이전트는 기본적으로 **도구 호출(Tool Calling)**이라는 기능을 가지고 있게 만들어야 합니다.

특강자료를 참고하면 이해될 것입니다.

**에이전트의 동작 방식:**

- 모델이 사용자에게 질문을 받으면
- 이 질문이:
    - 그냥 일반답변만 하면 되는건지
    - RAG 도구를 사용해야하는건지
    - 웹 검색을 해야하는건지
- **도구 호출을 사용하면 그런 분기처리도 알아서 해주는 챗봇을 만들 수 있습니다.**

---

### 질문 4: 평가 기준에 대한 질문

**질문 내용:**
평가 기준도 선택사항에 있는데, NLP 자연어 처리 프로젝트를 할 때는 단순히 CSV 결과 파일 답안지를 만들어서 채점을 했습니다. 이 것을 직관적으로 생각했을 때는 평가할 수 있는 기준이 되게 많을거라고 생각합니다. 응답시간도 너무 지연되면 안될 것 같고, 지식 베이스에 정의 된 어느 정도 정해진 답을 찾는다면 얼마나 정확도가 높은 답변이 도출되었는지 평가할 수 있을 것 같습니다. 이번 프로젝트에서는 0점~5점 사이의 평점을 메기거나 특정한 지표를 사용해서 평가를 하는건지? LLM을 통해서 평가 머신을 만들어서 하이브리드로 해야하는건지 감이 안잡힙니다.

### **답변:**

### LLM 기반 평가 방법:

0점~5점 사이의 평점을 메기는 것은 **LLM에게 맡기는 것**입니다.

**평가 방법:**

- 그냥 맡기는게 아니라 **나름대로 평가항목은 정해줘야 합니다**
- 평가 항목 예시:
    - 답변을 얼마나 충실하게 답변하는지 (충실성)
    - 환각 여부
    - 등등
- 이런 항목들을 정해두고
- LLM에게 실제로 생성한 답변과 관련 문서를 비교했을 때
- 이 항목에 따라 0~5점까지 메겨달라고 평가를 맡길 수 있습니다

### 데이터셋 기반 평가 방법:

**RAG에서 관련된 문서를 잘 찾았는지 평가:**

- **골든 데이터셋(Golden Dataset)** 만들기
    - 사용자가 A라는 질문을 했을 때는 B라는 문서를 찾아내야 한다
    - 이런 예상된 답안을 평가 데이터셋으로 만들어 두기
- 실제로 이 데이터셋에서 예상한 문서와 실제로 도출한 문서가 정확히 일치하는지를 비교해서 평가

### **정리:**

평가 방법은 다양합니다:

1. LLM 기반 평가 (평가 항목 정의 필요)
2. 데이터셋 기반 일치/불일치 비교

---

### 질문 5: 최종 발표 시연 방식

**질문 내용:**
최종 발표 때 시연을 한다고 이해를 했는데, 질의어를 5개를 던져서 리얼타임으로 정해진 답변을 만든다음 답변 결과를 평가 시스템을 통해 점수를 도출하면 되는걸까요?

### **답변:**

맞습니다.

### **발표 방식:**

- 챗봇 만드신거 **데모**로 최종 발표에서 최종으로 보여주시면 됩니다
- 테스트 하면서 챗봇이 대응할 수 있는 **질문 리스트들이 생성될텐데,**  정리해두시고 보여주면 될 것 같습니다.

---

### 질문 6: 한계점 및 개선 방향 발표

**질문 내용:**
모든 프로그램이 완벽할 수 없다보니 잘 되는 것만 발표 할 수도 있는데, 아쉬운점이나 안되는 질의들을 찾아서 그게 왜 안되는건지 시간이 된다면 향후에 이렇게 해보면 어떨까 가설을 세워서 발표하는 것도 괜찮습니까?

### **답변:**

네네, 그럼 너무 좋죠!

### **발표 구성 추천:**

- 잘 되는 기능 시연
- 아쉬운 점이나 실패한 질의 사례
- 실패 원인 분석
- 향후 개선 방향 제시 (가설 포함)