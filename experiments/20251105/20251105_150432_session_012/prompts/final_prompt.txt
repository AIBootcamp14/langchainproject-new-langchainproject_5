[SYSTEM PROMPT - beginner]
당신은 논문을 쉽게 설명하는 친절한 전문가입니다.

답변 규칙:
- 검색된 논문들을 번호로 정리하세요
- 각 논문마다:
  1. 제목 (저자, 연도)
  2. 핵심 내용 3-5줄 요약
  3. 왜 이 논문이 중요한지
  4. 유사도 점수 (있으면)
- 전문 용어는 괄호로 쉽게 설명
- 최대 5개까지만 소개
- 친근하고 이해하기 쉬운 톤

[USER PROMPT]
[논문 검색 결과]
## 검색된 논문

### 1. Neural Machine Translation: A Review and Survey
- **저자**: Felix Stahlberg
- **출판일**: 2019-12-04
- **카테고리**: None
- **인용수**: 0
- **URL**: http://arxiv.org/pdf/1912.02047v2
- **섹션**: 본문
- **유사도 점수(낮을수록 유사)**: 0.5720

sukhin, I. (2017). Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wal-
lach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Process-
ing Systems 30 (pp. 5998–6008). Curran Associates, Inc. URL: http://papers.nips.cc/paper/
7181-attention-is-all-you-need.pdf .
Vijayakumar, A. K., Cogswell, M., Selvaraju, R. R., Sun, Q., Lee, S., Crandall, D., & Batra, D.
(2016). Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint
arXiv:1610.02424 , .
Vilar, D. (2018). Learning hidden unit contribution for adapting neu...

---

### 2. Neural Machine Translation: A Review and Survey
- **저자**: Felix Stahlberg
- **출판일**: 2019-12-04
- **카테고리**: None
- **인용수**: 0
- **URL**: http://arxiv.org/pdf/1912.02047v2
- **섹션**: 본문
- **유사도 점수(낮을수록 유사)**: 0.5721

sukhin, I. (2017). Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wal-
lach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Process-
ing Systems 30 (pp. 5998–6008). Curran Associates, Inc. URL: http://papers.nips.cc/paper/
7181-attention-is-all-you-need.pdf .
Vijayakumar, A. K., Cogswell, M., Selvaraju, R. R., Sun, Q., Lee, S., Crandall, D., & Batra, D.
(2016). Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint
arXiv:1610.02424 , .
Vilar, D. (2018). Learning hidden unit contribution for adapting neu...

---

### 3. Neural Machine Translation: A Review and Survey
- **저자**: Felix Stahlberg
- **출판일**: 2019-12-04
- **카테고리**: None
- **인용수**: 0
- **URL**: http://arxiv.org/pdf/1912.02047v2
- **섹션**: 본문
- **유사도 점수(낮을수록 유사)**: 0.5722

sukhin, I. (2017). Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wal-
lach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in Neural Information Process-
ing Systems 30 (pp. 5998–6008). Curran Associates, Inc. URL: http://papers.nips.cc/paper/
7181-attention-is-all-you-need.pdf .
Vijayakumar, A. K., Cogswell, M., Selvaraju, R. R., Sun, Q., Lee, S., Crandall, D., & Batra, D.
(2016). Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint
arXiv:1610.02424 , .
Vilar, D. (2018). Learning hidden unit contribution for adapting neu...

---

### 4. Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition
- **저자**: Chendong Zhao, Jianzong Wang, Wen qi Wei, Xiaoyang Qu, Haoqian Wang, Jing Xiao
- **출판일**: 2022-09-30
- **카테고리**: None
- **인용수**: 0
- **URL**: http://arxiv.org/pdf/2209.15176v1
- **섹션**: 본문
- **유사도 점수(낮을수록 유사)**: 0.5777

[35] R. Fan, P. Zhou, W. Chen, J. Jia, and G. Liu, “An online attention-based
model for speech recognition,” Proc. Interspeech 2019 , pp. 4390–4394,
2019.
[36] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz, R. Pang,
W. Li, and C. Raffel, “Monotonic inﬁnite lookback attention for simulta-
neous machine translation,” in Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics , 2019, pp. 1313–1323.
[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural...

---

### 5. Adaptive Sparse and Monotonic Attention for Transformer-based Automatic Speech Recognition
- **저자**: Chendong Zhao, Jianzong Wang, Wen qi Wei, Xiaoyang Qu, Haoqian Wang, Jing Xiao
- **출판일**: 2022-09-30
- **카테고리**: None
- **인용수**: 0
- **URL**: http://arxiv.org/pdf/2209.15176v1
- **섹션**: 본문
- **유사도 점수(낮을수록 유사)**: 0.5777

[35] R. Fan, P. Zhou, W. Chen, J. Jia, and G. Liu, “An online attention-based
model for speech recognition,” Proc. Interspeech 2019 , pp. 4390–4394,
2019.
[36] N. Arivazhagan, C. Cherry, W. Macherey, C.-C. Chiu, S. Yavuz, R. Pang,
W. Li, and C. Raffel, “Monotonic inﬁnite lookback attention for simulta-
neous machine translation,” in Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics , 2019, pp. 1313–1323.
[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural...

---


[질문]
"Attention Is All You Need" 논문 요약해서 저장해줘

위 검색 결과를 바탕으로 질문에 답변해주세요.

===== 메타데이터 =====
tool: search_paper
difficulty: easy
level: beginner
