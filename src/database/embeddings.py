# ==========================================
# ğŸ“˜ Embeddings í†µí•© ëª¨ë“ˆ
# ------------------------------------------
# - OpenAI Embeddings íŒ©í† ë¦¬ í•¨ìˆ˜
# - PaperEmbeddingManager í´ë˜ìŠ¤ (Vector DB ì €ì¥)
# - configs/model_config.yaml ì„¤ì • ì‚¬ìš©
# ==========================================

# ------------------------- í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
import logging
import os
import time
from typing import Dict, List, Optional

# ------------------------- ì„œë“œíŒŒí‹° ë¼ì´ë¸ŒëŸ¬ë¦¬ ------------------------- #
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_postgres.vectorstores import PGVector

# ------------------------- í”„ë¡œì íŠ¸ ëª¨ë“ˆ ------------------------- #
from src.utils.config_loader import get_postgres_connection_string, get_model_config

logger = logging.getLogger(__name__)


# ==================== ê¸°ë³¸ê°’ ì„¤ì • ==================== #

DEFAULT_EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")


# ==================== OpenAI Embeddings íŒ©í† ë¦¬ ==================== #

def get_embeddings(model: Optional[str] = None) -> OpenAIEmbeddings:
    """
    OpenAIEmbeddings ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

    Args:
        model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª… (ë¯¸ì§€ì • ì‹œ í™˜ê²½ë³€ìˆ˜(EMBEDDING_MODEL) ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)

    Returns:
        OpenAIEmbeddings ì¸ìŠ¤í„´ìŠ¤

    ì‚¬ìš© ëª¨ë¸:
    - text-embedding-3-small: 1536ì°¨ì›, ë¹„ìš© íš¨ìœ¨ì 
    - text-embedding-3-large: 3072ì°¨ì›, ë†’ì€ ì •í™•ë„

    ì°¸ê³ :
    - OpenAIEmbeddingsëŠ” ë‚´ë¶€ì ìœ¼ë¡œ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì½ìŒ
    - configs/model_config.yamlì˜ embeddings.model ì„¤ì • ìš°ì„  ì‚¬ìš© ê¶Œì¥
    """
    # ëª¨ë¸ëª… ê²°ì • (íŒŒë¼ë¯¸í„° > í™˜ê²½ë³€ìˆ˜ > ê¸°ë³¸ê°’)
    model_name = model or DEFAULT_EMBEDDING_MODEL

    # OpenAI Embeddings ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    return OpenAIEmbeddings(model=model_name)


# ==================== PaperEmbeddingManager í´ë˜ìŠ¤ ==================== #

class PaperEmbeddingManager:
    """
    ë…¼ë¬¸ ì„ë² ë”© ë° Vector DB ì €ì¥ í´ë˜ìŠ¤

    LangChain PGVectorë¥¼ ì‚¬ìš©í•˜ì—¬ PostgreSQL(pgvector)ì— ë¬¸ì„œë¥¼ ì ì¬í•©ë‹ˆë‹¤.
    """

    def __init__(self, collection_name: str = "paper_chunks") -> None:
        """
        PaperEmbeddingManager ì´ˆê¸°í™”

        Args:
            collection_name: pgvector ì»¬ë ‰ì…˜ëª… (ê¸°ë³¸ê°’: paper_chunks)
        """
        # -------------- config_loaderë¡œ ëª¨ë¸ ì„¤ì • ë¡œë“œ -------------- #
        model_config = get_model_config()
        embedding_config = model_config['embeddings']

        # -------------- OpenAI Embeddings ì´ˆê¸°í™” -------------- #
        self.embeddings = OpenAIEmbeddings(
            model=embedding_config['model'],
            api_key=os.getenv("OPENAI_API_KEY"),
        )

        # -------------- config_loaderë¡œ PostgreSQL ì—°ê²° ë¬¸ìì—´ ê°€ì ¸ì˜¤ê¸° -------------- #
        conn = get_postgres_connection_string()

        # -------------- PGVector VectorStore ì´ˆê¸°í™” -------------- #
        self.vectorstore = PGVector(
            collection_name=collection_name,
            connection=conn,
            embeddings=self.embeddings,
        )

    def add_documents(self, documents: List[Document], batch_size: int = 50) -> int:
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ Vector DBì— ì €ì¥

        Args:
            documents: ì €ì¥í•  Document ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 50)

        Returns:
            ì €ì¥ëœ ë¬¸ì„œ ìˆ˜
        """
        total = 0
        num_batches = (len(documents) + batch_size - 1) // batch_size

        logger.info(f"Starting to add {len(documents)} documents in {num_batches} batches")

        for i in range(0, len(documents), batch_size):
            batch = documents[i : i + batch_size]
            batch_num = i // batch_size + 1

            try:
                self.vectorstore.add_documents(batch)
                total += len(batch)
                logger.info(f"Batch {batch_num}/{num_batches}: Added {len(batch)} documents (total: {total})")

                # Rate Limit ëŒ€ì‘: ë°°ì¹˜ ê°„ ëŒ€ê¸°
                if i + batch_size < len(documents):
                    time.sleep(0.1)  # 100ms ëŒ€ê¸°

            except Exception as e:
                logger.error(f"Batch {batch_num}/{num_batches} failed: {e}")
                # Rate Limit ì˜¤ë¥˜ ì‹œ ë” ê¸´ ëŒ€ê¸°
                if "rate limit" in str(e).lower() or "429" in str(e):
                    logger.warning("Rate limit detected, waiting 5 seconds...")
                    time.sleep(5)
                continue

        logger.info(f"Completed: {total}/{len(documents)} documents added")
        return total

    def add_documents_with_paper_id(
        self,
        documents: List[Document],
        paper_id_mapping: Dict[str, int],
        batch_size: int = 50
    ) -> int:
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ paper_id ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë°°ì¹˜ë¡œ ì €ì¥

        Args:
            documents: ì €ì¥í•  Document ë¦¬ìŠ¤íŠ¸ (metadataì— 'arxiv_id' ë˜ëŠ” 'entry_id' í¬í•¨)
            paper_id_mapping: arxiv_id â†’ paper_id ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 50)

        Returns:
            ì €ì¥ëœ ë¬¸ì„œ ìˆ˜
        """
        # ë©”íƒ€ë°ì´í„°ì— paper_id ì¶”ê°€
        enriched_docs = []
        for doc in documents:
            # arxiv_id ì¶”ì¶œ
            arxiv_id = doc.metadata.get("arxiv_id")
            if not arxiv_id:
                # entry_idì—ì„œ ì¶”ì¶œ
                entry_id = doc.metadata.get("entry_id", "")
                arxiv_id = entry_id.split("/")[-1] if entry_id else None

            # paper_id ë§¤í•‘
            if arxiv_id and arxiv_id in paper_id_mapping:
                doc.metadata["paper_id"] = paper_id_mapping[arxiv_id]
                enriched_docs.append(doc)
            else:
                logger.warning(f"Paper ID not found for arxiv_id: {arxiv_id}, skipping document")

        logger.info(f"Enriched {len(enriched_docs)}/{len(documents)} documents with paper_id")

        # ë°°ì¹˜ ì €ì¥ ì‹¤í–‰
        return self.add_documents(enriched_docs, batch_size=batch_size)
