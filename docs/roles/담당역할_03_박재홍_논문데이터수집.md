# 담당역할: 박재홍 - 논문 데이터 수집 및 DB 구축

## 담당자 정보
- **이름**: 박재홍
- **역할**: 데이터 인프라 구축 담당
- **참여 기간**: 단기 참여 (4일)
- **핵심 역할**: 논문 데이터 수집, Langchain Document 처리, DB 적재

---

## 담당 모듈 및 작업

### 1. 데이터 수집 (`scripts/`)
- arXiv API로 논문 수집 스크립트
- Semantic Scholar API 연동 (선택)
- 논문 메타데이터 수집

### 2. Langchain 문서 처리 (`src/data/`)
- Langchain Document Loader 구현 (PyPDFLoader)
- Langchain Text Splitter 구현 (RecursiveCharacterTextSplitter)
- PDF → Langchain Document 변환

### 3. 임베딩 및 Vector DB 적재 (`src/data/embeddings.py`)
- OpenAI Embeddings를 사용한 임베딩 생성
- Langchain PGVector (PostgreSQL + pgvector) 연동 및 문서 적재
- 용어집 데이터 임베딩 및 저장
- 배치 처리 최적화

### 4. 데이터베이스 초기 설정
- PostgreSQL 스키마 생성 (papers, glossary 테이블)
- pgvector extension 설치 및 초기화
- 벡터 컬렉션 생성
- 테스트 데이터 로드

---

## 데이터 수집 파이프라인 다이어그램

### 1. 논문 데이터 수집 파이프라인

```mermaid
graph LR
    subgraph Collection["🔸 수집"]
        A[arXiv API] --> B[논문 검색<br/>키워드별]
        B --> C[메타데이터<br/>추출]
        C --> D[PDF<br/>다운로드]
    end

    subgraph Processing["🔹 처리"]
        D --> E[PyPDFLoader]
        E --> F[Text<br/>Splitting]
        F --> G[Chunks<br/>생성]
    end

    subgraph Storage["🔺 저장"]
        G --> H[임베딩<br/>생성]
        H --> I1[PostgreSQL<br/>메타데이터]
        H --> I2[pgvector<br/>벡터]
    end

    I1 -.-> J[✅ 완료]
    I2 -.-> J

    style Collection fill:#e1f5ff,stroke:#01579b,stroke-width:2px,color:#000
    style Processing fill:#f3e5f5,stroke:#4a148c,stroke-width:2px,color:#000
    style Storage fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px,color:#000

    style A fill:#90caf9,stroke:#1976d2,color:#000
    style B fill:#64b5f6,stroke:#1976d2,color:#000
    style C fill:#42a5f5,stroke:#1565c0,color:#000
    style D fill:#64b5f6,stroke:#1976d2,color:#000
    style E fill:#ce93d8,stroke:#7b1fa2,color:#000
    style F fill:#ba68c8,stroke:#7b1fa2,color:#000
    style G fill:#ab47bc,stroke:#4a148c,color:#000
    style H fill:#a5d6a7,stroke:#388e3c,color:#000
    style I1 fill:#81c784,stroke:#2e7d32,color:#000
    style I2 fill:#81c784,stroke:#2e7d32,color:#000
    style J fill:#66bb6a,stroke:#2e7d32,color:#000
```

### 2. 문서 처리 흐름

```mermaid
sequenceDiagram
    autonumber
    participant Collector as Collector
    participant arXiv as arXiv API
    participant Loader as PyPDFLoader
    participant Splitter as TextSplitter
    participant Embed as Embeddings
    participant VDB as Vector DB
    participant PG as PostgreSQL

    Collector->>arXiv: 키워드 검색<br/>(Transformer, BERT...)
    arXiv-->>Collector: 논문 메타데이터 리스트

    loop 각 논문
        Collector->>arXiv: PDF 다운로드
        arXiv-->>Collector: PDF 파일

        Collector->>Loader: load_pdf(path)
        Loader-->>Collector: Document 리스트

        Collector->>Splitter: split_documents(docs)
        Splitter-->>Collector: 청크 리스트

        Collector->>Embed: embed_documents(chunks)
        Embed-->>Collector: 임베딩 벡터

        Collector->>PG: INSERT 메타데이터
        PG-->>Collector: paper_id

        Collector->>VDB: add_documents(chunks)
        VDB-->>Collector: ✅ 저장 완료
    end

    style Collector fill:#90caf9,stroke:#1976d2,color:#000
    style arXiv fill:#ba68c8,stroke:#7b1fa2,color:#000
    style Loader fill:#ce93d8,stroke:#7b1fa2,color:#000
    style Splitter fill:#ab47bc,stroke:#4a148c,color:#000
    style Embed fill:#ffcc80,stroke:#f57c00,color:#000
    style VDB fill:#a5d6a7,stroke:#388e3c,color:#000
    style PG fill:#81c784,stroke:#2e7d32,color:#000
```

---

## 핵심 작업: 논문 데이터 수집

### 목표
**최소 50-100편 논문 데이터**를 Langchain 파이프라인으로 처리하여 Vector DB 저장 완료

### 1. arXiv API로 논문 수집

**파일 경로**: `scripts/collect_arxiv_papers.py`

**구현 방법**:
1. `ArxivPaperCollector` 클래스 정의
   - 초기화 시 저장 디렉토리 설정 (기본값: "data/raw/pdfs")
   - 디렉토리 자동 생성

2. `collect_papers` 메서드 구현
   - arxiv.Search 객체 생성 (query, max_results, sort_by 설정)
   - 검색 결과를 순회하며 메타데이터 수집
   - 각 논문의 title, authors, published_date, summary, pdf_url, entry_id, categories, primary_category 추출
   - PDF 다운로드 (arxiv_id 기반 파일명)
   - 오류 발생 시 해당 논문 건너뛰기
   - 수집한 논문 메타데이터 리스트 반환

3. `collect_by_keywords` 메서드 구현
   - 여러 키워드로 반복 수집
   - 각 키워드당 지정된 수만큼 논문 수집
   - 전체 논문 리스트 통합
   - 중복 제거 (제목 기준)
   - 최종 수집 결과 반환

4. `remove_duplicates` 메서드 구현
   - 제목을 소문자로 정규화하여 중복 확인
   - 중복되지 않은 논문만 유지

5. 실행 스크립트 작성
   - AI/ML 관련 키워드 리스트 정의
   - 키워드당 15편씩 수집 (총 ~100편)
   - 메타데이터를 JSON 파일로 저장

### 예제 코드

```python
# scripts/collect_arxiv_papers.py

import arxiv
import os
import json
from datetime import datetime
from src.utils.logger import Logger

# Logger 초기화
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "data_collection"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

class ArxivPaperCollector:
    """arXiv에서 논문을 수집하는 클래스"""

    def __init__(self, save_dir="data/raw/pdfs"):
        """
        Args:
            save_dir: PDF 파일 저장 디렉토리
        """
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def collect_papers(self, query, max_results=50):
        """
        arXiv에서 논문 수집

        Args:
            query: 검색 쿼리
            max_results: 최대 수집 논문 수

        Returns:
            논문 메타데이터 리스트
        """
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.SubmittedDate
        )

        papers_data = []

        for result in search.results():
            try:
                # 메타데이터 수집
                paper_info = {
                    "title": result.title,
                    "authors": [author.name for author in result.authors],
                    "published_date": result.published.strftime("%Y-%m-%d"),
                    "summary": result.summary,
                    "pdf_url": result.pdf_url,
                    "entry_id": result.entry_id,
                    "categories": result.categories,
                    "primary_category": result.primary_category
                }

                papers_data.append(paper_info)

                # PDF 다운로드
                arxiv_id = result.entry_id.split('/')[-1]
                pdf_filename = f"{self.save_dir}/{arxiv_id}.pdf"

                result.download_pdf(filename=pdf_filename)

                logger.write(f"Downloaded: {result.title} ({arxiv_id})")

            except Exception as e:
                logger.write(f"Error downloading {result.title}: {e}")
                continue

        return papers_data

    def collect_by_keywords(self, keywords, per_keyword=15):
        """
        여러 키워드로 논문 수집

        Args:
            keywords: 키워드 리스트
            per_keyword: 키워드당 수집할 논문 수

        Returns:
            전체 논문 메타데이터 리스트
        """
        all_papers = []

        for keyword in keywords:
            logger.write(f"\nCollecting papers for keyword: {keyword}")
            papers = self.collect_papers(keyword, max_results=per_keyword)
            all_papers.extend(papers)

        # 중복 제거
        unique_papers = self.remove_duplicates(all_papers)

        return unique_papers

    def remove_duplicates(self, papers):
        """
        제목 기준으로 중복 논문 제거

        Args:
            papers: 논문 리스트

        Returns:
            중복 제거된 논문 리스트
        """
        seen_titles = set()
        unique_papers = []

        for paper in papers:
            title_normalized = paper['title'].lower().strip()

            if title_normalized not in seen_titles:
                unique_papers.append(paper)
                seen_titles.add(title_normalized)

        return unique_papers


# 실행 스크립트
if __name__ == "__main__":
    collector = ArxivPaperCollector()

    # AI/ML 관련 키워드
    keywords = [
        "transformer attention",
        "BERT GPT",
        "large language model",
        "retrieval augmented generation",
        "neural machine translation",
        "question answering",
        "AI agent"
    ]

    # 키워드당 15편씩 수집
    papers = collector.collect_by_keywords(keywords, per_keyword=15)

    logger.write(f"\n총 {len(papers)}편의 논문 수집 완료")

    # 메타데이터를 JSON 파일로 저장
    with open("data/raw/arxiv_papers_metadata.json", "w", encoding="utf-8") as f:
        json.dump(papers, f, indent=2, ensure_ascii=False)

    logger.write("메타데이터 저장 완료: data/raw/arxiv_papers_metadata.json")

    logger.close()
```

---

## Langchain Document Loader 구현

### 2. PDF → Langchain Document 변환

**파일 경로**: `src/data/document_loader.py`

**구현 방법**:
1. `PaperDocumentLoader` 클래스 정의
   - RecursiveCharacterTextSplitter 초기화
   - chunk_size: 1000 (청크 크기)
   - chunk_overlap: 200 (청크 간 중복, 맥락 유지)
   - separators: ["\n\n", "\n", ". ", " ", ""] (분할 우선순위)

2. `load_pdf` 메서드 구현
   - PyPDFLoader로 PDF 파일 로드
   - 추가 메타데이터가 있으면 각 문서에 업데이트
   - Langchain Document 리스트 반환

3. `load_and_split` 메서드 구현
   - PDF 파일을 로드하고 청크로 분할
   - load_pdf()로 PDF 로드
   - text_splitter.split_documents()로 청크 분할
   - 각 청크에 chunk_id 메타데이터 추가
   - 분할된 Document 리스트 반환

4. `load_all_pdfs` 메서드 구현
   - JSON 메타데이터 파일 로드
   - arXiv ID로 메타데이터 매핑 딕셔너리 생성
   - 디렉토리의 모든 PDF 파일 순회
   - 각 PDF에 대해 메타데이터 조회 및 로드
   - load_and_split()으로 청크 분할
   - 오류 발생 시 해당 파일 건너뛰기
   - 모든 청크를 통합하여 반환

### 예제 코드

```python
# src/data/document_loader.py

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import json
from datetime import datetime
from src.utils.logger import Logger

# Logger 초기화
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "document_loader"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

class PaperDocumentLoader:
    """논문 PDF를 Langchain Document로 변환하는 클래스"""

    def __init__(self):
        """RecursiveCharacterTextSplitter 초기화"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # 청크 크기
            chunk_overlap=200,  # 청크 간 중복 (맥락 유지)
            separators=["\n\n", "\n", ". ", " ", ""]  # 분할 우선순위
        )

    def load_pdf(self, pdf_path, metadata=None):
        """
        PDF 파일 로드

        Args:
            pdf_path: PDF 파일 경로
            metadata: 추가 메타데이터

        Returns:
            Langchain Document 리스트
        """
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # 추가 메타데이터 업데이트
        if metadata:
            for doc in documents:
                doc.metadata.update(metadata)

        return documents

    def load_and_split(self, pdf_path, metadata=None):
        """
        PDF 파일을 로드하고 청크로 분할

        Args:
            pdf_path: PDF 파일 경로
            metadata: 추가 메타데이터

        Returns:
            분할된 Document 리스트
        """
        # PDF 로드
        documents = self.load_pdf(pdf_path, metadata)

        # 청크 분할
        chunks = self.text_splitter.split_documents(documents)

        # 각 청크에 chunk_id 추가
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = i

        return chunks

    def load_all_pdfs(self, pdf_dir, metadata_json_path):
        """
        디렉토리의 모든 PDF 파일을 로드하고 분할

        Args:
            pdf_dir: PDF 파일 디렉토리
            metadata_json_path: 메타데이터 JSON 파일 경로

        Returns:
            모든 청크를 통합한 Document 리스트
        """
        # JSON 메타데이터 로드
        with open(metadata_json_path, 'r', encoding='utf-8') as f:
            papers_metadata = json.load(f)

        # arXiv ID로 메타데이터 매핑
        metadata_map = {}
        for paper in papers_metadata:
            arxiv_id = paper['entry_id'].split('/')[-1]
            metadata_map[arxiv_id] = paper

        all_chunks = []

        # 모든 PDF 파일 순회
        for filename in os.listdir(pdf_dir):
            if not filename.endswith('.pdf'):
                continue

            arxiv_id = filename.replace('.pdf', '')
            pdf_path = os.path.join(pdf_dir, filename)

            # 메타데이터 조회
            metadata = metadata_map.get(arxiv_id, {})

            try:
                # 청크 분할
                chunks = self.load_and_split(pdf_path, metadata)
                all_chunks.extend(chunks)

                logger.write(f"Loaded and split: {filename} ({len(chunks)} chunks)")

            except Exception as e:
                logger.write(f"Error loading {filename}: {e}")
                continue

        return all_chunks


# 실행 스크립트
if __name__ == "__main__":
    loader = PaperDocumentLoader()

    # 모든 PDF 로드 및 분할
    chunks = loader.load_all_pdfs(
        pdf_dir="data/raw/pdfs",
        metadata_json_path="data/raw/arxiv_papers_metadata.json"
    )

    logger.write(f"\n총 {len(chunks)}개의 청크 생성 완료")

    logger.close()
```

---

## 임베딩 및 Vector DB 적재

### 3. OpenAI Embeddings 생성 및 pgvector 저장

**파일 경로**: `src/data/embeddings.py`

**구현 방법**:
1. `PaperEmbeddingManager` 클래스 정의
   - OpenAI Embeddings 초기화 (모델: text-embedding-3-small)
   - API 키를 환경변수에서 로드
   - PostgreSQL + pgvector VectorStore 초기화
   - 컬렉션명: "paper_chunks"
   - PostgreSQL 연결 문자열 설정

2. `add_documents` 메서드 구현
   - Document 리스트를 배치로 나누어 처리
   - OpenAI API 속도 제한 대응을 위한 배치 처리
   - 각 배치를 vectorstore.add_documents()로 저장
   - 진행 상황 출력
   - 오류 발생 시 해당 배치 건너뛰기

3. `add_documents_with_paper_id` 메서드 구현
   - 각 문서의 URL에서 arXiv ID 추출
   - paper_id_mapping에서 PostgreSQL paper_id 조회
   - 문서 메타데이터에 paper_id 추가
   - add_documents()로 Vector DB에 저장

4. 실행 스크립트 작성
   - PaperDocumentLoader로 PDF 로드 및 분할
   - PaperEmbeddingManager로 임베딩 및 Vector DB 저장
   - 배치 크기: 50

### 예제 코드

```python
# src/data/embeddings.py

from langchain_openai import OpenAIEmbeddings
from langchain_postgres.vectorstores import PGVector
import os
from datetime import datetime
from src.utils.logger import Logger

# Logger 초기화
today = datetime.now().strftime("%Y%m%d")
time_now = datetime.now().strftime("%H%M%S")
experiment_name = "data_embedding"
log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
os.makedirs(log_dir, exist_ok=True)
logger = Logger(log_path=f"{log_dir}/experiment.log")

class PaperEmbeddingManager:
    """논문 임베딩 및 Vector DB 저장 클래스"""

    def __init__(self):
        """OpenAI Embeddings 및 pgvector VectorStore 초기화"""
        # OpenAI Embeddings 초기화
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )

        # PostgreSQL + pgvector VectorStore 초기화
        self.vectorstore = PGVector(
            collection_name="paper_chunks",
            embedding_function=self.embeddings,
            connection_string=os.getenv("DATABASE_URL", "postgresql://user:password@localhost:5432/papers")
        )

    def add_documents(self, documents, batch_size=50):
        """
        Document 리스트를 배치로 나누어 Vector DB에 저장

        Args:
            documents: Langchain Document 리스트
            batch_size: 배치 크기 (OpenAI API 속도 제한 대응)

        Returns:
            저장된 문서 수
        """
        total_docs = len(documents)
        saved_count = 0

        # 배치 처리
        for i in range(0, total_docs, batch_size):
            batch = documents[i:i + batch_size]

            try:
                self.vectorstore.add_documents(batch)
                saved_count += len(batch)

                logger.write(f"Saved batch {i//batch_size + 1}: {saved_count}/{total_docs} documents")

            except Exception as e:
                logger.write(f"Error saving batch {i//batch_size + 1}: {e}")
                continue

        return saved_count

    def add_documents_with_paper_id(self, documents, paper_id_mapping):
        """
        각 문서의 URL에서 arXiv ID를 추출하여 paper_id 추가 후 저장

        Args:
            documents: Langchain Document 리스트
            paper_id_mapping: arXiv ID -> paper_id 매핑 딕셔너리

        Returns:
            저장된 문서 수
        """
        # 각 문서에 paper_id 추가
        for doc in documents:
            pdf_url = doc.metadata.get('pdf_url', '')
            if pdf_url:
                arxiv_id = pdf_url.split('/')[-1].replace('.pdf', '')
                paper_id = paper_id_mapping.get(arxiv_id)

                if paper_id:
                    doc.metadata['paper_id'] = paper_id

        # Vector DB에 저장
        return self.add_documents(documents)


# 실행 스크립트
if __name__ == "__main__":
    import json
    from src.data.document_loader import PaperDocumentLoader

    # 1. PDF 로드 및 분할
    loader = PaperDocumentLoader()
    chunks = loader.load_all_pdfs(
        pdf_dir="data/raw/pdfs",
        metadata_json_path="data/raw/arxiv_papers_metadata.json"
    )

    logger.write(f"\n총 {len(chunks)}개의 청크 생성 완료")

    # 2. paper_id_mapping 로드
    with open("data/processed/paper_id_mapping.json", 'r') as f:
        paper_id_mapping = json.load(f)

    # 3. 임베딩 및 Vector DB 저장
    embedding_manager = PaperEmbeddingManager()
    saved_count = embedding_manager.add_documents_with_paper_id(
        chunks,
        paper_id_mapping
    )

    logger.write(f"\n총 {saved_count}개의 문서를 Vector DB에 저장 완료")

    logger.close()
```

---

## PostgreSQL 데이터베이스 설정

### 4. 스키마 생성 및 메타데이터 저장

**파일 경로**: `scripts/setup_database.py`

**구현 방법**:
1. `create_tables` 함수 구현
   - PostgreSQL 연결 및 커서 생성
   - papers 테이블 생성 (paper_id, title, authors, publish_date, source, url, category, citation_count, abstract, created_at)
   - glossary 테이블 생성 (term_id, term, definition, easy_explanation, hard_explanation, category, difficulty_level, related_terms, examples, created_at)
   - 인덱스 생성 (papers의 title, category, date 및 glossary의 term)
   - 커밋 및 완료 메시지 출력

2. `insert_paper_metadata` 함수 구현
   - JSON 메타데이터 리스트를 순회
   - 각 논문 데이터를 papers 테이블에 INSERT
   - ON CONFLICT (url) DO NOTHING으로 중복 방지
   - RETURNING paper_id로 삽입된 ID 조회
   - arxiv_id와 paper_id 매핑 딕셔너리 생성
   - 오류 발생 시 해당 논문 건너뛰기
   - 매핑 딕셔너리 반환

3. `insert_glossary_data` 함수 구현
   - 용어집 초기 데이터 리스트 정의 (Attention Mechanism, Fine-tuning, BLEU Score 등)
   - 각 용어 데이터를 glossary 테이블에 INSERT
   - ON CONFLICT (term) DO NOTHING으로 중복 방지
   - 오류 발생 시 해당 용어 건너뛰기

4. 실행 스크립트 작성
   - PostgreSQL 연결
   - create_tables()로 테이블 및 인덱스 생성
   - JSON 파일에서 논문 메타데이터 로드
   - insert_paper_metadata()로 논문 데이터 삽입
   - insert_glossary_data()로 용어집 데이터 삽입
   - paper_id_mapping을 JSON 파일로 저장

---

## 인수인계 문서

### 완료 항목 체크리스트
- [ ] arXiv에서 최소 50편 논문 수집
- [ ] PDF → Langchain Document 변환 완료
- [ ] PostgreSQL 스키마 생성 (papers, glossary)
- [ ] PostgreSQL + pgvector 컬렉션 생성 (paper_chunks, glossary_embeddings)
- [ ] 논문 메타데이터 PostgreSQL 저장
- [ ] 논문 임베딩 pgvector 저장
- [ ] 용어집 초기 데이터 삽입
- [ ] paper_id_mapping.json 생성

### 인수인계 내용

#### 1. 데이터 위치
- **PDF 파일**: `data/raw/pdfs/`
- **메타데이터**: `data/raw/arxiv_papers_metadata.json`
- **paper_id 매핑**: `data/processed/paper_id_mapping.json`
- **Vector DB**: PostgreSQL + pgvector (connection: `postgresql://user:password@localhost:5432/papers`)

#### 2. 추가 데이터 수집 방법
```bash
# 추가 논문 수집
python scripts/collect_arxiv_papers.py --query "새로운 키워드" --max-results 50

# 임베딩 및 DB 저장
python src/data/embeddings.py
```

#### 3. DB 연결 정보
- PostgreSQL + pgvector: `postgresql://user:password@localhost:5432/papers`
  - 논문 메타데이터: `papers` 테이블
  - 용어집: `glossary` 테이블
  - 벡터 임베딩: pgvector extension 사용

#### 4. 주의사항
- OpenAI API 키 환경변수 설정 필수: `OPENAI_API_KEY`
- 임베딩 생성 시 배치 크기 조절 (API 속도 제한)
- PDF 다운로드 실패 시 재시도 로직 필요

---

## 로깅 및 실험 추적 관리

### 로깅 시스템 사용

**중요**: 모든 출력은 Logger 클래스를 사용해야 합니다.

**파일 경로**: `src/utils/logger.py`

**사용 방법**:
1. Logger 인스턴스 생성
   - 실험 폴더 생성 및 Logger 초기화
   ```python
   import os
   from datetime import datetime
   from src.utils.logger import Logger

   # 실험 폴더 생성
   today = datetime.now().strftime("%Y%m%d")
   time_now = datetime.now().strftime("%H%M%S")
   experiment_name = "data_collection"  # 또는 "data_embedding" 등
   log_dir = f"experiments/{today}/{today}_{time_now}_{experiment_name}"
   os.makedirs(log_dir, exist_ok=True)

   # Logger 초기화
   logger = Logger(log_path=f"{log_dir}/experiment.log")
   ```

2. 로그 기록
   - `logger.write()` 사용 (print() 대신)
   - 예: `logger.write(f"Downloaded: {title}")`

3. 실험 종료
   - `logger.close()` 필수 호출

### 실험 폴더 구조

PRD 문서 06_실험_추적_관리.md 참조

---

## Feature 브랜치

**1단계: 데이터 인프라 구축 (박재홍)**
- `1-1. feature/data-collection` - arXiv 데이터 수집
- `1-2. feature/document-processing` - Document Loader/Splitter
- `1-3. feature/database-setup` - PostgreSQL + pgvector 설정

---

## 참고 PRD 문서

개발 시 반드시 참고해야 할 PRD 문서 목록:

### 필수 참고 문서
1. [01_프로젝트_개요.md](../PRD/01_프로젝트_개요.md) - 프로젝트 전체 개요
2. [02_프로젝트_구조.md](../PRD/02_프로젝트_구조.md) - 폴더 구조 (data/, scripts/)
3. [05_로깅_시스템.md](../PRD/05_로깅_시스템.md) ⭐ - Logger 사용법
4. [06_실험_추적_관리.md](../PRD/06_실험_추적_관리.md) ⭐ - 실험 폴더 구조
5. [10_기술_요구사항.md](../PRD/10_기술_요구사항.md) - arXiv API, PyPDFLoader, OpenAI Embeddings
6. [11_데이터베이스_설계.md](../PRD/11_데이터베이스_설계.md) - papers 테이블 스키마
7. [13_RAG_시스템_설계.md](../PRD/13_RAG_시스템_설계.md) - Document 처리 및 Text Splitting

### 참고 문서
- [03_브랜치_전략.md](../PRD/03_브랜치_전략.md) - Feature 브랜치
- [04_일정_관리.md](../PRD/04_일정_관리.md) - 개발 일정

---

## 참고 자료

- arXiv API: https://info.arxiv.org/help/api/index.html
- Langchain Document Loaders: https://python.langchain.com/docs/integrations/document_loaders/
- Langchain Text Splitters: https://python.langchain.com/docs/modules/data_connection/document_transformers/
- Langchain Embeddings: https://python.langchain.com/docs/integrations/text_embedding/
- pgvector 문서: https://github.com/pgvector/pgvector

