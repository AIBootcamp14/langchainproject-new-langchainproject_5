#!/usr/bin/env python
"""
íŠ¹ì • ë…¼ë¬¸ì˜ ì„ë² ë”©ì„ ì¬ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python scripts/data/reembed_paper.py --paper-id 1
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import argparse
import psycopg2
from dotenv import load_dotenv
from langchain_core.documents import Document

from src.database.embeddings import get_embeddings
from src.database.vector_store import get_pgvector_store

load_dotenv()


def delete_paper_embeddings(paper_id: int, collection_name: str = "paper_chunks"):
    """
    íŠ¹ì • ë…¼ë¬¸ì˜ ì„ë² ë”© ì‚­ì œ
    
    Args:
        paper_id: ë…¼ë¬¸ ID
        collection_name: ì»¬ë ‰ì…˜ëª…
    """
    print(f"\nğŸ—‘ï¸  paper_id={paper_id}ì˜ ê¸°ì¡´ ì„ë² ë”© ì‚­ì œ ì¤‘...")
    
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    cursor = conn.cursor()
    
    # ì»¬ë ‰ì…˜ UUID ê°€ì ¸ì˜¤ê¸°
    cursor.execute("""
        SELECT uuid FROM langchain_pg_collection WHERE name = %s;
    """, (collection_name,))
    collection_uuid = cursor.fetchone()[0]
    
    # ì‚­ì œ ì „ ê°œìˆ˜ í™•ì¸
    cursor.execute("""
        SELECT COUNT(*)
        FROM langchain_pg_embedding
        WHERE collection_id = %s
        AND cmetadata->>'paper_id' = %s;
    """, (collection_uuid, str(paper_id)))
    before_count = cursor.fetchone()[0]
    
    print(f"   ì‚­ì œ ëŒ€ìƒ: {before_count}ê°œ ì²­í¬")
    
    # ì‚­ì œ ì‹¤í–‰
    cursor.execute("""
        DELETE FROM langchain_pg_embedding
        WHERE collection_id = %s
        AND cmetadata->>'paper_id' = %s;
    """, (collection_uuid, str(paper_id)))
    
    conn.commit()
    cursor.close()
    conn.close()
    
    print(f"   âœ… {before_count}ê°œ ì²­í¬ ì‚­ì œ ì™„ë£Œ")


def get_paper_chunks(paper_id: int):
    """
    papers í…Œì´ë¸”ì—ì„œ ë…¼ë¬¸ ì •ë³´ì™€ PDF í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        paper_id: ë…¼ë¬¸ ID
        
    Returns:
        ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\nğŸ“„ paper_id={paper_id}ì˜ ì›ë³¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT paper_id, title, authors, publish_date, url, category, 
               citation_count, full_text
        FROM papers
        WHERE paper_id = %s;
    """, (paper_id,))
    
    row = cursor.fetchone()
    
    if not row:
        cursor.close()
        conn.close()
        raise ValueError(f"paper_id={paper_id}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    paper_info = {
        "paper_id": row[0],
        "title": row[1],
        "authors": row[2],
        "publish_date": row[3],
        "url": row[4],
        "category": row[5],
        "citation_count": row[6],
        "full_text": row[7]
    }
    
    cursor.close()
    conn.close()
    
    print(f"   ì œëª©: {paper_info['title']}")
    print(f"   ì €ì: {paper_info['authors'][:100]}...")
    print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(paper_info['full_text']) if paper_info['full_text'] else 0} ê¸€ì")
    
    return paper_info


def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ê²¹ì¹¨
        
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
    )
    
    chunks = text_splitter.split_text(text)
    return chunks


def reembed_paper(paper_id: int, collection_name: str = "paper_chunks", 
                  chunk_size: int = 1000, chunk_overlap: int = 200):
    """
    ë…¼ë¬¸ ì„ë² ë”© ì¬ìƒì„±
    
    Args:
        paper_id: ë…¼ë¬¸ ID
        collection_name: ì»¬ë ‰ì…˜ëª…
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ê²¹ì¹¨
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“ paper_id={paper_id} ì„ë² ë”© ì¬ìƒì„± ì‹œì‘")
    print(f"{'='*80}")
    
    # 1. ê¸°ì¡´ ì„ë² ë”© ì‚­ì œ
    delete_paper_embeddings(paper_id, collection_name)
    
    # 2. ì›ë³¸ ë°ì´í„° ë¡œë“œ
    paper_info = get_paper_chunks(paper_id)
    
    if not paper_info['full_text']:
        print(f"\nâŒ paper_id={paper_id}ì— full_textê°€ ì—†ìŠµë‹ˆë‹¤!")
        return False
    
    # 3. í…ìŠ¤íŠ¸ ì²­í‚¹
    print(f"\nâœ‚ï¸  í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘ (í¬ê¸°: {chunk_size}, ê²¹ì¹¨: {chunk_overlap})...")
    chunks = chunk_text(paper_info['full_text'], chunk_size, chunk_overlap)
    print(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # 4. Document ê°ì²´ ìƒì„±
    print(f"\nğŸ“¦ Document ê°ì²´ ìƒì„± ì¤‘...")
    documents = []
    for i, chunk in enumerate(chunks):
        doc = Document(
            page_content=chunk,
            metadata={
                "paper_id": paper_info['paper_id'],
                "title": paper_info['title'],
                "authors": paper_info['authors'],
                "publish_date": str(paper_info['publish_date']) if paper_info['publish_date'] else None,
                "url": paper_info['url'],
                "category": paper_info['category'],
                "citation_count": paper_info['citation_count'],
                "chunk_index": i,
                "source": f"paper_{paper_id}"
            }
        )
        documents.append(doc)
    
    print(f"   âœ… {len(documents)}ê°œ Document ìƒì„±")
    
    # 5. ì„ë² ë”© ìƒì„± ë° ì €ì¥
    print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘ (ëª¨ë¸: text-embedding-3-small)...")
    print(f"   ì˜ˆìƒ ì‹œê°„: ì•½ {len(documents) * 0.5:.0f}ì´ˆ")
    
    vector_store = get_pgvector_store(collection_name)
    vector_store.add_documents(documents)
    
    print(f"   âœ… {len(documents)}ê°œ ì„ë² ë”© ì €ì¥ ì™„ë£Œ")
    
    # 6. ê²€ì¦
    print(f"\nğŸ” ì¬ìƒì„± ê²€ì¦ ì¤‘...")
    
    conn = psycopg2.connect(os.getenv("DATABASE_URL"))
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT uuid FROM langchain_pg_collection WHERE name = %s;
    """, (collection_name,))
    collection_uuid = cursor.fetchone()[0]
    
    cursor.execute("""
        SELECT COUNT(*)
        FROM langchain_pg_embedding
        WHERE collection_id = %s
        AND cmetadata->>'paper_id' = %s;
    """, (collection_uuid, str(paper_id)))
    after_count = cursor.fetchone()[0]
    
    cursor.close()
    conn.close()
    
    print(f"   ì €ì¥ëœ ì²­í¬ ìˆ˜: {after_count}ê°œ")
    
    if after_count == len(documents):
        print(f"   âœ… ê²€ì¦ ì„±ê³µ!")
    else:
        print(f"   âš ï¸  ì²­í¬ ìˆ˜ ë¶ˆì¼ì¹˜: ìƒì„±={len(documents)}, ì €ì¥={after_count}")
    
    # 7. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘...")
    results = vector_store.similarity_search_with_score(
        f"{paper_info['title']}", 
        k=5
    )
    
    found = False
    for i, (doc, score) in enumerate(results, 1):
        if doc.metadata.get('paper_id') == paper_id:
            found = True
            print(f"   âœ… [{i}ìœ„] paper_id={paper_id} ë°œê²¬! (L2 ê±°ë¦¬: {score:.4f})")
            break
    
    if not found:
        print(f"   âš ï¸  paper_id={paper_id}ê°€ Top-5ì— ì—†ìŠµë‹ˆë‹¤. ì¶”ê°€ í™•ì¸ í•„ìš”.")
    
    print(f"\n{'='*80}")
    print(f"âœ… paper_id={paper_id} ì„ë² ë”© ì¬ìƒì„± ì™„ë£Œ!")
    print(f"{'='*80}\n")
    
    return True


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë…¼ë¬¸ ì„ë² ë”© ì¬ìƒì„±")
    parser.add_argument("--paper-id", type=int, required=True, help="ë…¼ë¬¸ ID")
    parser.add_argument("--collection", type=str, default="paper_chunks", help="ì»¬ë ‰ì…˜ëª…")
    parser.add_argument("--chunk-size", type=int, default=1000, help="ì²­í¬ í¬ê¸°")
    parser.add_argument("--chunk-overlap", type=int, default=200, help="ì²­í¬ ê²¹ì¹¨")
    
    args = parser.parse_args()
    
    try:
        success = reembed_paper(
            paper_id=args.paper_id,
            collection_name=args.collection,
            chunk_size=args.chunk_size,
            chunk_overlap=args.chunk_overlap
        )
        
        if success:
            print("ğŸ‰ ì„±ê³µ!")
            sys.exit(0)
        else:
            print("âŒ ì‹¤íŒ¨!")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
