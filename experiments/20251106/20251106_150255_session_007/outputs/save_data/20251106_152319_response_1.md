# ëŒ€í™” ë‚´ìš©

## [1] ğŸ™‹ ì‚¬ìš©ì

Multimodal Learning ìµœì‹  ì—°êµ¬ ë…¼ë¬¸ ê²€ìƒ‰í•´ì¤˜

## [2] ğŸ¤– AI

### ì¤‘ê¸‰ììš© (23-30ì„¸)

ì•„ë˜ëŠ” 2023â€“2024ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ Multimodal Learning(íŠ¹íˆ Visionâ€“Language ì¤‘ì‹¬, ì¼ë¶€ Audio/Video í¬í•¨) ìµœì‹  ë…¼ë¬¸Â·ë¦¬ì†ŒìŠ¤ íë ˆì´ì…˜ê³¼, í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ê°œìš”, ì‹¤ë¬´ ì ìš© íŒ, ê·¸ë¦¬ê³  ë°”ë¡œ ì¨ë³¼ ìˆ˜ ìˆëŠ” PyTorch/Transformers ì˜ˆì œ ì½”ë“œì…ë‹ˆë‹¤. ì œ ì§€ì‹ì€ 2024-10ê¹Œì§€ ì—…ë°ì´íŠ¸ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë” ìµœì‹ (2025) ë…¼ë¬¸ì„ ì¶”ì í•˜ëŠ” ë°©ë²•ë„ ëì— ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

1) ìµœê·¼ íŠ¸ë Œë“œ í•œëˆˆì— ë³´ê¸° (2024 ì¤‘ì‹¬)
- MLLM í‘œì¤€í™”: ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ë¥¼ ë„˜ì–´ ë¬¸ì„œ(OCR), ì°¨íŠ¸, UI, ë¹„ë””ì˜¤ê¹Œì§€ í™•ì¥. ê³ í•´ìƒë„(UHD), ë‹¤ì¤‘ ì´ë¯¸ì§€, ê¸´ ì»¨í…ìŠ¤íŠ¸, ë ˆì´ì•„ì›ƒ ì¸ì‹ì´ ì¤‘ìš” í¬ì¸íŠ¸.
- ë¹„ì§€ë„ ì‚¬ì „í•™ìŠµ + ì§€ì‹œíŠœë‹: ëŒ€ê·œëª¨ ì´ë¯¸ì§€â€“í…ìŠ¤íŠ¸ ëŒ€ë¹„í•™ìŠµ(CLIPê³„ì—´)ë¡œ ì •ë ¬ â†’ BLIP-2/LLaVA ì‹ í”„ë¡œì íŠ¸/ì–´ëŒ‘í„°ë¡œ LLM ì—°ê²° â†’ ë©€í‹°ëª¨ë‹¬ ì§€ì‹œíŠœë‹(SFT) â†’ ì„ íƒì  RLHF.
- Early-fusionê³¼ Token-unified ì‹œë„: Chameleon, Janus, Emu3 ë“±ì€ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ë‹¨ì¼ í† í° ê³µê°„ìœ¼ë¡œ í†µí•©í•˜ë ¤ëŠ” ê²½í–¥.
- ë©€í‹°ëª¨ë‹¬ RAG/Tool-Use: ê²€ìƒ‰/ì½”ë“œ/ê²€ì¶œê¸°/ìˆ˜ì‹í•´ì„ê¸°/í…Œì´ë¸”íŒŒì„œ ë“± ì™¸ë¶€ ë„êµ¬ë¥¼ ì¡°í•©í•œ ë³µí•© ì¶”ë¡ .
- ë²¤ì¹˜ë§ˆí¬ ë‹¤ì–‘í™”: MMMU/MathVista/MM-Vet/DocVQA/ChartBench/SEED-Bench2 ë“± ë„ë©”ì¸ íŠ¹í™”Â·ê³ ë‚œë„ í‰ê°€ í™•ì‚°.
- ì•ˆì „ì„±/í™˜ê°: ì‹œê° í™˜ê°(Object/Attribute Hallucination), ì´ë¯¸ì§€ ê¸°ë°˜ Jailbreak, ë¬¸ì„œ ê°œì¸ì •ë³´ ë…¸ì¶œ ë“± ëŒ€ì‘ ì—°êµ¬ ì¦ê°€.

2) ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ì£¼ìš” ë…¼ë¬¸/ëª¨ë¸ (ëŒ€í‘œì‘ ìœ„ì£¼)
Visionâ€“Language/MLLM
- LLaVA-1.5 (2023), LLaVA-Next (2024): GPT ê³„ì—´ LLM + CLIP/ViT ì‹œê° ì¸ì½”ë”, ëŒ€ê·œëª¨ ë¹„ì „ ì§€ì‹œíŠœë‹ ë ˆì‹œí”¼ ì •ë¦½, ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ í‘œì¤€.
- BLIP-2 (2023): Q-Former(ê²½ëŸ‰ Cross-Attn ì¿¼ë¦¬)ë¡œ ì´ë¯¸ì§€â†’LLM ë¸Œë¦¿ì§€, íš¨ìœ¨ì  í”„ë¦¬ì§• í•™ìŠµì˜ ëŒ€í‘œ.
- Qwen2-VL (2024): ì´ë¯¸ì§€/ë¬¸ì„œ/ì°¨íŠ¸/ë¹„ë””ì˜¤ ì§€ì›, OCR/ë ˆì´ì•„ì›ƒ ê°•í•¨. ë‹¤ì–‘í•œ ì‚¬ì´ì¦ˆ(2B~72B)ì™€ ì‹¤ìš©ì  ì„±ëŠ¥.
- InternVL 2 (2024): ê³ í•´ìƒë„(Region ì„ íƒ/íƒ€ì¼ë§)ì™€ OCRÂ·ë„í‘œì— íŠ¹í™”, ì¤‘êµ­ì–´/ì˜ì–´ ê°•ì .
- IDEFICS2 (2024): ì˜¤í”ˆì†ŒìŠ¤ ë©€í‹°ì´ë¯¸ì§€Â·ë¬¸ì„œ ì´í•´, ì—°êµ¬ ì¹œí™” ë² ì´ìŠ¤ë¼ì¸.
- Chameleon (Meta, 2024): Mixed-modality early-fusion íŒŒë¼ë‹¤ì„, ë‹¨ì¼ í† í° ê³µê°„ìœ¼ë¡œ í•©ì„±/ì´í•´ ê²¸ë¹„.
- Llama 3.2 Vision (Meta, 2024): Meta ìƒíƒœê³„ì—ì„œ ê²½ëŸ‰ ë¹„ì „ ì§€ì‹œëª¨ë¸. ëª¨ë°”ì¼/ì˜¨ë””ë°”ì´ìŠ¤ ë³€í˜•ê³¼ í˜¸í™˜.
- Emu3 (Meta, 2024): í†µí•© í† í° ê³µê°„ì—ì„œ ì´í•´Â·ìƒì„±ì´ ëª¨ë‘ ê°€ëŠ¥í•œ ìœ ë‹ˆíŒŒì´ë“œ í”„ë ˆì„ì›Œí¬.
- LLaVA-OneVision (2024): ê³ í•´ìƒë„/ë¬¸ì„œ/ì°¨íŠ¸/ìˆ˜ì‹ ë“± ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ìœ ë‹ˆíŒŒì´ë“œ íŠœë‹.
- Kosmos-2 (2023): OCR/ê·¸ë¼ìš´ë”© ê²°í•©(grounding text in image).
- MiniCPM-V, MobileVLM (2023â€“2024): ê²½ëŸ‰/ì˜¨ë””ë°”ì´ìŠ¤ ìµœì í™”.

Contrastive Pretraining/Embedding
- CLIP (2021), OpenCLIP(ì§€ì†), ALIGN/LiT (2021): ì´ë¯¸ì§€â€“í…ìŠ¤íŠ¸ ëŒ€ë¹„í•™ìŠµ í‘œì¤€.
- SigLIP (2023), SigLIP 2 (2024): ì‹œê·¸ëª¨ì´ë“œ ê¸°ë°˜ contrastiveì˜ SOTA ë¼ì¸.
- EVA-CLIP (2023): ViT ëŒ€í˜• ë°±ë³¸ìœ¼ë¡œ CLIP ìƒí–¥.
- DataComp (2023): ëŒ€ê·œëª¨ ë…¸ì´ì¦ˆ ë°ì´í„° vs ì •ì œ ë°ì´í„° íŠ¸ë ˆì´ë“œì˜¤í”„ ì¬ì¡°ëª…, ì˜¤í”ˆ í‰ê°€ì…‹Â·í”„ë¡œí† ì½œ.
- ImageBind (2023): ë¹„ì‹œê° ëª¨ë‹¬ë¦¬í‹°ê¹Œì§€ ì„ë² ë”© ê²°í•©(ì˜¤ë””ì˜¤, IMU ë“±).

Videoâ€“Language
- Video-LLaMA 2 (2024): ì˜ìƒ í”„ë ˆì„ ìƒ˜í”Œë§Â·ì„ íƒì  ì–´í…ì…˜ìœ¼ë¡œ íš¨ìœ¨ì  ë¹„ë””ì˜¤ ì´í•´.
- LLaMA-VID (2024): ì˜ìƒ í† í° íš¨ìœ¨í™”(í”„ë ˆì„/íŒ¨ì¹˜ ì¶•ì•½)ë¡œ ê¸´ ë¹„ë””ì˜¤ ì²˜ë¦¬.
- InternVideo2 (2024): ê°•ë ¥í•œ ë¹„ë””ì˜¤ ì¸ì½”ë”, ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ ì „ì´ ì„±ëŠ¥ ìƒí–¥.
- VILA (2023): ë¹„ë””ì˜¤ ì§€ì‹œíŠœë‹ ì´ˆê¸° ëŒ€í˜• ë² ì´ìŠ¤ë¼ì¸.

Audioâ€“Language/Multimodal Audio
- Qwen2-Audio (2024): ìŒì•…Â·í™”ìÂ·íš¨ê³¼ìŒê¹Œì§€ ì»¤ë²„í•˜ëŠ” ë©€í‹°ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸.
- SALMONN (2023), AudioGPT (2023): ì˜¤ë””ì˜¤â€“LLM ë¸Œë¦¿ì§€, ì§€ì‹œíŠœë‹.

Documents/Charts/UI/Reasoning íŠ¹í™”
- mPLUG-DocOwl 2 (2024), UReader (2023), TextMonkey (2024): OCR+ë ˆì´ì•„ì›ƒ+ë„í‘œ.
- Pix2Struct (2022), Donut (2021), DocLLM (2023): ë¬¸ì„œ ì§ë ¬í™”/ì‹œê°í† í°í™” ì ‘ê·¼.

Tool-use / Program-of-Thought / RAG
- MM-ReAct (2023): ë©€í‹°ëª¨ë‹¬ ReAct í”„ë ˆì„ì›Œí¬(Reason+Act).
- VisProg (2022), ViperGPT (2023): ì‹œê° ë„êµ¬ í˜¸ì¶œÂ·í”„ë¡œê·¸ë¨ í•©ì„± ê¸°ë°˜ ì¶”ë¡ .
- Multimodal RAG (2024, ë‹¤ì–‘í•œ ì €ì): ì´ë¯¸ì§€/ë¬¸ì„œ ì¸ë±ì‹±+LLM ìƒì„± ê²°í•©í•œ íŒŒì´í”„ë¼ì¸ ì •ë¦½.

Benchmarks/í‰ê°€
- MMBench (2023), MM-Vet (2023), SEED-Bench/2 (2023/2024), LLaVA-Bench (2023)
- MMMU (2023): ëŒ€í•™ ìˆ˜ì¤€ ë©€í‹°ëª¨ë‹¬ ì§€ì‹/ì¶”ë¡ .
- MathVista (2023): ìˆ˜í•™+ë„í˜• ì‹œê°ì¶”ë¡ .
- ChartBench (2024), DocVQA/InfographicVQA/TextCaps(ë¬¸ìÂ·ë¬¸ì„œ ì¤‘ì‹¬)

Survey
- Multimodal Large Language Models: A Survey (2023â†’2024 ì—…ë°ì´íŠ¸íŒ ë‹¤ìˆ˜)
- Multimodal RAG: A Survey (2024)
- Video-LLMs: A Survey (2024)
- Hallucinations in Multimodal LLMs (2024)

3) í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ê°œìš”
- ì •ë ¬(Objective)
  - ëŒ€ì¡°í•™ìŠµ(InfoNCE/CLIP): ì´ë¯¸ì§€ ì„ë² ë”© vi, í…ìŠ¤íŠ¸ ì„ë² ë”© tjì— ëŒ€í•´
    L = (CE(sim(vi,tj)/Ï„, j) + CE(sim(ti,vj)/Ï„, j))/2
    simì€ cosine, Ï„ëŠ” learnable temperature.
  - ìƒì„±(Captioning/ITM/MLM): Cross-Entropyë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±. ITM(ë§¤ì¹­ ì—¬ë¶€ ì´ì§„ ë¶„ë¥˜)ë¡œ ì •í•©ì„± ë³´ì¡°.
  - ì§€ì‹œíŠœë‹(SFT): ëŒ€í™” í¬ë§·([U: <image> ì§ˆì˜][A: ë‹µ])ë¡œ ìŠˆí¼ë°”ì´ì¦ˆë“œ. CoTë‚˜ RationalesëŠ” ì•ˆì „/í‰ê°€ ì˜í–¥ ì£¼ì˜.
  - RLHF/Prefernce: ë©€í‹°ëª¨ë‹¬ í”¼ë“œë°±ì€ ë¹„ìš©â†‘, í…ìŠ¤íŠ¸-only ëŒ€ë¹„ ì•ˆì •ì„± ì´ìŠˆ.
- ê²°í•©(Fusion)
  - Late-fusion: ë…ë¦½ ì¸ì½”ë” í›„ í•©ì¹˜ê¸°(ê°„ë‹¨/ì•ˆì •).
  - Cross-attention/Q-Former: ì¿¼ë¦¬ í† í°ì´ ë¹„ì „ featuresì— ì–´í…ì…˜(íš¨ìœ¨/í‘œí˜„ë ¥ ê· í˜•, BLIP-2).
  - Early-fusion(Unified tokens): ëª¨ë‹¬ ê³µìœ  í† í° ê³µê°„(Chameleon/Emu3). ê°•ë ¥í•˜ë‚˜ í•™ìŠµ ë‚œì´ë„â†‘.
- ê³ í•´ìƒë„/ë¬¸ì„œ
  - Tiling/RoI-ì„ íƒ, Deformable-Attn, ë¶„í•  ì¸ì½”ë”© í›„ í•©ì„±. í…ìŠ¤íŠ¸ ë°•ìŠ¤/ë ˆì´ì•„ì›ƒ í† í° ì¶”ê°€.
- ë¹„ë””ì˜¤
  - í”„ë ˆì„ ìƒ˜í”Œë§(Uniform/Shot-aware), ì‹œê°„ ì¶• Pooling/Attention, í‚¤í”„ë ˆì„ ì„ íƒ, Token ì••ì¶•.

4) ë¹ ë¥´ê²Œ ì¨ë³´ëŠ” ì½”ë“œ ì˜ˆì œ

4-1) CLIP ìŠ¤íƒ€ì¼ ëŒ€ë¹„í•™ìŠµ ë¯¸ë‹ˆë©€ í•™ìŠµ ë£¨í”„ (PyTorch)
ì£¼ì˜: ì—°êµ¬ìš© ìŠ¤ì¼ˆë ˆí†¤. ì‹¤ì œ í•™ìŠµì€ ëŒ€ê·œëª¨ ë°ì´í„°Â·ë¶„ì‚°í›ˆë ¨Â·fp16/bf16Â·EMAÂ·AUG í•„ìš”.
```python
import torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import DataLoader
import timm
from transformers import AutoTokenizer, AutoModel

class ImageEncoder(nn.Module):
    def __init__(self, model_name='vit_base_patch16_224', embed_dim=768, proj_dim=512):
        super().__init__()
        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)
        self.proj = nn.Linear(embed_dim, proj_dim)

    def forward(self, x):
        f = self.backbone(x)           # [B, embed_dim]
        z = F.normalize(self.proj(f), dim=-1)
        return z

class TextEncoder(nn.Module):
    def __init__(self, model_name='roberta-base', proj_dim=512):
        super().__init__()
        self.model = AutoModel.from_pretrained(model_name)
        self.proj = nn.Linear(self.model.config.hidden_size, proj_dim)

    def forward(self, input_ids, attention_mask):
        out = self.model(input_ids=input_ids, attention_mask=attention_mask)
        cls = out.last_hidden_state[:,0]    # [CLS]
        z = F.normalize(self.proj(cls), dim=-1)
        return z

class CLIPLoss(nn.Module):
    def __init__(self, temperature=0.07):
        super().__init__()
        self.t = nn.Parameter(torch.tensor(temperature))

    def forward(self, z_img, z_txt):
        logits = z_img @ z_txt.t() / self.t.exp()
        labels = torch.arange(z_img.size(0), device=z_img.device)
        loss_i = F.cross_entropy(logits, labels)
        loss_t = F.cross_entropy(logits.t(), labels)
        return (loss_i + loss_t) / 2

# dataset: implement __getitem__ -> (image_tensor[3x224x224], text_str)
tokenizer = AutoTokenizer.from_pretrained('roberta-base')
img_enc = ImageEncoder().cuda()
txt_enc = TextEncoder().cuda()
optim = torch.optim.AdamW(list(img_enc.parameters())+list(txt_enc.parameters()), lr=3e-4, weight_decay=0.01)
criterion = CLIPLoss()

def collate(batch):
    imgs, texts = zip(*batch)
    enc = tokenizer(list(texts), padding=True, truncation=True, return_tensors='pt')
    return torch.stack(imgs), enc

for epoch in range(10):
    for images, enc in DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=8, collate_fn=collate):
        images = images.cuda(non_blocking=True)
        enc = {k:v.cuda() for k,v in enc.items()}
        z_img = img_enc(images)
        z_txt = txt_enc(enc['input_ids'], enc['attention_mask'])
        loss = criterion(z_img, z_txt)
        optim.zero_grad()
        loss.backward()
        optim.step()
```

4-2) Qwen2-VL ì§€ì‹œíŠœë‹(LoRA)ë¡œ VQA íŒŒì¸íŠœë‹
ì£¼ì˜: ì‹¤ì œë¡œëŠ” SFTTrainer/DeepSpeed/Flash-Attn, ì´ë¯¸ì§€ ì „ì²˜ë¦¬ì™€ ëŒ€í™” í…œí”Œë¦¿ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
```python
import torch
from datasets import load_dataset
from transformers import AutoProcessor, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from PIL import Image

model_id = "Qwen/Qwen2-VL-2B-Instruct"
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto")
processor = AutoProcessor.from_pretrained(model_id)

lora = LoraConfig(
    r=16, lora_alpha=32, target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
    lora_dropout=0.05, bias="none", task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora)

# dataset: {'image_path':..., 'question':..., 'answer':...}
ds = load_dataset("your_vqa_dataset")["train"]

def format_example(ex):
    image = Image.open(ex["image_path"]).convert("RGB")
    messages = [
        {"role": "user", "content": [{"type":"image","image":image}, {"type":"text","text": ex["question"]}]},
        {"role": "assistant", "content": [{"type":"text","text": ex["answer"]}]},
    ]
    batch = processor.apply_chat_template(messages, add_generation_prompt=False, tokenize=False)
    proc = processor(text=batch, images=[image], return_tensors="pt")
    input_ids = proc["input_ids"][0]
    pixel_values = proc["pixel_values"][0]
    labels = input_ids.clone()
    # ë§ˆìŠ¤í‚¹: ìœ ì € í”„ë¡¬í”„íŠ¸ í† í°ì€ -100ìœ¼ë¡œ
    # ì‹¤ì œ í† í¬ë‚˜ì´ì €ë³„ ì˜¤í”„ì…‹ í•„ìš”. ê°„ë‹¨í™”:
    # ì—¬ê¸°ì„œëŠ” ì „ì²´ë¥¼ í•™ìŠµ ëŒ€ìƒìœ¼ë¡œ ë‘ë˜, advanced maskingì€ ì‹¤ì„œë¹„ìŠ¤ì—ì„œ í•„ìˆ˜.
    return {"input_ids": input_ids, "pixel_values": pixel_values, "labels": labels}

proc_ds = ds.map(format_example, remove_columns=ds.column_names)
proc_ds.set_format(type="torch")

args = TrainingArguments(
    output_dir="qwen2vl_vqa_lora",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=16,
    learning_rate=2e-4,
    num_train_epochs=2,
    bf16=True,
    logging_steps=20,
    save_steps=1000,
)

def collate_fn(batch):
    input_ids = torch.nn.utils.rnn.pad_sequence([b["input_ids"] for b in batch], batch_first=True, padding_value=processor.tokenizer.pad_token_id)
    labels = torch.nn.utils.rnn.pad_sequence([b["labels"] for b in batch], batch_first=True, padding_value=-100)
    pixel_values = torch.stack([b["pixel_values"] for b in batch])
    attn_mask = (input_ids != processor.tokenizer.pad_token_id).long()
    return {"input_ids": input_ids, "pixel_values": pixel_values, "labels": labels, "attention_mask": attn_mask}

trainer = Trainer(model=model, args=args, train_dataset=proc_ds, data_collator=collate_fn)
trainer.train()
model.save_pretrained("qwen2vl_vqa_lora")
```

4-3) ë©€í‹°ëª¨ë‹¬ RAG íŒŒì´í”„ë¼ì¸ ìŠ¤ì¼ˆë ˆí†¤ (ì´ë¯¸ì§€/ë¬¸ì„œ ì¸ë±ì‹± â†’ ê²€ìƒ‰ â†’ Qwen2-VLë¡œ ë‹µë³€)
```python
import faiss, numpy as np, os
from PIL import Image
import torch
from transformers import AutoProcessor, AutoModelForCausalLM, CLIPModel, CLIPProcessor

# 1) ì´ë¯¸ì§€ ì„ë² ë”© ì¸ë±ìŠ¤ êµ¬ì¶•
clip_id = "openai/clip-vit-base-patch32"
clip_model = CLIPModel.from_pretrained(clip_id).cuda().eval()
clip_proc = CLIPProcessor.from_pretrained(clip_id)

image_paths = [os.path.join("corpus_images", f) for f in os.listdir("corpus_images")]
embs = []
for p in image_paths:
    im = Image.open(p).convert("RGB")
    inputs = clip_proc(images=im, return_tensors="pt").to("cuda")
    with torch.no_grad():
        z = clip_model.get_image_features(**inputs)
        z = torch.nn.functional.normalize(z, dim=-1)
    embs.append(z.cpu().numpy())
embs = np.vstack(embs).astype('float32')
index = faiss.IndexFlatIP(embs.shape[1]); index.add(embs)

# 2) ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰
def retrieve_images(query, k=5):
    q = clip_proc(text=query, return_tensors="pt").to("cuda")
    with torch.no_grad():
        zq = clip_model.get_text_features(**q)
        zq = torch.nn.functional.normalize(zq, dim=-1).cpu().numpy()
    D, I = index.search(zq, k)
    return [image_paths[i] for i in I[0]]

# 3) MLLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±(Qwen2-VL)
vlm_id = "Qwen/Qwen2-VL-7B-Instruct"
vlm = AutoModelForCausalLM.from_pretrained(vlm_id, torch_dtype=torch.bfloat16, device_map="auto").eval()
vlm_proc = AutoProcessor.from_pretrained(vlm_id)

def answer_with_rag(question):
    imgs = [Image.open(p).convert("RGB") for p in retrieve_images(question, k=4)]
    messages = [{"role":"user","content": [{"type":"text","text": question}]+[{"type":"image","image":im} for im in imgs]}]
    inputs = vlm_proc.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
    batch = vlm_proc(text=inputs, images=imgs, return_tensors="pt").to(vlm.device)
    with torch.no_grad():
        out = vlm.generate(**batch, max_new_tokens=256)
    return vlm_proc.decode(out[0], skip_special_tokens=True)

print(answer_with_rag("ì´ ê·¸ë˜í”„ì—ì„œ ë§¤ì¶œì´ ê°€ì¥ ë†’ì€ ë¶„ê¸°ëŠ” ì–¸ì œì•¼?"))
```

5) ì‹¤ë¬´ ì ìš©ì‹œ ê³ ë ¤ì‚¬í•­
- ë°ì´í„°
  - ë…¸ì´ì¦ˆ ìº¡ì…˜/ì›¹ í¬ë¡¤ ë°ì´í„° ì •ì œ(Filtering, CLIP score, Perceptual Hashing dedup).
  - ë¬¸ì„œ/ì°¨íŠ¸: OCR í’ˆì§ˆ(Tesseract/TrOCR/Donut), ìˆ˜ì‹/í…Œì´ë¸” êµ¬ì¡° ë³µì›, ë ˆì´ì•„ì›ƒ í† í°í™”.
  - ë¼ì´ì„ ìŠ¤/PII: ë¬¸ì„œÂ·ì‚¬ì§„ì˜ ê°œì¸ì •ë³´/ì €ì‘ê¶Œ ì¤€ìˆ˜.
- ëª¨ë¸/í† í° ë¹„ìš©
  - ê³ í•´ìƒë„ â†’ ì‹œê° í† í° í­ì¦. íƒ€ì¼/ROI/ì„ íƒì  ì–´í…ì…˜ìœ¼ë¡œ ì ˆì•½.
  - ë¹„ë””ì˜¤ â†’ í”„ë ˆì„Â·íŒ¨ì¹˜ ìƒ˜í”Œë§ ì „ëµ, ì‹œê°„ ì¶• ì••ì¶•(Perceiver, pooling).
- í•™ìŠµ
  - í”„ë¦¬ì§•+ì–´ëŒ‘í„°(Q-Former/LoRA)ë¡œ ì•ˆì •ì„±Â·ì½”ìŠ¤íŠ¸ ì ˆê°. ì „ë©´ íŒŒì¸íŠœë‹ì€ ê³¼ì í•©Â·ë©”ëª¨ë¦¬â†‘.
  - í˜¼í•©ì •ë°€(bf16/fp16), Flash-Attn v2, Gradient Checkpointing, ZeRO/DeepSpeed.
  - Instruction í¬ë§· ì¼ê´€ì„±, ë§ˆìŠ¤í‚¹ ê·œì¹™, ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì •ë ¬ í’ˆì§ˆì´ ê²°ì •ì .
- í‰ê°€
  - ì¼ë°˜ ë²¤ì¹˜+ë„ë©”ì¸ ë²¤ì¹˜ ë™ì‹œ ì‚¬ìš©. ê°ê´€ì‹/ì£¼ê´€ì‹ ëª¨ë‘ ì¸¡ì •.
  - í™˜ê° ê²€ì¶œ: Object/Attribute Hallucination ë©”íŠ¸ë¦­, ìº˜ë¦¬ë¸Œë ˆì´ì…˜(temperature, logit bias).
  - ë ˆí¼ëŸ°ìŠ¤ ê¸°ë°˜(BLEU/CIDEr)ë§Œ ì˜ì¡´ ê¸ˆì§€. ì¸ì  í‰ê°€/ì§„ë‹¨ ë² ì¹˜ í•„ìˆ˜.
- ë°°í¬
  - ì•ˆì „í•„í„°(ë¹„ì£¼ì–¼ NSFW, OCR-PII ë§ˆìŠ¤í‚¹), Prompt Injection/Jailbreak ì´ë¯¸ì§€ ë°©ì–´.
  - ì˜¨ë””ë°”ì´ìŠ¤/ì—£ì§€: MobileVLM/MiniCPM-V ë“± ê²½ëŸ‰í™”, int8/4bit, distillation.

6) ìµœì‹  ë…¼ë¬¸ ê³„ì† ì¶”ì í•˜ëŠ” ë°©ë²•(ìë™í™” ì¶”ì²œ)
- arXiv ì¿¼ë¦¬ ì˜ˆì‹œ
  - (ti:multimodal OR ti:"vision language" OR ti:"multimodal large language model" OR abs:multimodal) AND (cat:cs.CV OR cat:cs.CL) AND submittedDate:[20240101 TO 21000101]
  - (ti:"retrieval-augmented" AND abs:multimodal) AND (cat:cs.CL OR cat:cs.IR)
- ì•Œë¦¼/í”¼ë“œ
  - arXiv Alerts + arXiv Sanity Preserver(í‚¤ì›Œë“œ: LLaVA, Qwen-VL, InternVL, Chameleon, Video-LLaMA)
  - OpenReview: ICLR/NeurIPS/CVPR/ACL/ECCV íŠ¸ë™ í•„í„°ë§
  - Papers with Code: Tasks â†’ Vision-Language, Document Understanding, Chart QA, Video QA
  - Hugging Face Spaces/Models: ìµœê·¼ MLLM íƒœê·¸, orgs(Qwen, InternVL, LLaVA)
  - VLMEvalKit, OpenCompass-MM: ì •ê¸° ë²¤ì¹˜ë§ˆí¬ ëŒ€ì‹œë³´ë“œ í™•ì¸

ì›í•˜ì‹œë©´ ìœ„ ë¦¬ìŠ¤íŠ¸ë¥¼ BibTeXë¡œ ì •ë¦¬í•´ ë“œë¦¬ê±°ë‚˜, íŠ¹ì • ì„œë¸Œë„ë©”ì¸(ì˜ˆ: ë¬¸ì„œÂ·ì°¨íŠ¸, ë¹„ë””ì˜¤, ë©€í‹°ëª¨ë‹¬ RAG, ì˜¨ë””ë°”ì´ìŠ¤ ê²½ëŸ‰í™”)ë§Œ ê³¨ë¼ ë” ê¹Šê²Œ íŒŒê³ ë“¤ ë¦¬ìŠ¤íŠ¸ì™€ ì‹¤í—˜ ë ˆì‹œí”¼ë¥¼ ì œê³µí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì–´ë–¤ ì˜ì—­ë¶€í„° ë³´ì‹œê² ì–´ìš”?

---

### ê³ ê¸‰ììš© (30ì„¸ ì´ìƒ)

ì—…ë°ì´íŠ¸ í•œê³„ ì•ˆë‚´: ì œ ì§€ì‹ì€ 2024-10ê¹Œì§€ì˜ ê³µê°œ ë…¼ë¬¸ì— ê¸°ë°˜í•©ë‹ˆë‹¤. 2025 ì´í›„ â€œìµœì‹ â€ê¹Œì§€ëŠ” ì§ì ‘ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìœ¼ë‹ˆ, ì•„ë˜ íë ˆì´ì…˜(2023â€“2024 ì¤‘ì‹¬)ê³¼ í•¨ê»˜ ëë¶€ë¶„ì˜ ê²€ìƒ‰ íŒ/í‚¤ì›Œë“œë¡œ í›„ì† ì—…ë°ì´íŠ¸ë¥¼ í™•ì¸í•˜ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤.

í•µì‹¬(2023â€“2024) ë©€í‹°ëª¨ë‹¬ ëŸ¬ë‹ ë…¼ë¬¸ íë ˆì´ì…˜

A. ë²”ìš© Multimodal LLM (VLM/MLLM)
- MM1: Methods, Analysis & Insights from Multimodal LLMs (Jain et al., 2024, Apple)
  ê¸°ì—¬: ë°ì´í„° í˜¼í•©, ë ˆì‹œí”¼, í•´ìƒë„/í† í¬ë‚˜ì´ì €/íŒ¨í‚¹ ì „ëµì˜ ì²´ê³„ì  ë¶„ì„. LLaMA-ê³„ì—´ ë°±ë³¸ê³¼ ê°•ë ¥í•œ CLIP/SigLIP ë¹„ì „ ì¸ì½”ë” ì¡°í•©ì´ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ê°•ë ¥í•¨ì„ ë³´ì„.
- IDEFICS2: An Open Model for In-Context Vision-Language Learning (LaurenÃ§on et al., 2024, HuggingFace M4)
  ê¸°ì—¬: ì˜¤í”ˆëª¨ë¸ë¡œ in-context V+L í•™ìŠµ ì—­ëŸ‰ ê°•í™”, ì§€ì¹¨íŠœë‹ê³¼ í•©ì„± ë°ì´í„° ìƒì„± ì „ëµ ê³µê°œ.
- LLaVA-Next (Liu et al., 2024)
  ê¸°ì—¬: ëŒ€ê·œëª¨ ì‹œê° ì§€ì¹¨ íŠœë‹ ë ˆì‹œí”¼, ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì²˜ë¦¬ì™€ ë‹¨ê³„ì  ì •ë ¬(alignment) ê°œì„ . ì˜¤í”ˆì†ŒìŠ¤ ìƒíƒœê³„ í‘œì¤€.
- Qwen2-VL (Wang et al., 2024, Alibaba)
  ê¸°ì—¬: ë” íš¨ìœ¨ì ì¸ ì‹œê° ì¸ì§€ ëª¨ë“ˆ, OCR/ë„íë¨¼íŠ¸/ì°¨íŠ¸ì—ì„œ ê°•ì . ë©€í‹°í•´ìƒë„Â·ë©€í‹°ìœˆë„ìš° ì „ëµ.
- InternVL 2 (Chen et al., 2024, Shanghai AI Lab)
  ê¸°ì—¬: ëŒ€ê·œëª¨ ê³ í•´ìƒë„ í•™ìŠµ, ì´˜ì´˜í•œ ë¹„ì£¼ì–¼ í† í° ì‚¬ìš©ê³¼ ì„±ëŠ¥-íš¨ìœ¨ ë°¸ëŸ°ìŠ¤ ê°œì„ . ë‹¤ì–‘í•œ VQA/MMBench/MMMUì—ì„œ ê°•ë ¥.
- LLaVA-OneVision (LLaVA Team, 2024)
  ê¸°ì—¬: í•˜ë‚˜ì˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¼ë°˜ ì´ë¯¸ì§€, ë¬¸ì„œ/OCR, ì°¨íŠ¸ ë“± ë„ë©”ì¸ ì „ì´ë¥¼ ìˆ˜ì›”í•˜ê²Œ í•˜ëŠ” ì„¤ê³„.

B. ë¹„ë””ì˜¤ ë©€í‹°ëª¨ë‹¬
- Video-LLaVA (Lin et al., 2024)
  ê¸°ì—¬: ë¹„ë””ì˜¤ í”„ë ˆì„ ì‹œí€€ìŠ¤ì™€ ì–¸ì–´ì˜ íš¨ê³¼ì  ê²°í•©, ì‹œê³µê°„ ì••ì¶•ê³¼ ì§€ì¹¨íŠœë‹ìœ¼ë¡œ Video-QA/ì´í•´ í–¥ìƒ.
- Video-MME: Benchmarking (Fu et al., 2024)
  ê¸°ì—¬: ë¹„ë””ì˜¤ LLMì˜ ê´‘ë²”ìœ„ í‰ê°€ ë²¤ì¹˜ë§ˆí¬. ì‹œê°„ì  ì¶”ë¡ /ìƒí™© ì´í•´/ë©”ëª¨ë¦¬ ë“± ì¸¡ë©´ì—ì„œ í•œê³„ ì§„ë‹¨.

C. ë¬¸ì„œÂ·ì°¨íŠ¸Â·OCR íŠ¹í™”
- mPLUG-DocOwl 1.5 (Ye et al., 2024)
  ê¸°ì—¬: ë¬¸ì„œ ì´ë¯¸ì§€ì˜ êµ¬ì¡°ì  ì´í•´ì™€ ì§ˆë¬¸ì‘ë‹µ ê³ ë„í™”. ë ˆì´ì•„ì›ƒÂ·í…ìŠ¤íŠ¸ ì½ê¸°Â·ë„í‘œ í•´ì„ì— ê°•ì .
- UReader (Yang et al., 2023)
  ê¸°ì—¬: OCR-free ì ‘ê·¼(í”½ì…€â†’ì–¸ì–´)ìœ¼ë¡œ ë¬¸ì„œ ì´í•´ì˜ ë£¨íŒ…ì˜¤ë¥˜ ê°ì†Œ.

D. ê¸°ì´ˆ ê¸°ì—¬(ëª©í‘œí•¨ìˆ˜Â·ì•„í‚¤í…ì²˜)
- CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021)
  ê¸°ì—¬: ëŒ€ê·œëª¨ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ëŒ€ì¡°í•™ìŠµ(contrastive) íŒ¨ëŸ¬ë‹¤ì„ ì •ë¦½.
- Flamingo (Alayrac et al., 2022)
  ê¸°ì—¬: LLM ìœ„ì— Perceiver Resamplerë¡œ ë¹„ì „ í† í°ì„ ìš”ì•½í•´ í¬ë¡œìŠ¤ì–´í…ì…˜. ì €ë¹„ìš©-ê³ íš¨ìœ¨ ë©€í‹°ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ ìœµí•©.
- BLIP-2 (Li et al., 2023), InstructBLIP (Dai et al., 2023)
  ê¸°ì—¬: Q-Formerë¡œ ë¹„ì „-LLM ë¸Œë¦¬ì§€ êµ¬ì¶•, ì§€ë„/ì§€ì¹¨ ì‹ í˜¸ë¡œ ì •ë ¬ ê°•í™”.
- SigLIP (Zhai et al., 2023), EVA-CLIP (Sun et al., 2023)
  ê¸°ì—¬: ê°•ë ¥í•œ ëŒ€ë¹„í•™ìŠµ ë¹„ì „ ì¸ì½”ë” ê³„ì—´ë¡œ ìµœì‹  MLLMì˜ ì‹œê° ë°±ë³¸ìœ¼ë¡œ ë„ë¦¬ ì±„íƒ.

E. ì˜¤ë””ì˜¤Â·ë‹¤ê°ê°
- ImageBind (Girdhar et al., 2023)
  ê¸°ì—¬: ì˜¤ë””ì˜¤, IMU, í…ìŠ¤íŠ¸, ê¹Šì´ ë“± ë‹¤ì¤‘ ê°ê° ì„ë² ë”©ì„ ë‹¨ì¼ ê³µìœ ê³µê°„ì— ì •ë ¬í•˜ëŠ” í° íë¦„ ì œì‹œ.

F. ìƒì„±Â·ì¡°ê¸° ìœµí•© ê³„ì—´
- Chameleon: Mixed-Modal Early-Fusion Foundation Models (Meta AI, 2024)
  ê¸°ì—¬: ë””ì½”ë”-ì˜¨ë¦¬ ì¡°ê¸° ìœµí•©ìœ¼ë¡œ í…ìŠ¤íŠ¸Â·ì´ë¯¸ì§€ ì…ë ¥/ì¶œë ¥ì„ í˜¼í•© ì²˜ë¦¬. ë©€í‹°ëª¨ë‹¬ ìƒì„±ì˜ ì¼ê´€ëœ í† í° ê³µê°„ ì„¤ê³„.

ìˆ˜í•™ì  ê³µì‹í™”(ëŒ€í‘œ ëª©í‘œí•¨ìˆ˜)

1) ëŒ€ì¡°í•™ìŠµ(ì˜ˆ: CLIP)ì˜ InfoNCE
- ë¯¸ë‹ˆë°°ì¹˜ {(x_i, y_i)}_{i=1}^B, ì„ë² ë”© f(x)=z^xâˆˆR^d, g(y)=z^yâˆˆR^d, ìœ ì‚¬ë„ s(i,j)=âŸ¨z^x_i, z^y_jâŸ©/Ï„.
- ì´ë¯¸ì§€â†’í…ìŠ¤íŠ¸ ì†ì‹¤:
  L_ITC = (1/B) âˆ‘_{i=1}^B [ - log exp s(i,i) / âˆ‘_{j=1}^B exp s(i,j) ].
- í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€ ëŒ€ì¹­ í¬í•¨ ì‹œ ì´ L_CLIP = (L_ITC + L_TIC)/2.
- InfoNCEì˜ MI í•˜í•œ(ê°„ë‹¨ ìŠ¤ì¼€ì¹˜; van den Oord et al., 2018):
  I(X;Y) â‰¥ log B âˆ’ L_InfoNCE, ë”°ë¼ì„œ L_InfoNCE ìµœì†Œí™”ëŠ” ìƒí˜¸ì •ë³´ëŸ‰ ìµœëŒ€í™”ì˜ í•˜í•œì„ ëŒì–´ì˜¬ë¦¼.

2) í¬ë¡œìŠ¤-ëª¨ë‹¬ ìƒì„±/ì§€ì¹¨íŠœë‹(LLM ë””ì½”ë”)
- ì‹œê° ì»¨í…ìŠ¤íŠ¸ VëŠ” ë¹„ì „ ì¸ì½”ë” h(V)âˆˆR^{N_vÃ—d}ë¥¼ í†µí•´ í† í°í™”. í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ T=(t_1â€¦t_L).
- í¬ë¡œìŠ¤ì–´í…ì…˜ ê¸°ë°˜ í™•ë¥ :
  p(T|V)=âˆ_{â„“=1}^L p(t_â„“ | t_{<â„“}, h(V); Î˜),
  L_LM = âˆ’âˆ‘_{â„“=1}^L log p(t_â„“ | t_{<â„“}, h(V)).
- ì‹¤ì „ì—ì„œëŠ” ë‹¤ëª©ì  í•©ì„±:
  L_total = Î»_con L_CLIP + Î»_cap L_LM(cap) + Î»_itm L_ITM + Î»_ins L_LM(instr),
  ì—¬ê¸°ì„œ ITMì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­, instrëŠ” ì§€ì¹¨íŠœë‹ ì†ì‹¤.

3) CCA/InfoMax ê´€ì 
- z_x=W_x^T x, z_y=W_y^T yì— ëŒ€í•´ ìµœëŒ€ê³µë¶„ì‚° ì •ë ¬(Deep CCAë¥˜):
  max_{W_x,W_y} corr(z_x,z_y) s.t. Var(z_x)=Var(z_y)=I.
- ë¹„ì„ í˜• ë”¥ ë„¤íŠ¸ì›Œí¬ë¡œ ê·¼ì‚¬í•˜ê±°ë‚˜ InfoNCEë¡œ MI í•˜í•œì„ ì§ì ‘ ìµœëŒ€í™”.

ì´ë¡ ì Â·ê³„ì‚° ë³µì¡ë„

- ë°°ì¹˜ ëŒ€ë¹„í•™ìŠµ(CLIPë¥˜)
  - ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚° O(B^2 d), ë©”ëª¨ë¦¬ O(B^2). ëŒ€ê·œëª¨ B(>16k)ì¼ ë•Œ í†µì‹ /ë©”ëª¨ë¦¬ ë³‘ëª©ì´ ì§€ë°°.
- íŠ¸ëœìŠ¤í¬ë¨¸ í¬ë¡œìŠ¤ì–´í…ì…˜
  - ì…ë ¥ ê¸¸ì´ L=L_v+L_t, íˆë“  d, í—¤ë“œ ìˆ˜ H.
  - ì‹œê°„ O(H L^2 d), ë©”ëª¨ë¦¬ O(L^2). ê³ í•´ìƒë„ ì´ë¯¸ì§€(í° L_v)ë‚˜ ê¸´ ë¹„ë””ì˜¤(í”„ë ˆì„ T, í”„ë ˆì„ë‹¹ í† í° Nâ†’ L_vâ‰ˆTN)ì—ì„œ ê¸‰ì¦.
- Perceiver Resampler(Flamingo)
  - ë¹„ì „ í† í° N_vë¥¼ rê°œì˜ ìš”ì•½ í† í°ìœ¼ë¡œ ì¶•ì•½. ì–´í…ì…˜ ë¹„ìš©ì„ O(N_v r d)+O(r^2 d)ë¡œ ê°ì†Œ(râ‰ªN_v). ì „ì²´ Lì„ í¬ê²Œ ì¤„ì—¬ ì‹¤ìš©í™”.
- Q-Former(BLIP-2)
  - í•™ìŠµëœ ì§ˆì˜ qâˆˆR^{rÃ—d}ê°€ ë¹„ì „ í† í°ì— ì£¼ì˜ë¥¼ ì£¼ì–´ rê°œì˜ ë©€í‹°ëª¨ë‹¬ í† í° ìƒì„±. ë¹„ìš© O(N_v r d).
- ë¹„ë””ì˜¤
  - ë‹¨ìˆœ í’€ ì–´í…ì…˜: O((TN+L_t)^2 d). í”„ë ˆì„ ìƒ˜í”Œë§, ì‹œê°„ì  ì••ì¶•(Pooling/State-Space/Mamba), ìœˆë„ìš°/ì €ë­í¬/ì„ í˜• ì–´í…ì…˜ìœ¼ë¡œ ì™„í™”.
- í›ˆë ¨ FLOPs ê·¼ì‚¬
  - íŠ¸ëœìŠ¤í¬ë¨¸ 1ì¸µë‹¹ O(L d^2 + H L^2 d). LLM(ìˆ˜ì‹­ì–µ íŒŒë¼ë¯¸í„°) + ë¹„ì „ ì¸ì½”ë”(ViT-L/14, ViT-g ë“±) ë™ì‹œ í•™ìŠµ ì‹œ, ë°ì´í„° í¬ê¸°Ã—ìŠ¤í…Ã—ì¸µìˆ˜ì— ë¹„ë¡€. ìµœì‹  MLLM í•™ìŠµì€ ìˆ˜Ã—10^22 FLOPs ê·œëª¨ë„ ë³´ê³ ë¨(ëŒ€í˜• ì‚°ì—… ëª¨ë¸ ê¸°ì¤€).

í‰ê°€ ë²¤ì¹˜ë§ˆí¬(ëŒ€í‘œ, 2023â€“2024)ì™€ ë™í–¥ ìš”ì•½
- MMBench (Liu et al., 2023), SEED-Bench (Li et al., 2023), MME (Fu et al., 2023)
  - ì¼ë°˜ ë©€í‹°ëª¨ë‹¬ ì´í•´. InternVL2, Qwen2-VL, MM1, LLaVA-Next ë“±ì´ ê°•ë ¥í•œ ì„±ëŠ¥ ë³´ê³ .
- MMMU (Yue et al., 2024)
  - ëŒ€í•™/ì „ë¬¸ ê³¼ëª© ë²”ì£¼ì˜ ê³ ë‚œë„ ì§€ì‹ ì¶”ë¡ . íì‡„ëª¨ë¸(GPT-4Vë¥˜)ì´ ì„ ë„, ìµœê³  ì˜¤í”ˆëª¨ë¸ì€ ë¹ ë¥´ê²Œ ì¶”ê²© ì¤‘.
- MathVista (Lu et al., 2023)
  - ìˆ˜í•™Â·ë„í˜•Â·ì‹œê° ì¶”ë¡ . ì‹œê°-ìˆ˜í•™ ìœµí•©ì—ì„œ ì—¬ì „íˆ ë‚œì œ.
- DocVQA/ChartQA/InfographicVQA
  - ê³ í•´ìƒë„Â·ë ˆì´ì•„ì›ƒ ë¯¼ê°. Qwen2-VL, DocOwl ê³„ì—´ì´ ì„ ì „. íƒ€ì¼ë§/ìœˆë„ìš°+ë¬¸ì ì¸ì‹ ì •ë ¬ì´ í•µì‹¬.
- Video-MME (Fu et al., 2024), NExT-QA ë“±
  - ì¥ê¸° ì‹œê°„ ì´í•´ì™€ ë©”ëª¨ë¦¬ í•œê³„ê°€ ì£¼ìš” ë³‘ëª©. ìƒ˜í”Œë§Â·ìš”ì•½Â·ë©”ëª¨ë¦¬ ëª¨ë“ˆì´ ì„±ëŠ¥ ì¢Œìš°.
- í™˜ê° í‰ê°€: POPE (Li et al., 2023)
  - ê°ì²´ ìˆ˜ì¤€ í™˜ê° ì¸¡ì •. ê³ í•´ìƒë„, OCR, ì™¸ë¶€ ì§€ì‹ ì ‘ì†, ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì´ í™˜ê° ì €ê°ì— ê¸°ì—¬.

ìµœê·¼ ì—°êµ¬ íŠ¸ë Œë“œ ë¹„êµ
- ê³ í•´ìƒë„ ì²˜ë¦¬: ìŠ¬ë¼ì´ë”© ìœˆë„ìš°/íƒ€ì¼ë§ + ì„ íƒì  ì§‘ì¤‘(resampler/Q-Former/region pooling)ìœ¼ë¡œ ê³„ì‚° ë¹„ìš©ê³¼ ì„±ëŠ¥ ê· í˜•.
- ì—”ë“œíˆ¬ì—”ë“œ vs ë™ê²° ë°±ë³¸: ë™ê²° CLIP+ì§€ì¹¨íŠœë‹(ì €ë¹„ìš©)ì—ì„œ ì—”ë“œíˆ¬ì—”ë“œ(ê³ ë¹„ìš©Â·ê³ ì„±ëŠ¥)ë¡œ ì „í™˜í•˜ëŠ” íë¦„. ë¯¸ì„¸ìˆ˜ì • ë²”ìœ„ì™€ ë°ì´í„° ë ˆì‹œí”¼ê°€ ì„±íŒ¨ë¥¼ ì¢Œìš°(MM1).
- ì¡°ê¸° ìœµí•©(decoder-only)ê³¼ í†µí•© í† í°í™”: Chameleonë¥˜ê°€ ì´ë¯¸ì§€Â·í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì¼ í† í° ê³µê°„ì—ì„œ ì²˜ë¦¬. ì‹œê° í† í¬ë‚˜ì´ì €(VQ-VAE/VQGAN ê³„ì—´) í’ˆì§ˆì´ í•µì‹¬ ë³‘ëª©.
- ì¥ê¸° ë¹„ë””ì˜¤: í”„ë ˆì„ ì••ì¶•, State-Space(Mamba)Â·ë©”ëª¨ë¦¬ í† í°Â·í•˜ì´ë¸Œë¦¬ë“œ ì–´í…ì…˜ìœ¼ë¡œ L^2 ë³‘ëª© ì™„í™”.
- ë©€í‹°ëª¨ë‹¬ RLHF/ì„ í˜¸í•™ìŠµ: ì‹œê° ì§€ì‹œì¶”ì¢…(Visual Instruction Tuning)ì—ì„œ ì„ í˜¸ì‹ í˜¸ ê²°í•©ìœ¼ë¡œ ì•ˆì „ì„±Â·ìœ ìš©ì„± ê°œì„  ì‹œë„ ì¦ê°€.
- íˆ´Â·ì—ì´ì „íŠ¸í™”: ìº¡ì…”ë‹/ê²€ì¶œ/OCR/ê²€ìƒ‰/ì½”ë“œ ì‹¤í–‰ê³¼ì˜ ë„êµ¬ ì—°ê²°ë¡œ ì •í™•ë„Â·ê·¼ê±°ì„± ê°•í™”.

ë¯¸í•´ê²° ê³¼ì œì™€ í–¥í›„ ë°©í–¥
- í™˜ê°/ê·¼ê±°ì„±: ì†ŒìŠ¤ ì¦ê±°ì— ê·¼ê±°í•œ í•´ë‹µ ìƒì„±ê³¼ ì‹ ë¢°ë„ ë³´ì •. ì²´ê³„ì  ìº˜ë¦¬ë¸Œë ˆì´ì…˜Â·ë¶ˆí™•ì‹¤ì„± ì¶”ì • ì—°êµ¬ í•„ìš”.
- ì¥ê¸° ë¹„ë””ì˜¤Â·ë©€í‹°í˜ì´ì§€ ë¬¸ì„œ: ìˆ˜ë§Œ~ìˆ˜ì‹­ë§Œ í† í° ë§¥ë½ì—ì„œ ì„ í˜• í˜¹ì€ ì•„í‚¤í…ì²˜ì  ì„œë¸Œì¿¼ë“œë¦­ ì†”ë£¨ì…˜ì˜ ì •í™•ì„± ìœ ì§€.
- ë‹¤ê°ê° í†µí•©(ì˜¤ë””ì˜¤Â·3DÂ·ì´‰ê°): ImageBindë¥˜ì˜ ê³µìœ  ì„ë² ë”©ì„ MLLM ë””ì½”ë”©ê³¼ ê²°í•©í•´ ì§„ì •í•œ ì„ì˜-ëª¨ë‹¬(any-to-any) ìƒí˜¸ìš´ìš©.
- ë°ì´í„° ê±°ë²„ë„ŒìŠ¤: ì €ì‘ê¶Œ/í’ˆì§ˆ/í¸í–¥ ì œì–´, í•©ì„± ë°ì´í„°ì™€ í•„í„°ë§ ë ˆì‹œí”¼ì˜ ì •ëŸ‰ì  ì˜í–¥ ë¶„ì„(MM1ë¥˜ì˜ ë©”íƒ€ ì—°êµ¬ í™•ëŒ€).
- íš¨ìœ¨ì„±: í›ˆë ¨/ì¶”ë¡  ë¹„ìš©ì„ ì¤„ì´ë©´ì„œ ê³ í•´ìƒë„Â·ì¥ë¬¸ë§¥ì„ ìœ ì§€í•˜ëŠ” í† í°í™”/ì••ì¶•/ë¼ì´íŠ¸ì›¨ì´íŠ¸ ì–´í…ì…˜.

ë¹ ë¥¸ ì‹¤ë¬´ ì§€ì¹¨(ì–´ë””ì„œ ì–´ë–»ê²Œ ìµœì‹ ì„ ì°¾ì„ê¹Œ)
- arXiv ì¹´í…Œê³ ë¦¬/í‚¤ì›Œë“œ
  - cs.CV, cs.CL, cs.LGì—ì„œ â€œmultimodal large language modelâ€, â€œvision language modelâ€, â€œvisual instruction tuningâ€, â€œvideo-llmâ€, â€œdocument understandingâ€, â€œOCR-freeâ€, â€œresamplerâ€, â€œPerceiverâ€, â€œQ-Formerâ€, â€œInfoNCEâ€.
- Papers with Code SOTA
  - íƒœìŠ¤í¬ë³„: MMBench, MMMU, SEED-Bench, DocVQA, ChartQA, TextVQA, Video-QA, MathVista.
- ì˜¤í”ˆì†ŒìŠ¤ ì¡°ì§/ë¦¬í¬
  - LLaVA, HuggingFace M4/IDEFICS, Shanghai AI Lab(InternVL), Alibaba Qwen, Apple ML(â€œMM1â€), THU/PKUì˜ Video-LLM ë¼ì¸.
- ì£¼ê°„ íë ˆì´ì…˜
  - â€œawesome-multimodalâ€, â€œmultimodal-llm-surveyâ€ ë¦¬í¬, arXiv Sanity/Scholar Alertsë¡œ í‚¤ì›Œë“œ ì•Œë¦¼.

ì°¸ê³  ë¬¸í—Œ(ì„ ì •)
- Radford et al., 2021. Learning Transferable Visual Models From Natural Language Supervision (CLIP).
- Jia et al., 2021. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (ALIGN).
- Alayrac et al., 2022. Flamingo: a Visual Language Model for Few-Shot Learning.
- Jaegle et al., 2021. Perceiver IO: A General Architecture for Structured Inputs & Outputs.
- Li et al., 2023. BLIP-2: Bootstrapping Language-Image Pre-training.
- Dai et al., 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.
- Zhai et al., 2023. SigLIP: CLIP Without Contrastive Pairs.
- Sun et al., 2023. EVA-CLIP: Improved Training Techniques for CLIP at Scale.
- Liu et al., 2023. Visual Instruction Tuning (LLaVA).
- Jain et al., 2024. MM1: Methods, Analysis & Insights from Multimodal LLMs.
- LaurenÃ§on et al., 2024. IDEFICS2: An Open Model for In-Context Vision-Language Learning.
- Wang et al., 2024. Qwen2-VL.
- Chen et al., 2024. InternVL 2.
- Lin et al., 2024. Video-LLaVA.
- Fu et al., 2024. MME / Video-MME Benchmarks.
- Yue et al., 2024. MMMU: Massive Multi-discipline Multimodal Understanding.
- Lu et al., 2023. MathVista: Evaluating Math Reasoning in Vision-Language Models.
- Li et al., 2023. POPE: Evaluating Object Hallucination in VQA.

í•„ìš”ì‹œ, ê´€ì‹¬ ì˜ì—­(ì˜ˆ: ë¹„ë””ì˜¤, ë¬¸ì„œ/OCR, ì˜ë£Œ, ë¡œë³´í‹±ìŠ¤ ë“±)ê³¼ ëª¨ë¸ í¬ê¸°/ì˜¤í”ˆì†ŒìŠ¤ ì—¬ë¶€ë¥¼ ì•Œë ¤ì£¼ì‹œë©´, ë” ì¢í˜€ì„œ ìµœì‹ (2024â€“2025) ì „ìš© ëª©ë¡ê³¼ ë¹„êµí‘œ, ëª©í‘œí•¨ìˆ˜/ë¹„ìš© ë¶„ì„ì„ ë§ì¶¤ ì œì‘í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

---

## [3] ğŸ™‹ ì‚¬ìš©ì

ì „ì²´ ì €ì¥í•´ì¤˜
