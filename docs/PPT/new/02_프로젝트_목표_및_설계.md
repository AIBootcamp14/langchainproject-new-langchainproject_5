# 02. 프로젝트 목표 및 설계
> PRD 기반 시스템 아키텍처 및 설계 내용 (Pages 7-20)

## 📑 목차
1. [문제 정의](#문제-정의)
2. [솔루션 제시](#솔루션-제시)
3. [프로젝트 목표](#프로젝트-목표)
4. [시스템 아키텍처](#시스템-아키텍처)
5. [Agent Flow 설계](#agent-flow-설계)
6. [데이터베이스 설계](#데이터베이스-설계)
7. [LangGraph 설계](#langgraph-설계)
8. [RAG 시스템 설계](#rag-시스템-설계)
9. [도구 정의](#도구-정의)
10. [프롬프트 엔지니어링](#프롬프트-엔지니어링)
11. [UI/UX 설계](#uiux-설계)
12. [성능 목표](#성능-목표)
13. [역할 분담](#역할-분담)
14. [개발 환경](#개발-환경)

---

## 문제 정의
### 페이지 7: 프로젝트 목표 (인트로)

**PPT 내용:**

**제목:** 논문 리뷰 챗봇
**부제:** AI/ML 논문을 누구나 쉽게 이해하고 활용할 수 있는 지능형 챗봇 개발
**부가 설명:** 논문을 읽고 이해하기란 쉽지 않을 뿐더러, 사용자마다 이해도의 차이가 존재할 수 있습니다. 저희는 그러한 정보 격차를 완화하고, 진입장벽을 낮추고자 해당 프로젝트를 기획하게 되었습니다.

**발표 대본:**
```
안녕하세요. 2번째 파트인 프로젝트 목표 및 설계를 발표하겠습니다.

저희 프로젝트는 AI/ML 논문을 누구나 쉽게 이해하고
활용할 수 있는 지능형 챗봇 개발을 목표로 합니다.

논문을 읽고 이해하기란 쉽지 않을 뿐더러,
사용자마다 이해도의 차이가 존재합니다.

저희는 이러한 정보 격차를 완화하고
진입장벽을 낮추고자 이 프로젝트를 기획했습니다.
```

---

## 목표 달성 흐름도
### 페이지 8: 목표 달성 흐름도

**PPT 내용:**

**Mermaid 다이어그램이 이미 PPT에 존재하므로 해당 내용을 설명합니다**

**발표 대본:**
```
저희 프로젝트의 목표 달성 흐름을 보여드리겠습니다.

문제 정의 단계에서는 정보 과부하, 높은 진입장벽, 검색의 한계라는
세 가지 핵심 문제를 식별했습니다.

솔루션 단계에서는 AI Agent 라우팅으로 질문에 맞는 도구를 자동 선택하고,
난이도별 답변으로 초심자와 전문가 모두에게 최적화된 설명을 제공하며,
하이브리드 검색으로 로컬 DB와 웹 검색을 결합하는 세 가지 해결책을 제시합니다.

최종적으로 학습 시간 50% 단축, 진입장벽 낮춤,
효율적 정보 탐색이라는 목표 달성을 지향합니다.
```

---

## 난이도 설정
### 페이지 9: LLM 답변 난이도 설정

**PPT 내용:**

**테이블: 난이도별 LLM 선택**
| 구분 | Easy Mode (초보자) | Hard Mode (전문가) |
|------|-------------------|-------------------|
| **LLM 모델** | Solar Pro2 | GPT-4 |
| **비용** | 무료 (학습 크레딧) | 최대 $10 설정 |
| **답변 스타일** | 친절하고 이해하기 쉬운 언어 | 전문적이고 기술적인 언어 |
| **용어 설명** | 전문 용어 풀어서 설명, 쉬운 한글 용어 | 기술 용어 정확히 사용 |
| **설명 방식** | 비유와 예시 많이 활용 | 실험 결과와 평가 지표 포함, 비판적 분석 |
| **기술 세부사항** | 수식 최소화 | 수식, 알고리즘, 연구 방법론 포함 |

**발표 대본:**
```
사용자 질문에 대한 답변 생성 시 난이도에 따라
적절한 LLM을 선택하고 컨텍스트를 구성합니다.

Easy Mode는 Solar Pro2 모델을 사용하며,
학습 크레딧으로 무료로 이용할 수 있습니다.
초심자를 위한 쉬운 설명과 용어를 사용하여 논문 진입 장벽을 낮춥니다.
전문 용어는 풀어서 설명하고, 비유와 예시를 많이 활용합니다.

Hard Mode는 GPT-4 모델을 사용하며, 최대 10달러로 비용을 설정했습니다.
전문가를 위한 기술적 설명으로 실험 결과와 평가 지표를 포함하고,
비판적 분석을 제공합니다. 연구 방법론과 한계점을 언급하며
기술적 세부사항, 수식, 알고리즘을 포함합니다.
```

---

## 데이터베이스 설계
### 페이지 10: PostgreSQL + pgvector 선택

**PPT 내용:**

**테이블: 데이터베이스 비교**
| DB 타입 | 벡터 검색 | 관계형 데이터 | 비용 | 학습 곡선 |
|---------|----------|--------------|------|----------|
| **PostgreSQL + pgvector** | ✅ pgvector | ✅ | 무료 | 중간 |
| MySQL | ❌ 별도 DB 필요 | ✅ | 무료 | 낮음 |
| Pinecone | ✅ | ⚠ 메타데이터만 | 유료 | 낮음 |
| Weaviate | ✅ | ⚠ 제한적 | 무료 | 높음 |
| Chroma | ✅ | ❌ | 무료 | 낮음 |

**선택 이유:**
- 논문 본문 벡터 검색 + 메타데이터 관리를 단일 DB로 처리
- 운영 복잡도 감소, 비용 절감
- 논문 50-100편 규모에서 충분한 성능

**발표 대본:**
```
데이터베이스 설계에서는 PostgreSQL과 pgvector를 선택했습니다.

다양한 데이터베이스를 비교한 결과, PostgreSQL with pgvector는
벡터 검색과 관계형 데이터를 모두 지원하며, 무료이면서
학습 곡선이 중간 수준으로 적절했습니다.

MySQL은 벡터 검색을 위해 별도 DB가 필요하고,
Pinecone은 유료이며 관계형 데이터 관리가 제한적입니다.

PostgreSQL with pgvector를 선택한 이유는
논문 본문의 벡터 검색과 메타데이터 관리를
단일 DB로 처리할 수 있어 운영 복잡도가 감소하고
비용이 절감되기 때문입니다.

저희 프로젝트의 논문 50-100편 규모에서 충분한 성능을 제공합니다.
```

---

## 데이터 수집
### 페이지 11: 논문 데이터 수집 프로세스

**PPT 내용:**

**논문 데이터 수집 다이어그램이 이미 PPT에 있습니다**

**발표 대본:**
```
논문 데이터 수집 프로세스를 설명드리겠습니다.

먼저 arXiv API를 통해 AI/ML 분야의 논문 50-100편을 수집합니다.
주요 카테고리는 Transformer, BERT, GPT, Attention Mechanism 등입니다.

수집된 논문은 PDF 형식으로 다운로드하여 로컬에 저장하고,
Document Loader를 사용하여 텍스트를 추출합니다.

추출된 텍스트는 Text Splitter로 청크 단위로 분할하며,
각 청크는 1000자 크기, 200자 중복으로 설정했습니다.

분할된 청크는 OpenAI Embeddings로 벡터화하여
PostgreSQL의 pgvector extension을 통해 저장합니다.

동시에 논문 메타데이터인 제목, 저자, 출판일, arXiv ID 등은
PostgreSQL의 papers 테이블에 저장하여 관리합니다.
```

---

## 데이터베이스 스키마
### 페이지 12: 데이터베이스 스키마 설계

**PPT 내용:**

**테이블 1: 주요 테이블 구조**
| 테이블 | 주요 컬럼 | 용도 | 인덱스 |
|--------|----------|------|--------|
| **papers** | paper_id, title, authors, abstract | 논문 메타데이터 | title (GIN), date (B-Tree) |
| **paper_chunks** | chunk_id, paper_id, embedding | 벡터 검색용 청크 | embedding (HNSW) |
| **glossary** | term_id, term, easy/hard_explanation | 용어 정의 | term (B-Tree) |
| **glossary_embeddings** | term_id, embedding | 용어 유사도 검색 | embedding (HNSW) |
| **query_logs** | log_id, query, tool_used, response_time | 성능 모니터링 | created_at (B-Tree) |

**테이블 2: DB 상세 스펙**
| 테이블 | 용도 | 주요 필드 | 인덱스 |
|--------|------|-----------|---------|
| papers | 논문 메타데이터 | id, title, authors, arxiv_id | arxiv_id, published_date |
| embeddings | 벡터 임베딩 | paper_id, embedding(1536d), chunk_text | paper_id, embedding(ivfflat) |
| glossary | 용어 사전 | term, definition | term(btree) |
| chat_history | 대화 기록 | user_id, question, answer, mode | user_id, created_at |

**발표 대본:**
```
데이터베이스 스키마 설계를 보여드리겠습니다.

papers 테이블은 논문의 메타데이터를 저장하며,
제목, 저자, 초록, arXiv ID 등의 정보가 포함됩니다.
title 컬럼은 GIN 인덱스로, 날짜는 B-Tree 인덱스로 관리합니다.

paper_chunks 테이블은 벡터 검색을 위한 청크를 저장합니다.
1536차원의 embedding 벡터는 HNSW 인덱스로 빠른 검색이 가능합니다.

glossary 테이블은 전문 용어와 난이도별 설명을 저장하며,
용어 검색을 위해 B-Tree 인덱스를 사용합니다.

glossary_embeddings 테이블은 용어 유사도 검색을 위한
벡터를 저장하여 하이브리드 검색을 지원합니다.

query_logs 테이블은 성능 모니터링을 위해
사용자 질문, 사용된 도구, 응답 시간을 기록합니다.
```

---

## 시스템 아키텍처
### 페이지 10: 전체 시스템 구조

**PPT 내용:**

```mermaid
graph TB
    subgraph Frontend["🎨 Frontend Layer"]
        UI[Streamlit UI]
        CHAT[Chat Interface]
    end

    subgraph Backend["⚙️ Backend Layer"]
        AGENT[LangGraph Agent]
        ROUTER[Tool Router]
        MEMORY[Memory System]
    end

    subgraph Tools["🔧 Tool Layer"]
        RAG1[RAG 논문 검색]
        RAG2[RAG 용어집]
        SQL[Text2SQL]
        WEB[Web Search]
        SUM[Summarizer]
        GEN[General Answer]
        SAVE[File Saver]
    end

    subgraph Data["💾 Data Layer"]
        PG[(PostgreSQL<br/>+ pgvector)]
        EMB[Embeddings]
        DOCS[Documents]
    end

    subgraph LLM["🤖 LLM Layer"]
        GPT4[GPT-4<br/>Hard Mode]
        SOLAR[Solar Pro2<br/>Easy Mode]
    end

    UI --> CHAT
    CHAT --> AGENT
    AGENT --> ROUTER
    ROUTER --> Tools
    Tools --> Data
    AGENT --> LLM
    AGENT --> MEMORY

    %% 스타일링
    style Frontend fill:#E8F5E9,stroke:#4CAF50,stroke-width:2px
    style Backend fill:#E3F2FD,stroke:#2196F3,stroke-width:2px
    style Tools fill:#FFF3E0,stroke:#FF9800,stroke-width:2px
    style Data fill:#F3E5F5,stroke:#9C27B0,stroke-width:2px
    style LLM fill:#FCE4EC,stroke:#E91E63,stroke-width:2px
```

**발표 스크립트:**
```
전체 시스템 아키텍처는 5개 레이어로 구성됩니다.

Frontend Layer는 Streamlit으로 구현한 웹 인터페이스로
사용자와 상호작용합니다.

Backend Layer는 LangGraph Agent가 핵심으로
도구 라우팅과 메모리 관리를 담당합니다.

Tool Layer는 7개의 도구가 각자의 역할을 수행하며
RAG 검색, Text2SQL, 웹 검색 등을 제공합니다.

Data Layer는 PostgreSQL과 pgvector를 사용하여
논문 데이터와 벡터 임베딩을 저장합니다.

LLM Layer는 GPT-4와 Solar Pro2를 사용하여
난이도별 응답을 생성합니다.

각 레이어는 명확하게 분리되어 있어
유지보수와 확장이 용이한 구조입니다.
```

---

## Agent Flow 설계
### 페이지 11: Agent 워크플로우

**PPT 내용:**

```mermaid
graph TB
    subgraph Flow["🔄 Agent Workflow"]
        START([사용자 입력])

        ANALYZE{질문 분석}

        ROUTE{도구 선택}

        RAG_FLOW[RAG 검색]
        SQL_FLOW[Text2SQL]
        WEB_FLOW[웹 검색]
        SUM_FLOW[요약]
        GEN_FLOW[일반 답변]

        COMBINE[결과 통합]

        MODE{난이도 선택}

        EASY[Easy Mode<br/>Solar Pro2]
        HARD[Hard Mode<br/>GPT-4]

        RESPONSE([응답 생성])

        START --> ANALYZE
        ANALYZE --> ROUTE

        ROUTE -->|논문 검색| RAG_FLOW
        ROUTE -->|DB 조회| SQL_FLOW
        ROUTE -->|최신 정보| WEB_FLOW
        ROUTE -->|요약 요청| SUM_FLOW
        ROUTE -->|일반 질문| GEN_FLOW

        RAG_FLOW --> COMBINE
        SQL_FLOW --> COMBINE
        WEB_FLOW --> COMBINE
        SUM_FLOW --> COMBINE
        GEN_FLOW --> COMBINE

        COMBINE --> MODE

        MODE -->|초심자| EASY
        MODE -->|전문가| HARD

        EASY --> RESPONSE
        HARD --> RESPONSE
    end

    %% 스타일링
    style START fill:#C8E6C9,stroke:#4CAF50,stroke-width:2px
    style RESPONSE fill:#FFCDD2,stroke:#F44336,stroke-width:2px
    style ANALYZE fill:#E1F5FE,stroke:#03A9F4,stroke-width:2px
    style ROUTE fill:#FFF9C4,stroke:#FFEB3B,stroke-width:2px
    style MODE fill:#F3E5F5,stroke:#9C27B0,stroke-width:2px
```

**발표 스크립트:**
```
Agent의 워크플로우를 설명드리겠습니다.

사용자 입력이 들어오면 먼저 질문을 분석하여
어떤 도구를 사용할지 결정합니다.

질문 유형에 따라 RAG 검색, Text2SQL, 웹 검색,
요약, 일반 답변 중 적절한 도구를 선택합니다.

도구 실행 결과를 통합한 후,
사용자가 선택한 난이도에 따라
Easy Mode는 Solar Pro2로, Hard Mode는 GPT-4로
최종 응답을 생성합니다.

이 과정은 LangGraph의 State Machine으로 구현되어
각 단계가 체계적으로 실행됩니다.
```

---

## 데이터베이스 설계
### 페이지 12: DB 스키마

**PPT 내용:**

```mermaid
erDiagram
    PAPERS ||--o{ EMBEDDINGS : has
    PAPERS ||--o{ SUMMARIES : has
    GLOSSARY ||--o{ GLOSSARY_EMBEDDINGS : has
    USERS ||--o{ CHAT_HISTORY : has

    PAPERS {
        uuid id PK
        string title
        string authors
        text abstract
        date published_date
        string arxiv_id
        json metadata
        timestamp created_at
    }

    EMBEDDINGS {
        uuid id PK
        uuid paper_id FK
        vector embedding
        text chunk_text
        int chunk_index
        timestamp created_at
    }

    GLOSSARY {
        uuid id PK
        string term
        text definition
        text context
        timestamp created_at
    }

    CHAT_HISTORY {
        uuid id PK
        uuid user_id FK
        text question
        text answer
        string mode
        json tools_used
        timestamp created_at
    }
```

**테이블: DB 상세 스펙**
| 테이블 | 용도 | 주요 필드 | 인덱스 |
|--------|------|-----------|---------|
| papers | 논문 메타데이터 | id, title, authors, arxiv_id | arxiv_id, published_date |
| embeddings | 벡터 임베딩 | paper_id, embedding(1536d), chunk_text | paper_id, embedding(ivfflat) |
| glossary | 용어 사전 | term, definition | term(btree) |
| chat_history | 대화 기록 | user_id, question, answer, mode | user_id, created_at |

**발표 스크립트:**
```
데이터베이스는 PostgreSQL과 pgvector를 사용하여 설계했습니다.

papers 테이블에는 논문의 메타데이터를,
embeddings 테이블에는 논문 청크별 벡터 임베딩을 저장합니다.

glossary 테이블은 전문 용어와 정의를 관리하고,
chat_history 테이블은 사용자 대화 기록을 저장합니다.

특히 pgvector의 ivfflat 인덱스를 사용하여
1536차원 벡터에 대한 빠른 유사도 검색을 구현했습니다.

이를 통해 의미 기반 검색과 컨텍스트 유지가 가능합니다.
```

---

## LangGraph 설계
### 페이지 13: State Machine 설계

**PPT 내용:**

```mermaid
graph LR
    subgraph States["🔄 LangGraph States"]
        START((Start))

        INIT[Initialize<br/>State]

        ANALYZE[Analyze<br/>Question]

        ROUTE[Route to<br/>Tools]

        EXECUTE[Execute<br/>Tools]

        MERGE[Merge<br/>Results]

        GENERATE[Generate<br/>Response]

        END((End))

        START --> INIT
        INIT --> ANALYZE
        ANALYZE --> ROUTE
        ROUTE --> EXECUTE
        EXECUTE --> MERGE
        MERGE --> GENERATE
        GENERATE --> END
    end

    %% 스타일링
    style START fill:#C8E6C9,stroke:#4CAF50,stroke-width:3px
    style END fill:#FFCDD2,stroke:#F44336,stroke-width:3px
    style INIT fill:#E1F5FE,stroke:#03A9F4,stroke-width:2px
    style ANALYZE fill:#FFF9C4,stroke:#FFEB3B,stroke-width:2px
    style ROUTE fill:#F3E5F5,stroke:#9C27B0,stroke-width:2px
    style EXECUTE fill:#FFE0B2,stroke:#FF9800,stroke-width:2px
    style MERGE fill:#E8EAF6,stroke:#5C6BC0,stroke-width:2px
    style GENERATE fill:#FCE4EC,stroke:#E91E63,stroke-width:2px
```

**코드: State 정의**
```python
class AgentState(TypedDict):
    messages: list[BaseMessage]
    question: str
    tool_calls: list[str]
    tool_results: dict
    mode: str  # "easy" or "hard"
    final_answer: str
```

**발표 스크립트:**
```
LangGraph를 사용한 State Machine 설계입니다.

각 State는 명확한 역할을 가지고 있습니다.
Initialize는 초기 상태 설정,
Analyze는 질문 분석,
Route는 도구 라우팅,
Execute는 도구 실행,
Merge는 결과 통합,
Generate는 최종 응답 생성을 담당합니다.

State 간 전이는 조건에 따라 결정되며,
필요시 이전 State로 돌아갈 수 있는 유연한 구조입니다.

이를 통해 복잡한 워크플로우도 체계적으로 관리할 수 있습니다.
```

---

## RAG 시스템 설계
### 페이지 14: RAG Pipeline

**PPT 내용:**

```mermaid
graph LR
    subgraph RAGPipeline["🔍 RAG Pipeline"]
        subgraph Indexing["📥 Indexing Phase"]
            direction TB
            LOAD[Document<br/>Loader]
            SPLIT[Text<br/>Splitter]
            EMBED[Embedding<br/>Generator]
            STORE[Vector<br/>Store]

            LOAD --> SPLIT
            SPLIT --> EMBED
            EMBED --> STORE
        end

        subgraph Retrieval["🔎 Retrieval Phase"]
            direction TB
            QUERY[Query<br/>Processing]
            SEARCH[Vector<br/>Search]
            RERANK[Re-ranking]
            CONTEXT[Context<br/>Building]

            QUERY --> SEARCH
            SEARCH --> RERANK
            RERANK --> CONTEXT
        end

        subgraph Generation["✨ Generation Phase"]
            direction TB
            PROMPT[Prompt<br/>Template]
            LLM[LLM<br/>Call]
            POST[Post<br/>Processing]

            PROMPT --> LLM
            LLM --> POST
        end

        Indexing --> Retrieval
        Retrieval --> Generation
    end

    %% 스타일링
    style Indexing fill:#E8F5E9,stroke:#4CAF50,stroke-width:2px
    style Retrieval fill:#E3F2FD,stroke:#2196F3,stroke-width:2px
    style Generation fill:#FFF3E0,stroke:#FF9800,stroke-width:2px
```

**테이블: RAG 파라미터**
| 단계 | 파라미터 | 값 | 설명 |
|------|----------|-----|------|
| Splitting | chunk_size | 1000 | 청크 크기 |
| Splitting | overlap | 200 | 청크 중복 |
| Embedding | model | text-embedding-3-small | OpenAI 임베딩 |
| Search | top_k | 5 | 검색 결과 수 |
| Reranking | threshold | 0.7 | 유사도 임계값 |

**발표 스크립트:**
```
RAG 시스템은 세 단계로 구성됩니다.

첫째, Indexing 단계에서는 논문을 로드하고
청크로 분할한 후 임베딩을 생성하여 벡터 DB에 저장합니다.

둘째, Retrieval 단계에서는 사용자 쿼리를 처리하고
벡터 검색을 수행한 후 재순위화하여 컨텍스트를 구성합니다.

셋째, Generation 단계에서는 프롬프트 템플릿에
검색 결과를 결합하여 LLM으로 응답을 생성합니다.

청크 크기는 1000자, 중복은 200자로 설정하고,
OpenAI의 text-embedding-3-small 모델을 사용합니다.
상위 5개 결과를 검색하며 유사도 0.7 이상만 사용합니다.
```

---

## 도구 정의
### 페이지 15: 7가지 도구 상세

**PPT 내용:**

**테이블: 도구별 기능 정의**
| 도구명 | 용도 | 입력 | 출력 | 사용 시나리오 |
|--------|------|------|------|---------------|
| **RAG 논문 검색** | 논문 DB에서 검색 | 쿼리, top_k | 관련 논문 리스트 | "Transformer 논문 찾아줘" |
| **RAG 용어집** | 용어 정의 검색 | 용어 | 정의, 예시 | "Attention이 뭐야?" |
| **Text2SQL** | DB 통계 조회 | 자연어 질문 | SQL 결과 | "최근 일주일 논문 수" |
| **웹 검색** | 최신 정보 검색 | 검색어 | 웹 결과 | "GPT-5 최신 소식" |
| **요약** | 논문 요약 생성 | 논문 ID | 요약문 | "이 논문 요약해줘" |
| **일반 답변** | 일반 질문 응답 | 질문 | 답변 | "딥러닝이란?" |
| **파일 저장** | 대화 내용 저장 | 내용, 경로 | 저장 결과 | "대화 내용 저장해줘" |

**발표 스크립트:**
```
시스템에는 7가지 도구가 통합되어 있습니다.

RAG 논문 검색은 벡터 DB에서 의미적으로 유사한 논문을,
RAG 용어집은 전문 용어의 정의를 검색합니다.

Text2SQL은 자연어를 SQL로 변환하여 통계를 조회하고,
웹 검색은 최신 정보를 실시간으로 가져옵니다.

요약 도구는 긴 논문을 간단하게 요약하고,
일반 답변은 RAG 없이 LLM의 지식으로 응답합니다.

파일 저장 도구는 대화 내용을 파일로 저장합니다.

각 도구는 명확한 역할이 있어 효율적으로 작동합니다.
```

---

## 프롬프트 엔지니어링
### 페이지 16: 프롬프트 설계

**PPT 내용:**

```python
# Easy Mode 프롬프트 예시
EASY_MODE_PROMPT = """
당신은 친절한 AI 논문 도우미입니다.
초심자도 이해할 수 있도록 쉽게 설명해주세요.

규칙:
1. 전문 용어는 반드시 쉬운 말로 풀어서 설명
2. 예시를 들어 설명
3. 단계별로 차근차근 설명
4. 이모지를 사용하여 친근하게

Context: {context}
Question: {question}
"""

# Hard Mode 프롬프트 예시
HARD_MODE_PROMPT = """
당신은 전문적인 AI 연구자입니다.
정확하고 상세한 기술적 설명을 제공하세요.

규칙:
1. 기술적 용어를 정확히 사용
2. 수식과 알고리즘 포함
3. 관련 논문 레퍼런스 제시
4. 비판적 분석 포함

Context: {context}
Question: {question}
"""
```

**테이블: 프롬프트 전략**
| 구분 | Easy Mode | Hard Mode |
|------|-----------|-----------|
| **톤** | 친근하고 쉬운 | 전문적이고 정확한 |
| **용어** | 쉬운 말로 풀어서 | 기술 용어 사용 |
| **설명** | 예시 중심 | 이론 중심 |
| **길이** | 간단명료 | 상세하고 깊이있게 |

**발표 스크립트:**
```
프롬프트 엔지니어링은 난이도별로 다르게 설계했습니다.

Easy Mode는 초심자를 위해 친근한 톤으로
전문 용어를 쉽게 풀어서 설명하고,
예시를 들어 이해를 돕습니다.

Hard Mode는 전문가를 위해 정확한 기술 용어를 사용하고
수식과 알고리즘을 포함하여 깊이 있게 설명합니다.

각 모드별로 다른 LLM을 사용하여
Easy Mode는 Solar Pro2로 비용을 절감하고,
Hard Mode는 GPT-4로 높은 품질을 보장합니다.

이를 통해 사용자 수준에 맞는 최적의 답변을 제공합니다.
```

---

## UI/UX 설계
### 페이지 17: 인터페이스 설계

**PPT 내용:**

```mermaid
graph TB
    subgraph UI["🎨 Streamlit UI 구성"]
        subgraph Sidebar["📱 사이드바"]
            MODE[난이도 선택<br/>Easy/Hard]
            TOOLS[도구 선택]
            HISTORY[대화 기록]
            SETTINGS[설정]
        end

        subgraph Main["📋 메인 화면"]
            CHAT[채팅 인터페이스]
            INPUT[입력창]
            RESPONSE[응답 영역]
            SOURCES[출처 표시]
        end

        subgraph Features["⚡ 특수 기능"]
            STREAM[스트리밍 응답]
            EXPORT[대화 내보내기]
            CLEAR[대화 초기화]
        end
    end

    %% 스타일링
    style Sidebar fill:#E8F5E9,stroke:#4CAF50,stroke-width:2px
    style Main fill:#E3F2FD,stroke:#2196F3,stroke-width:2px
    style Features fill:#FFF3E0,stroke:#FF9800,stroke-width:2px
```

**테이블: UI 컴포넌트**
| 컴포넌트 | 기능 | 구현 방법 |
|----------|------|-----------|
| 채팅 인터페이스 | 대화형 UI | st.chat_message() |
| 난이도 선택 | Easy/Hard 토글 | st.radio() |
| 도구 표시 | 사용된 도구 시각화 | st.info() |
| 스트리밍 | 실시간 응답 | st.write_stream() |
| 출처 표시 | 참조 논문 링크 | st.expander() |

**발표 스크립트:**
```
UI는 Streamlit으로 구현하여 직관적인 인터페이스를 제공합니다.

사이드바에서 난이도를 선택하고 도구를 확인할 수 있으며,
대화 기록을 관리할 수 있습니다.

메인 화면은 채팅 인터페이스로 구성되어
자연스러운 대화가 가능합니다.

특히 스트리밍 응답으로 실시간으로 답변이 생성되는 것을
볼 수 있어 사용자 경험을 향상시킵니다.

응답과 함께 사용된 도구와 참조 논문을 표시하여
투명성과 신뢰성을 제공합니다.
```

---

## 성능 목표
### 페이지 18: 성능 지표

**PPT 내용:**

```mermaid
graph LR
    subgraph Metrics["📊 성능 지표"]
        subgraph Speed["⚡ 속도"]
            S1[응답 시간<br/>< 3초]
            S2[검색 시간<br/>< 1초]
            S3[임베딩 생성<br/>< 0.5초]
        end

        subgraph Accuracy["🎯 정확도"]
            A1[검색 정확도<br/>> 85%]
            A2[도구 선택<br/>> 90%]
            A3[SQL 정확도<br/>> 80%]
        end

        subgraph Scale["📈 확장성"]
            SC1[동시 사용자<br/>50명]
            SC2[논문 DB<br/>10만 편]
            SC3[일일 쿼리<br/>1000건]
        end
    end

    %% 스타일링
    style Speed fill:#E1F5FE,stroke:#03A9F4,stroke-width:2px
    style Accuracy fill:#F3E5F5,stroke:#9C27B0,stroke-width:2px
    style Scale fill:#FFF3E0,stroke:#FF9800,stroke-width:2px
```

**발표 스크립트:**
```
성능 목표를 세 가지 측면에서 설정했습니다.

속도 면에서는 전체 응답 시간 3초 이내,
벡터 검색 1초 이내, 임베딩 생성 0.5초 이내를 목표로 합니다.

정확도 면에서는 검색 정확도 85% 이상,
도구 선택 정확도 90% 이상, SQL 변환 정확도 80% 이상을 목표로 합니다.

확장성 면에서는 동시 사용자 50명 지원,
논문 DB 10만 편 저장, 일일 1000건 쿼리 처리를 목표로 합니다.

이러한 성능 목표 달성을 위해 캐싱, 인덱싱,
비동기 처리 등의 최적화를 적용했습니다.
```

---

## 역할 분담
### 페이지 19: 팀 역할 분담

**PPT 내용:**

```mermaid
graph TB
    subgraph Team["👥 팀 역할 분담"]
        subgraph Leader["👑 팀장: 최현화"]
            L1[프로젝트 총괄]
            L2[LangGraph Agent]
            L3[메모리 시스템]
            L4[Streamlit UI]
        end

        subgraph Member1["👨‍💻 박재홍"]
            M11[데이터 수집]
            M12[DB 구축]
            M13[문서 처리]
        end

        subgraph Member2["👨‍💻 신준엽"]
            M21[RAG 시스템]
            M22[용어집 도구]
            M23[Text2SQL]
        end

        subgraph Member3["👩‍💻 임예슬"]
            M31[프롬프트 엔지니어링]
            M32[웹 검색 도구]
            M33[파일 저장 도구]
        end
    end

    %% 스타일링
    style Leader fill:#E8EAF6,stroke:#5C6BC0,stroke-width:2px
    style Member1 fill:#E8F5E9,stroke:#4CAF50,stroke-width:2px
    style Member2 fill:#FFF3E0,stroke:#FF9800,stroke-width:2px
    style Member3 fill:#FCE4EC,stroke:#E91E63,stroke-width:2px
```

**발표 스크립트:**
```
팀 역할을 명확하게 분담하여 효율적으로 개발했습니다.

팀장인 최현화는 프로젝트를 총괄하며
LangGraph Agent와 메모리 시스템, UI를 담당했습니다.

박재홍 팀원은 논문 데이터 수집과 DB 구축,
문서 처리 파이프라인을 구현했습니다.

신준엽 팀원은 RAG 시스템과 용어집 도구,
Text2SQL 기능을 개발했습니다.

임예슬 팀원은 프롬프트 엔지니어링과
웹 검색, 파일 저장 도구를 담당했습니다.

각자의 전문성을 살려 병렬로 개발을 진행하고
정기적인 통합을 통해 시스템을 완성했습니다.
```

---

## 개발 환경
### 페이지 20: 기술 스택

**PPT 내용:**

**테이블: 기술 스택 상세**
| 분류 | 기술 | 버전 | 용도 |
|------|------|------|------|
| **Language** | Python | 3.11 | 메인 개발 언어 |
| **Framework** | LangGraph | 0.2.x | Agent 프레임워크 |
| **Database** | PostgreSQL | 16 | 메인 데이터베이스 |
| **Vector DB** | pgvector | 0.7.0 | 벡터 검색 |
| **Frontend** | Streamlit | 1.40 | 웹 인터페이스 |
| **LLM** | GPT-4 | Latest | Hard Mode |
| **LLM** | Solar Pro2 | Latest | Easy Mode |
| **Embedding** | OpenAI | text-embedding-3-small | 벡터 생성 |
| **Search** | Tavily | 1.0 | 웹 검색 API |
| **Version Control** | Git | - | 버전 관리 |

**발표 스크립트:**
```
개발 환경과 기술 스택을 소개합니다.

Python 3.11을 메인 개발 언어로 사용하고,
LangGraph로 Agent 시스템을 구축했습니다.

PostgreSQL 16과 pgvector 0.7을 사용하여
벡터 데이터베이스를 구축했습니다.

Frontend는 Streamlit 1.40으로 개발하여
빠르게 프로토타입을 완성할 수 있었습니다.

LLM은 GPT-4와 Solar Pro2를 사용하여
성능과 비용의 균형을 맞췄습니다.

임베딩은 OpenAI의 text-embedding-3-small을 사용하고,
웹 검색은 Tavily API를 활용했습니다.

이러한 최신 기술 스택을 활용하여
안정적이고 확장 가능한 시스템을 구축했습니다.
```

---

## 요약

### 02_프로젝트_목표_및_설계.md 완료
- **페이지 7-20**: 프로젝트 목표 및 설계 내용
- **주요 내용**:
  - 문제 정의와 솔루션 제시
  - 시스템 아키텍처 (5-Layer)
  - Agent Flow와 LangGraph State Machine
  - PostgreSQL + pgvector DB 설계
  - RAG Pipeline 구조
  - 7가지 도구 정의
  - Easy/Hard Mode 프롬프트 엔지니어링
  - Streamlit UI 설계
  - 성능 목표와 역할 분담
  - 기술 스택

### 특징
- PRD 문서 참조하여 작성
- mermaid 다이어그램으로 시각화
- 각 페이지마다 발표 스크립트 포함
- 테이블로 상세 정보 제공